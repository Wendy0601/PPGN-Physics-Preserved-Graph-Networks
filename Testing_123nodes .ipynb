{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, scipy\n",
    "import torch   \n",
    "import scipy.sparse as sp \n",
    "import numpy as np  \n",
    "import torch.nn.functional as F \n",
    "import pickle\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt \n",
    "from math import *\n",
    "from numpy import transpose,matrix,exp,conj\n",
    "from numpy.linalg import inv \n",
    "from src_lwt.util_final import * \n",
    "import torch.nn as nn\n",
    "from load_data import *\n",
    "from PPGN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    np_load_old = np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k) \n",
    "    data = np.load('./data/info.npy')\n",
    "    bus_dict = data[0]  \n",
    "    bus_phase= data[1] \n",
    "    load_phase= data[3]  \n",
    "    fault_bus_list = sorted(list(bus_dict.keys())) \n",
    "    bus_to_index = dict((bus, index) for index, bus in enumerate(fault_bus_list))#  \n",
    "    index_to_bus = dict((index, bus) for bus, index in bus_to_index.items())\n",
    "    singlePhaseBus = [bus for bus, phases in bus_phase.items() if len(phases) == 1]\n",
    "    singlePhaseIndex = [bus_to_index[i] for i in singlePhaseBus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters  \n",
    "dropout = 0\n",
    "dropout_outlayer = 0 \n",
    "root = \"./data/\" \n",
    "num_node = 128   \n",
    "dim_input = 6\n",
    "num_labelper = 33\n",
    "agg_func = \"MEAN\"\n",
    "epochs =  100 \n",
    "b_sz = 32\n",
    "global seed\n",
    "seed = 842 \n",
    "learn_method = 'sup' \n",
    "max_vali_f1 = 0  \n",
    "hidden_emb_size = [32,32,32] \n",
    "num_layers = len(hidden_emb_size) \n",
    "dataSet = 'loc'\n",
    "ds = dataSet\n",
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\") \n",
    "random.seed( seed)\n",
    "np.random.seed( seed)\n",
    "torch.manual_seed( seed)\n",
    "torch.cuda.manual_seed_all( seed)\n",
    "lr = 0.001\n",
    "weight_name = 'A_short' \n",
    "k=3  \n",
    "measured_index =[73, 94, 105, 118, 72, 79, 24, 41, 69, 90, 84, 78, 122, 49, 66, 104, 109, 10, 36, 31, 85]\n",
    "dataC = dataCenter(num_labelper,seed, measured_index)   \n",
    "A, prob_A = select_A_prob(k, 'A_short') # option 1: 'A_short': using shortest path 'A_adam': using admittance matrix\n",
    "nodes_layers  = dic_nodes_neib(num_layers,A,prob_A )   \n",
    "\n",
    "\n",
    "modelname =   str(num_labelper) + '_all_'  + '.pkl'\n",
    "saveroot = \"./\"\n",
    "savepath = os.path.join(saveroot, modelname ) \n",
    "savebest = os.path.join(os.path.join(saveroot, '00_saved_final'), learn_method + '_'+ str(num_labelper) + '_all_' +str(len(measured_index)) + '.pkl') \n",
    "retrain =     True  \n",
    "#load data\n",
    "features, labels, ind_train ,  ind_test,    neib_observ,ind_labels, ind_measured= load_all_types( num_labelper , measured_index,seed = seed )\n",
    "with open(os.path.join(root, 'neib.pickle'), 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "neib  = dic['neib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './00_saved_final/sup_33_all_21.pkl'\n",
      "=> loaded checkpoint './00_saved_final/sup_33_all_21.pkl' (epoch 499)\n"
     ]
    }
   ],
   "source": [
    "#Establish PPGN\n",
    "graph_stageI = GraphSage(num_layers, dim_input,hidden_emb_size  ,   A, prob_A, dropout = dropout ,device = device,    agg_method= agg_func  )\n",
    "graph_stageI.to(device) \n",
    "classification = Outlayer_fully( hidden_emb_size[-1],  num_node,  dropout = dropout_outlayer) \n",
    "classification.to(device)\n",
    "models = [graph_stageI, classification]\n",
    "params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params.append(param)\n",
    "optimizer = torch.optim.Adam(params, lr = lr, weight_decay = 5e-3)  \n",
    "if retrain:\n",
    "    models, optimizer, start_epoch = load_checkpoint(models, optimizer, savebest) \n",
    "    graph_stageI, classification  = models[0], models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './00_saved_final/sup_33_all_21.pkl'\n",
      "=> loaded checkpoint './00_saved_final/sup_33_all_21.pkl' (epoch 499)\n",
      "Test F1:0.9765,  Acc:0.9779, Acc 1 hop: 0.9806 \n"
     ]
    }
   ],
   "source": [
    "# load the saved model\n",
    "models, optimizer, start_epoch = load_checkpoint(models, optimizer , savebest)\n",
    "graph_stageI, classification = models[0], models[1]\n",
    "labels_neib = one_hot_neib(labels, neib )\n",
    "models = [graph_stageI, classification]\n",
    "params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.requires_grad = False\n",
    "            params.append(param)\n",
    "\n",
    "embs = graph_stageI( nodes_layers ,features)\n",
    "logists =  classification(embs)\n",
    "predicts = torch.max(logists, 1)[1]\n",
    "labels_test = labels[ind_test] \n",
    "labels_neib_test = labels_neib[ind_test]\n",
    "assert len(labels[ind_test] ) == len(predicts[ind_test]  ) \n",
    "test_f1 = f1_score(labels[ind_test] , predicts[ind_test].cpu().data, average=\"macro\")\n",
    "acc = accuracy_score(labels_test, predicts[ind_test].cpu().data)\n",
    "acc_neib = hop_acc(labels_neib_test, predicts[ind_test].cpu().data)\n",
    "print(\"Test F1:%.4f,  Acc:%.4f, Acc 1 hop: %.4f \" %(test_f1, acc, acc_neib))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage II parameters\n",
    "A_119 = A_labels(A )\n",
    "Adj = constructW_stageI(embs , logists  ,A_119, 120 )\n",
    "features_X = torch.reshape(features, [features.shape[0],features.shape[1] *features.shape[2] ])\n",
    "seed = 42; epochs =300\n",
    "lr = 0.001\n",
    "weight_decay = 5e-5\n",
    "batch_size = 32\n",
    "hidden = [128*3,128*3, 128*3 ]\n",
    "dropout =0 \n",
    "fastmode = False\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed( seed)   \n",
    "retrain2= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GCN(nfeat=features_X.shape[1],\n",
    "            nhid= hidden,\n",
    "            nclass=119, \n",
    "            dropout= dropout)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(),\n",
    "                       lr= lr, weight_decay= weight_decay) \n",
    "model_name =  'StageII_all' + str(num_labelper) + '_.pkl'\n",
    "newname = 'StageII_all' + str(num_labelper) + '_.pkl'\n",
    "savepath2 = os.path.join(saveroot, model_name) \n",
    "savebest2 = os.path.join(saveroot, newname) \n",
    "if retrain2:\n",
    "    model2.load_state_dict(torch.load(savepath2)['state_dict'] )   \n",
    "    optimizer2.load_state_dict(torch.load(savepath2)['optimizer'] )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 4.7916 acc_train: 0.0100 loss_val: 4.6885 acc_val: 0.0316 time: 3.0135s\n",
      "Test set results: loss= 4.6847 accuracy= 0.0350 1-hop accuracy = 0.0998\n",
      "Epoch: 0002 loss_train: 4.6885 acc_train: 0.0316 loss_val: 4.5963 acc_val: 0.0519 time: 2.8297s\n",
      "Test set results: loss= 4.5883 accuracy= 0.0580 1-hop accuracy = 0.1076\n",
      "Epoch: 0003 loss_train: 4.5963 acc_train: 0.0519 loss_val: 4.5059 acc_val: 0.0482 time: 2.8227s\n",
      "Test set results: loss= 4.4942 accuracy= 0.0568 1-hop accuracy = 0.0965\n",
      "Epoch: 0004 loss_train: 4.5059 acc_train: 0.0482 loss_val: 4.4127 acc_val: 0.0500 time: 2.7729s\n",
      "Test set results: loss= 4.3976 accuracy= 0.0594 1-hop accuracy = 0.1065\n",
      "Epoch: 0005 loss_train: 4.4127 acc_train: 0.0500 loss_val: 4.3163 acc_val: 0.0500 time: 2.9354s\n",
      "Test set results: loss= 4.2985 accuracy= 0.0595 1-hop accuracy = 0.1074\n",
      "Epoch: 0006 loss_train: 4.3163 acc_train: 0.0500 loss_val: 4.2187 acc_val: 0.0500 time: 2.9370s\n",
      "Test set results: loss= 4.1977 accuracy= 0.0592 1-hop accuracy = 0.1069\n",
      "Epoch: 0007 loss_train: 4.2187 acc_train: 0.0500 loss_val: 4.1233 acc_val: 0.0500 time: 2.9474s\n",
      "Test set results: loss= 4.0975 accuracy= 0.0592 1-hop accuracy = 0.1062\n",
      "Epoch: 0008 loss_train: 4.1233 acc_train: 0.0500 loss_val: 4.0346 acc_val: 0.0614 time: 2.8341s\n",
      "Test set results: loss= 4.0018 accuracy= 0.0722 1-hop accuracy = 0.1210\n",
      "Epoch: 0009 loss_train: 4.0346 acc_train: 0.0614 loss_val: 3.9566 acc_val: 0.0624 time: 2.8896s\n",
      "Test set results: loss= 3.9153 accuracy= 0.0749 1-hop accuracy = 0.1239\n",
      "Epoch: 0010 loss_train: 3.9566 acc_train: 0.0624 loss_val: 3.8873 acc_val: 0.0721 time: 2.8670s\n",
      "Test set results: loss= 3.8377 accuracy= 0.0908 1-hop accuracy = 0.1355\n",
      "Epoch: 0011 loss_train: 3.8873 acc_train: 0.0721 loss_val: 3.8202 acc_val: 0.0838 time: 2.8227s\n",
      "Test set results: loss= 3.7639 accuracy= 0.1052 1-hop accuracy = 0.1657\n",
      "Epoch: 0012 loss_train: 3.8202 acc_train: 0.0838 loss_val: 3.7546 acc_val: 0.1131 time: 2.7548s\n",
      "Test set results: loss= 3.6924 accuracy= 0.1212 1-hop accuracy = 0.1815\n",
      "Epoch: 0013 loss_train: 3.7546 acc_train: 0.1131 loss_val: 3.6921 acc_val: 0.1125 time: 2.8630s\n",
      "Test set results: loss= 3.6241 accuracy= 0.1257 1-hop accuracy = 0.2025\n",
      "Epoch: 0014 loss_train: 3.6921 acc_train: 0.1125 loss_val: 3.6355 acc_val: 0.1289 time: 2.8090s\n",
      "Test set results: loss= 3.5611 accuracy= 0.1420 1-hop accuracy = 0.2174\n",
      "Epoch: 0015 loss_train: 3.6355 acc_train: 0.1289 loss_val: 3.5842 acc_val: 0.1389 time: 2.8107s\n",
      "Test set results: loss= 3.5027 accuracy= 0.1567 1-hop accuracy = 0.2303\n",
      "Epoch: 0016 loss_train: 3.5842 acc_train: 0.1389 loss_val: 3.5353 acc_val: 0.1417 time: 2.7723s\n",
      "Test set results: loss= 3.4458 accuracy= 0.1604 1-hop accuracy = 0.2512\n",
      "Epoch: 0017 loss_train: 3.5353 acc_train: 0.1417 loss_val: 3.4870 acc_val: 0.1777 time: 2.9270s\n",
      "Test set results: loss= 3.3888 accuracy= 0.1877 1-hop accuracy = 0.2927\n",
      "Epoch: 0018 loss_train: 3.4870 acc_train: 0.1777 loss_val: 3.4400 acc_val: 0.1693 time: 3.0758s\n",
      "Test set results: loss= 3.3327 accuracy= 0.1926 1-hop accuracy = 0.2910\n",
      "Epoch: 0019 loss_train: 3.4400 acc_train: 0.1693 loss_val: 3.3951 acc_val: 0.1744 time: 2.8006s\n",
      "Test set results: loss= 3.2793 accuracy= 0.1977 1-hop accuracy = 0.3125\n",
      "Epoch: 0020 loss_train: 3.3951 acc_train: 0.1744 loss_val: 3.3510 acc_val: 0.1905 time: 2.8488s\n",
      "Test set results: loss= 3.2278 accuracy= 0.2171 1-hop accuracy = 0.3329\n",
      "Epoch: 0021 loss_train: 3.3510 acc_train: 0.1905 loss_val: 3.3054 acc_val: 0.1941 time: 2.9693s\n",
      "Test set results: loss= 3.1758 accuracy= 0.2280 1-hop accuracy = 0.3525\n",
      "Epoch: 0022 loss_train: 3.3054 acc_train: 0.1941 loss_val: 3.2587 acc_val: 0.2078 time: 2.8736s\n",
      "Test set results: loss= 3.1239 accuracy= 0.2333 1-hop accuracy = 0.3652\n",
      "Epoch: 0023 loss_train: 3.2587 acc_train: 0.2078 loss_val: 3.2141 acc_val: 0.2154 time: 2.8005s\n",
      "Test set results: loss= 3.0748 accuracy= 0.2357 1-hop accuracy = 0.3737\n",
      "Epoch: 0024 loss_train: 3.2141 acc_train: 0.2154 loss_val: 3.1717 acc_val: 0.2298 time: 2.9715s\n",
      "Test set results: loss= 3.0282 accuracy= 0.2525 1-hop accuracy = 0.4085\n",
      "Epoch: 0025 loss_train: 3.1717 acc_train: 0.2298 loss_val: 3.1292 acc_val: 0.2415 time: 2.7998s\n",
      "Test set results: loss= 2.9818 accuracy= 0.2655 1-hop accuracy = 0.4299\n",
      "Epoch: 0026 loss_train: 3.1292 acc_train: 0.2415 loss_val: 3.0856 acc_val: 0.2492 time: 2.7611s\n",
      "Test set results: loss= 2.9349 accuracy= 0.2806 1-hop accuracy = 0.4523\n",
      "Epoch: 0027 loss_train: 3.0856 acc_train: 0.2492 loss_val: 3.0423 acc_val: 0.2711 time: 2.7877s\n",
      "Test set results: loss= 2.8891 accuracy= 0.3050 1-hop accuracy = 0.4648\n",
      "Epoch: 0028 loss_train: 3.0423 acc_train: 0.2711 loss_val: 3.0005 acc_val: 0.2709 time: 2.7401s\n",
      "Test set results: loss= 2.8457 accuracy= 0.3100 1-hop accuracy = 0.4659\n",
      "Epoch: 0029 loss_train: 3.0005 acc_train: 0.2709 loss_val: 2.9589 acc_val: 0.2907 time: 2.7621s\n",
      "Test set results: loss= 2.8033 accuracy= 0.3266 1-hop accuracy = 0.4861\n",
      "Epoch: 0030 loss_train: 2.9589 acc_train: 0.2907 loss_val: 2.9170 acc_val: 0.3116 time: 2.7354s\n",
      "Test set results: loss= 2.7613 accuracy= 0.3471 1-hop accuracy = 0.5056\n",
      "Epoch: 0031 loss_train: 2.9170 acc_train: 0.3116 loss_val: 2.8759 acc_val: 0.3306 time: 2.7567s\n",
      "Test set results: loss= 2.7203 accuracy= 0.3648 1-hop accuracy = 0.5173\n",
      "Epoch: 0032 loss_train: 2.8759 acc_train: 0.3306 loss_val: 2.8354 acc_val: 0.3333 time: 2.7531s\n",
      "Test set results: loss= 2.6802 accuracy= 0.3672 1-hop accuracy = 0.5292\n",
      "Epoch: 0033 loss_train: 2.8354 acc_train: 0.3333 loss_val: 2.7950 acc_val: 0.3435 time: 2.6878s\n",
      "Test set results: loss= 2.6404 accuracy= 0.3820 1-hop accuracy = 0.5470\n",
      "Epoch: 0034 loss_train: 2.7950 acc_train: 0.3435 loss_val: 2.7551 acc_val: 0.3485 time: 2.7548s\n",
      "Test set results: loss= 2.6016 accuracy= 0.3853 1-hop accuracy = 0.5550\n",
      "Epoch: 0035 loss_train: 2.7551 acc_train: 0.3485 loss_val: 2.7164 acc_val: 0.3589 time: 2.7960s\n",
      "Test set results: loss= 2.5650 accuracy= 0.3975 1-hop accuracy = 0.5641\n",
      "Epoch: 0036 loss_train: 2.7164 acc_train: 0.3589 loss_val: 2.6781 acc_val: 0.3640 time: 2.8061s\n",
      "Test set results: loss= 2.5295 accuracy= 0.3942 1-hop accuracy = 0.5654\n",
      "Epoch: 0037 loss_train: 2.6781 acc_train: 0.3640 loss_val: 2.6402 acc_val: 0.3755 time: 2.7874s\n",
      "Test set results: loss= 2.4948 accuracy= 0.4114 1-hop accuracy = 0.5802\n",
      "Epoch: 0038 loss_train: 2.6402 acc_train: 0.3755 loss_val: 2.6037 acc_val: 0.3764 time: 2.7915s\n",
      "Test set results: loss= 2.4615 accuracy= 0.4097 1-hop accuracy = 0.5866\n",
      "Epoch: 0039 loss_train: 2.6037 acc_train: 0.3764 loss_val: 2.5682 acc_val: 0.3838 time: 2.7358s\n",
      "Test set results: loss= 2.4288 accuracy= 0.4141 1-hop accuracy = 0.5830\n",
      "Epoch: 0040 loss_train: 2.5682 acc_train: 0.3838 loss_val: 2.5327 acc_val: 0.3928 time: 2.7836s\n",
      "Test set results: loss= 2.3954 accuracy= 0.4212 1-hop accuracy = 0.5859\n",
      "Epoch: 0041 loss_train: 2.5327 acc_train: 0.3928 loss_val: 2.4978 acc_val: 0.4035 time: 2.7645s\n",
      "Test set results: loss= 2.3616 accuracy= 0.4322 1-hop accuracy = 0.6049\n",
      "Epoch: 0042 loss_train: 2.4978 acc_train: 0.4035 loss_val: 2.4642 acc_val: 0.4042 time: 2.7607s\n",
      "Test set results: loss= 2.3289 accuracy= 0.4368 1-hop accuracy = 0.6166\n",
      "Epoch: 0043 loss_train: 2.4642 acc_train: 0.4042 loss_val: 2.4315 acc_val: 0.4125 time: 2.7857s\n",
      "Test set results: loss= 2.2981 accuracy= 0.4497 1-hop accuracy = 0.6273\n",
      "Epoch: 0044 loss_train: 2.4315 acc_train: 0.4125 loss_val: 2.3993 acc_val: 0.4342 time: 2.8021s\n",
      "Test set results: loss= 2.2689 accuracy= 0.4739 1-hop accuracy = 0.6480\n",
      "Epoch: 0045 loss_train: 2.3993 acc_train: 0.4342 loss_val: 2.3681 acc_val: 0.4412 time: 2.8060s\n",
      "Test set results: loss= 2.2402 accuracy= 0.4809 1-hop accuracy = 0.6589\n",
      "Epoch: 0046 loss_train: 2.3681 acc_train: 0.4412 loss_val: 2.3376 acc_val: 0.4435 time: 2.8395s\n",
      "Test set results: loss= 2.2111 accuracy= 0.4843 1-hop accuracy = 0.6596\n",
      "Epoch: 0047 loss_train: 2.3376 acc_train: 0.4435 loss_val: 2.3078 acc_val: 0.4469 time: 2.7783s\n",
      "Test set results: loss= 2.1823 accuracy= 0.4846 1-hop accuracy = 0.6649\n",
      "Epoch: 0048 loss_train: 2.3078 acc_train: 0.4469 loss_val: 2.2787 acc_val: 0.4529 time: 2.7992s\n",
      "Test set results: loss= 2.1551 accuracy= 0.4905 1-hop accuracy = 0.6741\n",
      "Epoch: 0049 loss_train: 2.2787 acc_train: 0.4529 loss_val: 2.2500 acc_val: 0.4596 time: 2.7697s\n",
      "Test set results: loss= 2.1294 accuracy= 0.4975 1-hop accuracy = 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 loss_train: 2.2500 acc_train: 0.4596 loss_val: 2.2220 acc_val: 0.4609 time: 2.7763s\n",
      "Test set results: loss= 2.1046 accuracy= 0.5022 1-hop accuracy = 0.6846\n",
      "Epoch: 0051 loss_train: 2.2220 acc_train: 0.4609 loss_val: 2.1946 acc_val: 0.4638 time: 2.7811s\n",
      "Test set results: loss= 2.0797 accuracy= 0.5060 1-hop accuracy = 0.6901\n",
      "Epoch: 0052 loss_train: 2.1946 acc_train: 0.4638 loss_val: 2.1678 acc_val: 0.4655 time: 2.7970s\n",
      "Test set results: loss= 2.0550 accuracy= 0.5064 1-hop accuracy = 0.6934\n",
      "Epoch: 0053 loss_train: 2.1678 acc_train: 0.4655 loss_val: 2.1418 acc_val: 0.4675 time: 2.7931s\n",
      "Test set results: loss= 2.0313 accuracy= 0.5078 1-hop accuracy = 0.6941\n",
      "Epoch: 0054 loss_train: 2.1418 acc_train: 0.4675 loss_val: 2.1161 acc_val: 0.4716 time: 2.8242s\n",
      "Test set results: loss= 2.0086 accuracy= 0.5109 1-hop accuracy = 0.6989\n",
      "Epoch: 0055 loss_train: 2.1161 acc_train: 0.4716 loss_val: 2.0909 acc_val: 0.4735 time: 2.7895s\n",
      "Test set results: loss= 1.9870 accuracy= 0.5139 1-hop accuracy = 0.7048\n",
      "Epoch: 0056 loss_train: 2.0909 acc_train: 0.4735 loss_val: 2.0662 acc_val: 0.4786 time: 2.7505s\n",
      "Test set results: loss= 1.9663 accuracy= 0.5167 1-hop accuracy = 0.7078\n",
      "Epoch: 0057 loss_train: 2.0662 acc_train: 0.4786 loss_val: 2.0420 acc_val: 0.4823 time: 2.7905s\n",
      "Test set results: loss= 1.9461 accuracy= 0.5187 1-hop accuracy = 0.7107\n",
      "Epoch: 0058 loss_train: 2.0420 acc_train: 0.4823 loss_val: 2.0186 acc_val: 0.4858 time: 2.8234s\n",
      "Test set results: loss= 1.9266 accuracy= 0.5199 1-hop accuracy = 0.7127\n",
      "Epoch: 0059 loss_train: 2.0186 acc_train: 0.4858 loss_val: 1.9956 acc_val: 0.4855 time: 2.7925s\n",
      "Test set results: loss= 1.9069 accuracy= 0.5207 1-hop accuracy = 0.7137\n",
      "Epoch: 0060 loss_train: 1.9956 acc_train: 0.4855 loss_val: 1.9731 acc_val: 0.4886 time: 2.8546s\n",
      "Test set results: loss= 1.8872 accuracy= 0.5218 1-hop accuracy = 0.7166\n",
      "Epoch: 0061 loss_train: 1.9731 acc_train: 0.4886 loss_val: 1.9512 acc_val: 0.4911 time: 2.9229s\n",
      "Test set results: loss= 1.8683 accuracy= 0.5225 1-hop accuracy = 0.7195\n",
      "Epoch: 0062 loss_train: 1.9512 acc_train: 0.4911 loss_val: 1.9296 acc_val: 0.4950 time: 2.8132s\n",
      "Test set results: loss= 1.8501 accuracy= 0.5259 1-hop accuracy = 0.7239\n",
      "Epoch: 0063 loss_train: 1.9296 acc_train: 0.4950 loss_val: 1.9085 acc_val: 0.4993 time: 2.9085s\n",
      "Test set results: loss= 1.8324 accuracy= 0.5293 1-hop accuracy = 0.7289\n",
      "Epoch: 0064 loss_train: 1.9085 acc_train: 0.4993 loss_val: 1.8880 acc_val: 0.5029 time: 2.8003s\n",
      "Test set results: loss= 1.8157 accuracy= 0.5319 1-hop accuracy = 0.7336\n",
      "Epoch: 0065 loss_train: 1.8880 acc_train: 0.5029 loss_val: 1.8677 acc_val: 0.5059 time: 2.8505s\n",
      "Test set results: loss= 1.7990 accuracy= 0.5344 1-hop accuracy = 0.7391\n",
      "Epoch: 0066 loss_train: 1.8677 acc_train: 0.5059 loss_val: 1.8478 acc_val: 0.5078 time: 2.7990s\n",
      "Test set results: loss= 1.7821 accuracy= 0.5360 1-hop accuracy = 0.7414\n",
      "Epoch: 0067 loss_train: 1.8478 acc_train: 0.5078 loss_val: 1.8281 acc_val: 0.5079 time: 2.8512s\n",
      "Test set results: loss= 1.7657 accuracy= 0.5365 1-hop accuracy = 0.7432\n",
      "Epoch: 0068 loss_train: 1.8281 acc_train: 0.5079 loss_val: 1.8087 acc_val: 0.5108 time: 2.8346s\n",
      "Test set results: loss= 1.7494 accuracy= 0.5382 1-hop accuracy = 0.7461\n",
      "Epoch: 0069 loss_train: 1.8087 acc_train: 0.5108 loss_val: 1.7898 acc_val: 0.5165 time: 2.8239s\n",
      "Test set results: loss= 1.7332 accuracy= 0.5400 1-hop accuracy = 0.7502\n",
      "Epoch: 0070 loss_train: 1.7898 acc_train: 0.5165 loss_val: 1.7711 acc_val: 0.5194 time: 2.8708s\n",
      "Test set results: loss= 1.7173 accuracy= 0.5421 1-hop accuracy = 0.7537\n",
      "Epoch: 0071 loss_train: 1.7711 acc_train: 0.5194 loss_val: 1.7527 acc_val: 0.5213 time: 2.8790s\n",
      "Test set results: loss= 1.7010 accuracy= 0.5445 1-hop accuracy = 0.7555\n",
      "Epoch: 0072 loss_train: 1.7527 acc_train: 0.5213 loss_val: 1.7346 acc_val: 0.5262 time: 2.8068s\n",
      "Test set results: loss= 1.6853 accuracy= 0.5469 1-hop accuracy = 0.7577\n",
      "Epoch: 0073 loss_train: 1.7346 acc_train: 0.5262 loss_val: 1.7169 acc_val: 0.5283 time: 2.8679s\n",
      "Test set results: loss= 1.6700 accuracy= 0.5507 1-hop accuracy = 0.7625\n",
      "Epoch: 0074 loss_train: 1.7169 acc_train: 0.5283 loss_val: 1.6995 acc_val: 0.5349 time: 2.8896s\n",
      "Test set results: loss= 1.6552 accuracy= 0.5534 1-hop accuracy = 0.7690\n",
      "Epoch: 0075 loss_train: 1.6995 acc_train: 0.5349 loss_val: 1.6824 acc_val: 0.5380 time: 2.8345s\n",
      "Test set results: loss= 1.6405 accuracy= 0.5554 1-hop accuracy = 0.7713\n",
      "Epoch: 0076 loss_train: 1.6824 acc_train: 0.5380 loss_val: 1.6656 acc_val: 0.5373 time: 2.8485s\n",
      "Test set results: loss= 1.6260 accuracy= 0.5559 1-hop accuracy = 0.7754\n",
      "Epoch: 0077 loss_train: 1.6656 acc_train: 0.5373 loss_val: 1.6493 acc_val: 0.5449 time: 2.8537s\n",
      "Test set results: loss= 1.6110 accuracy= 0.5618 1-hop accuracy = 0.7792\n",
      "Epoch: 0078 loss_train: 1.6493 acc_train: 0.5449 loss_val: 1.6341 acc_val: 0.5439 time: 2.8536s\n",
      "Test set results: loss= 1.5999 accuracy= 0.5662 1-hop accuracy = 0.7829\n",
      "Epoch: 0079 loss_train: 1.6341 acc_train: 0.5439 loss_val: 1.6219 acc_val: 0.5599 time: 2.8590s\n",
      "Test set results: loss= 1.5847 accuracy= 0.5685 1-hop accuracy = 0.7899\n",
      "Epoch: 0080 loss_train: 1.6219 acc_train: 0.5599 loss_val: 1.6127 acc_val: 0.5489 time: 2.8660s\n",
      "Test set results: loss= 1.5822 accuracy= 0.5669 1-hop accuracy = 0.7832\n",
      "Epoch: 0081 loss_train: 1.6127 acc_train: 0.5489 loss_val: 1.5935 acc_val: 0.5707 time: 2.8717s\n",
      "Test set results: loss= 1.5599 accuracy= 0.5745 1-hop accuracy = 0.7955\n",
      "Epoch: 0082 loss_train: 1.5935 acc_train: 0.5707 loss_val: 1.5737 acc_val: 0.5598 time: 2.8369s\n",
      "Test set results: loss= 1.5473 accuracy= 0.5759 1-hop accuracy = 0.7967\n",
      "Epoch: 0083 loss_train: 1.5737 acc_train: 0.5598 loss_val: 1.5664 acc_val: 0.5577 time: 2.8946s\n",
      "Test set results: loss= 1.5424 accuracy= 0.5739 1-hop accuracy = 0.7931\n",
      "Epoch: 0084 loss_train: 1.5664 acc_train: 0.5577 loss_val: 1.5509 acc_val: 0.5761 time: 2.8842s\n",
      "Test set results: loss= 1.5253 accuracy= 0.5801 1-hop accuracy = 0.8038\n",
      "Epoch: 0085 loss_train: 1.5509 acc_train: 0.5761 loss_val: 1.5339 acc_val: 0.5757 time: 2.8920s\n",
      "Test set results: loss= 1.5158 accuracy= 0.5843 1-hop accuracy = 0.8126\n",
      "Epoch: 0086 loss_train: 1.5339 acc_train: 0.5757 loss_val: 1.5258 acc_val: 0.5664 time: 2.8518s\n",
      "Test set results: loss= 1.5108 accuracy= 0.5805 1-hop accuracy = 0.8042\n",
      "Epoch: 0087 loss_train: 1.5258 acc_train: 0.5664 loss_val: 1.5106 acc_val: 0.5824 time: 2.9121s\n",
      "Test set results: loss= 1.4947 accuracy= 0.5864 1-hop accuracy = 0.8114\n",
      "Epoch: 0088 loss_train: 1.5106 acc_train: 0.5824 loss_val: 1.4968 acc_val: 0.5856 time: 3.0729s\n",
      "Test set results: loss= 1.4850 accuracy= 0.5901 1-hop accuracy = 0.8190\n",
      "Epoch: 0089 loss_train: 1.4968 acc_train: 0.5856 loss_val: 1.4876 acc_val: 0.5721 time: 2.8440s\n",
      "Test set results: loss= 1.4800 accuracy= 0.5867 1-hop accuracy = 0.8141\n",
      "Epoch: 0090 loss_train: 1.4876 acc_train: 0.5721 loss_val: 1.4731 acc_val: 0.5885 time: 2.9093s\n",
      "Test set results: loss= 1.4664 accuracy= 0.5918 1-hop accuracy = 0.8178\n",
      "Epoch: 0091 loss_train: 1.4731 acc_train: 0.5885 loss_val: 1.4613 acc_val: 0.5940 time: 2.9071s\n",
      "Test set results: loss= 1.4577 accuracy= 0.5949 1-hop accuracy = 0.8277\n",
      "Epoch: 0092 loss_train: 1.4613 acc_train: 0.5940 loss_val: 1.4513 acc_val: 0.5814 time: 2.9770s\n",
      "Test set results: loss= 1.4522 accuracy= 0.5943 1-hop accuracy = 0.8251\n",
      "Epoch: 0093 loss_train: 1.4513 acc_train: 0.5814 loss_val: 1.4378 acc_val: 0.5983 time: 2.9406s\n",
      "Test set results: loss= 1.4396 accuracy= 0.5980 1-hop accuracy = 0.8277\n",
      "Epoch: 0094 loss_train: 1.4378 acc_train: 0.5983 loss_val: 1.4268 acc_val: 0.6037 time: 2.9246s\n",
      "Test set results: loss= 1.4310 accuracy= 0.6018 1-hop accuracy = 0.8355\n",
      "Epoch: 0095 loss_train: 1.4268 acc_train: 0.6037 loss_val: 1.4167 acc_val: 0.5940 time: 2.9982s\n",
      "Test set results: loss= 1.4267 accuracy= 0.6000 1-hop accuracy = 0.8371\n",
      "Epoch: 0096 loss_train: 1.4167 acc_train: 0.5940 loss_val: 1.4043 acc_val: 0.6069 time: 2.9365s\n",
      "Test set results: loss= 1.4159 accuracy= 0.6060 1-hop accuracy = 0.8394\n",
      "Epoch: 0097 loss_train: 1.4043 acc_train: 0.6069 loss_val: 1.3935 acc_val: 0.6100 time: 2.8891s\n",
      "Test set results: loss= 1.4066 accuracy= 0.6112 1-hop accuracy = 0.8490\n",
      "Epoch: 0098 loss_train: 1.3935 acc_train: 0.6100 loss_val: 1.3841 acc_val: 0.6062 time: 2.9753s\n",
      "Test set results: loss= 1.4016 accuracy= 0.6113 1-hop accuracy = 0.8523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0099 loss_train: 1.3841 acc_train: 0.6062 loss_val: 1.3726 acc_val: 0.6152 time: 2.9357s\n",
      "Test set results: loss= 1.3925 accuracy= 0.6148 1-hop accuracy = 0.8524\n",
      "Epoch: 0100 loss_train: 1.3726 acc_train: 0.6152 loss_val: 1.3620 acc_val: 0.6205 time: 2.9743s\n",
      "Test set results: loss= 1.3855 accuracy= 0.6172 1-hop accuracy = 0.8569\n",
      "Epoch: 0101 loss_train: 1.3620 acc_train: 0.6205 loss_val: 1.3529 acc_val: 0.6172 time: 2.9666s\n",
      "Test set results: loss= 1.3800 accuracy= 0.6154 1-hop accuracy = 0.8585\n",
      "Epoch: 0102 loss_train: 1.3529 acc_train: 0.6172 loss_val: 1.3423 acc_val: 0.6221 time: 3.0285s\n",
      "Test set results: loss= 1.3709 accuracy= 0.6183 1-hop accuracy = 0.8572\n",
      "Epoch: 0103 loss_train: 1.3423 acc_train: 0.6221 loss_val: 1.3315 acc_val: 0.6269 time: 2.9658s\n",
      "Test set results: loss= 1.3644 accuracy= 0.6231 1-hop accuracy = 0.8638\n",
      "Epoch: 0104 loss_train: 1.3315 acc_train: 0.6269 loss_val: 1.3224 acc_val: 0.6252 time: 2.9534s\n",
      "Test set results: loss= 1.3600 accuracy= 0.6199 1-hop accuracy = 0.8631\n",
      "Epoch: 0105 loss_train: 1.3224 acc_train: 0.6252 loss_val: 1.3130 acc_val: 0.6269 time: 3.0216s\n",
      "Test set results: loss= 1.3529 accuracy= 0.6225 1-hop accuracy = 0.8631\n",
      "Epoch: 0106 loss_train: 1.3130 acc_train: 0.6269 loss_val: 1.3027 acc_val: 0.6316 time: 3.0119s\n",
      "Test set results: loss= 1.3467 accuracy= 0.6266 1-hop accuracy = 0.8683\n",
      "Epoch: 0107 loss_train: 1.3027 acc_train: 0.6316 loss_val: 1.2933 acc_val: 0.6301 time: 3.0473s\n",
      "Test set results: loss= 1.3411 accuracy= 0.6251 1-hop accuracy = 0.8706\n",
      "Epoch: 0108 loss_train: 1.2933 acc_train: 0.6301 loss_val: 1.2846 acc_val: 0.6331 time: 2.9891s\n",
      "Test set results: loss= 1.3346 accuracy= 0.6267 1-hop accuracy = 0.8710\n",
      "Epoch: 0109 loss_train: 1.2846 acc_train: 0.6331 loss_val: 1.2754 acc_val: 0.6373 time: 3.0603s\n",
      "Test set results: loss= 1.3293 accuracy= 0.6282 1-hop accuracy = 0.8734\n",
      "Epoch: 0110 loss_train: 1.2754 acc_train: 0.6373 loss_val: 1.2659 acc_val: 0.6391 time: 3.0467s\n",
      "Test set results: loss= 1.3228 accuracy= 0.6288 1-hop accuracy = 0.8755\n",
      "Epoch: 0111 loss_train: 1.2659 acc_train: 0.6391 loss_val: 1.2572 acc_val: 0.6386 time: 3.0922s\n",
      "Test set results: loss= 1.3172 accuracy= 0.6292 1-hop accuracy = 0.8755\n",
      "Epoch: 0112 loss_train: 1.2572 acc_train: 0.6386 loss_val: 1.2489 acc_val: 0.6405 time: 3.0454s\n",
      "Test set results: loss= 1.3126 accuracy= 0.6293 1-hop accuracy = 0.8773\n",
      "Epoch: 0113 loss_train: 1.2489 acc_train: 0.6405 loss_val: 1.2404 acc_val: 0.6436 time: 3.0795s\n",
      "Test set results: loss= 1.3058 accuracy= 0.6328 1-hop accuracy = 0.8799\n",
      "Epoch: 0114 loss_train: 1.2404 acc_train: 0.6436 loss_val: 1.2317 acc_val: 0.6415 time: 3.0796s\n",
      "Test set results: loss= 1.3019 accuracy= 0.6336 1-hop accuracy = 0.8809\n",
      "Epoch: 0115 loss_train: 1.2317 acc_train: 0.6415 loss_val: 1.2232 acc_val: 0.6454 time: 3.0406s\n",
      "Test set results: loss= 1.2969 accuracy= 0.6334 1-hop accuracy = 0.8819\n",
      "Epoch: 0116 loss_train: 1.2232 acc_train: 0.6454 loss_val: 1.2153 acc_val: 0.6467 time: 3.0963s\n",
      "Test set results: loss= 1.2912 accuracy= 0.6356 1-hop accuracy = 0.8861\n",
      "Epoch: 0117 loss_train: 1.2153 acc_train: 0.6467 loss_val: 1.2074 acc_val: 0.6480 time: 3.0393s\n",
      "Test set results: loss= 1.2877 accuracy= 0.6359 1-hop accuracy = 0.8846\n",
      "Epoch: 0118 loss_train: 1.2074 acc_train: 0.6480 loss_val: 1.1994 acc_val: 0.6507 time: 3.1443s\n",
      "Test set results: loss= 1.2825 accuracy= 0.6383 1-hop accuracy = 0.8906\n",
      "Epoch: 0119 loss_train: 1.1994 acc_train: 0.6507 loss_val: 1.1914 acc_val: 0.6508 time: 3.0925s\n",
      "Test set results: loss= 1.2780 accuracy= 0.6391 1-hop accuracy = 0.8906\n",
      "Epoch: 0120 loss_train: 1.1914 acc_train: 0.6508 loss_val: 1.1838 acc_val: 0.6508 time: 3.0443s\n",
      "Test set results: loss= 1.2747 accuracy= 0.6384 1-hop accuracy = 0.8917\n",
      "Epoch: 0121 loss_train: 1.1838 acc_train: 0.6508 loss_val: 1.1764 acc_val: 0.6568 time: 3.0946s\n",
      "Test set results: loss= 1.2697 accuracy= 0.6417 1-hop accuracy = 0.8938\n",
      "Epoch: 0122 loss_train: 1.1764 acc_train: 0.6568 loss_val: 1.1691 acc_val: 0.6558 time: 3.0995s\n",
      "Test set results: loss= 1.2662 accuracy= 0.6410 1-hop accuracy = 0.8933\n",
      "Epoch: 0123 loss_train: 1.1691 acc_train: 0.6558 loss_val: 1.1621 acc_val: 0.6560 time: 3.1201s\n",
      "Test set results: loss= 1.2616 accuracy= 0.6426 1-hop accuracy = 0.8967\n",
      "Epoch: 0124 loss_train: 1.1621 acc_train: 0.6560 loss_val: 1.1556 acc_val: 0.6622 time: 3.1341s\n",
      "Test set results: loss= 1.2599 accuracy= 0.6435 1-hop accuracy = 0.8998\n",
      "Epoch: 0125 loss_train: 1.1556 acc_train: 0.6622 loss_val: 1.1508 acc_val: 0.6615 time: 3.1192s\n",
      "Test set results: loss= 1.2548 accuracy= 0.6514 1-hop accuracy = 0.9051\n",
      "Epoch: 0126 loss_train: 1.1508 acc_train: 0.6615 loss_val: 1.1483 acc_val: 0.6525 time: 3.1532s\n",
      "Test set results: loss= 1.2587 accuracy= 0.6391 1-hop accuracy = 0.9003\n",
      "Epoch: 0127 loss_train: 1.1483 acc_train: 0.6525 loss_val: 1.1427 acc_val: 0.6592 time: 3.1607s\n",
      "Test set results: loss= 1.2503 accuracy= 0.6458 1-hop accuracy = 0.9065\n",
      "Epoch: 0128 loss_train: 1.1427 acc_train: 0.6592 loss_val: 1.1319 acc_val: 0.6632 time: 3.1320s\n",
      "Test set results: loss= 1.2465 accuracy= 0.6480 1-hop accuracy = 0.9120\n",
      "Epoch: 0129 loss_train: 1.1319 acc_train: 0.6632 loss_val: 1.1232 acc_val: 0.6654 time: 3.0725s\n",
      "Test set results: loss= 1.2431 accuracy= 0.6483 1-hop accuracy = 0.9025\n",
      "Epoch: 0130 loss_train: 1.1232 acc_train: 0.6654 loss_val: 1.1192 acc_val: 0.6650 time: 3.1516s\n",
      "Test set results: loss= 1.2379 accuracy= 0.6546 1-hop accuracy = 0.9136\n",
      "Epoch: 0131 loss_train: 1.1192 acc_train: 0.6650 loss_val: 1.1129 acc_val: 0.6690 time: 3.1508s\n",
      "Test set results: loss= 1.2377 accuracy= 0.6471 1-hop accuracy = 0.9146\n",
      "Epoch: 0132 loss_train: 1.1129 acc_train: 0.6690 loss_val: 1.1053 acc_val: 0.6728 time: 3.1378s\n",
      "Test set results: loss= 1.2320 accuracy= 0.6547 1-hop accuracy = 0.9161\n",
      "Epoch: 0133 loss_train: 1.1053 acc_train: 0.6728 loss_val: 1.1008 acc_val: 0.6731 time: 3.1679s\n",
      "Test set results: loss= 1.2288 accuracy= 0.6586 1-hop accuracy = 0.9167\n",
      "Epoch: 0134 loss_train: 1.1008 acc_train: 0.6731 loss_val: 1.0943 acc_val: 0.6752 time: 3.1947s\n",
      "Test set results: loss= 1.2294 accuracy= 0.6480 1-hop accuracy = 0.9161\n",
      "Epoch: 0135 loss_train: 1.0943 acc_train: 0.6752 loss_val: 1.0865 acc_val: 0.6786 time: 3.2028s\n",
      "Test set results: loss= 1.2218 accuracy= 0.6563 1-hop accuracy = 0.9212\n",
      "Epoch: 0136 loss_train: 1.0865 acc_train: 0.6786 loss_val: 1.0828 acc_val: 0.6801 time: 3.1174s\n",
      "Test set results: loss= 1.2196 accuracy= 0.6618 1-hop accuracy = 0.9226\n",
      "Epoch: 0137 loss_train: 1.0828 acc_train: 0.6801 loss_val: 1.0776 acc_val: 0.6769 time: 3.1650s\n",
      "Test set results: loss= 1.2215 accuracy= 0.6524 1-hop accuracy = 0.9182\n",
      "Epoch: 0138 loss_train: 1.0776 acc_train: 0.6769 loss_val: 1.0697 acc_val: 0.6816 time: 3.2001s\n",
      "Test set results: loss= 1.2140 accuracy= 0.6593 1-hop accuracy = 0.9231\n",
      "Epoch: 0139 loss_train: 1.0697 acc_train: 0.6816 loss_val: 1.0649 acc_val: 0.6819 time: 3.2766s\n",
      "Test set results: loss= 1.2114 accuracy= 0.6621 1-hop accuracy = 0.9251\n",
      "Epoch: 0140 loss_train: 1.0649 acc_train: 0.6819 loss_val: 1.0612 acc_val: 0.6862 time: 3.2452s\n",
      "Test set results: loss= 1.2127 accuracy= 0.6596 1-hop accuracy = 0.9247\n",
      "Epoch: 0141 loss_train: 1.0612 acc_train: 0.6862 loss_val: 1.0543 acc_val: 0.6866 time: 3.2791s\n",
      "Test set results: loss= 1.2064 accuracy= 0.6606 1-hop accuracy = 0.9276\n",
      "Epoch: 0142 loss_train: 1.0543 acc_train: 0.6866 loss_val: 1.0485 acc_val: 0.6850 time: 3.1943s\n",
      "Test set results: loss= 1.2048 accuracy= 0.6602 1-hop accuracy = 0.9290\n",
      "Epoch: 0143 loss_train: 1.0485 acc_train: 0.6850 loss_val: 1.0447 acc_val: 0.6845 time: 3.1897s\n",
      "Test set results: loss= 1.2046 accuracy= 0.6640 1-hop accuracy = 0.9276\n",
      "Epoch: 0144 loss_train: 1.0447 acc_train: 0.6845 loss_val: 1.0389 acc_val: 0.6934 time: 3.2085s\n",
      "Test set results: loss= 1.1980 accuracy= 0.6642 1-hop accuracy = 0.9331\n",
      "Epoch: 0145 loss_train: 1.0389 acc_train: 0.6934 loss_val: 1.0334 acc_val: 0.6909 time: 3.2022s\n",
      "Test set results: loss= 1.1973 accuracy= 0.6632 1-hop accuracy = 0.9326\n",
      "Epoch: 0146 loss_train: 1.0334 acc_train: 0.6909 loss_val: 1.0293 acc_val: 0.6877 time: 3.2553s\n",
      "Test set results: loss= 1.1974 accuracy= 0.6641 1-hop accuracy = 0.9276\n",
      "Epoch: 0147 loss_train: 1.0293 acc_train: 0.6877 loss_val: 1.0240 acc_val: 0.6968 time: 3.2005s\n",
      "Test set results: loss= 1.1911 accuracy= 0.6689 1-hop accuracy = 0.9365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0148 loss_train: 1.0240 acc_train: 0.6968 loss_val: 1.0185 acc_val: 0.6949 time: 3.2523s\n",
      "Test set results: loss= 1.1903 accuracy= 0.6658 1-hop accuracy = 0.9337\n",
      "Epoch: 0149 loss_train: 1.0185 acc_train: 0.6949 loss_val: 1.0146 acc_val: 0.6914 time: 3.1884s\n",
      "Test set results: loss= 1.1890 accuracy= 0.6641 1-hop accuracy = 0.9306\n",
      "Epoch: 0150 loss_train: 1.0146 acc_train: 0.6914 loss_val: 1.0098 acc_val: 0.6989 time: 3.2090s\n",
      "Test set results: loss= 1.1831 accuracy= 0.6730 1-hop accuracy = 0.9416\n",
      "Epoch: 0151 loss_train: 1.0098 acc_train: 0.6989 loss_val: 1.0043 acc_val: 0.6958 time: 3.2282s\n",
      "Test set results: loss= 1.1845 accuracy= 0.6672 1-hop accuracy = 0.9350\n",
      "Epoch: 0152 loss_train: 1.0043 acc_train: 0.6958 loss_val: 1.0001 acc_val: 0.6986 time: 3.2360s\n",
      "Test set results: loss= 1.1830 accuracy= 0.6664 1-hop accuracy = 0.9355\n",
      "Epoch: 0153 loss_train: 1.0001 acc_train: 0.6986 loss_val: 0.9959 acc_val: 0.7004 time: 3.2116s\n",
      "Test set results: loss= 1.1778 accuracy= 0.6762 1-hop accuracy = 0.9434\n",
      "Epoch: 0154 loss_train: 0.9959 acc_train: 0.7004 loss_val: 0.9910 acc_val: 0.6966 time: 3.2700s\n",
      "Test set results: loss= 1.1780 accuracy= 0.6699 1-hop accuracy = 0.9395\n",
      "Epoch: 0155 loss_train: 0.9910 acc_train: 0.6966 loss_val: 0.9867 acc_val: 0.7022 time: 3.2563s\n",
      "Test set results: loss= 1.1767 accuracy= 0.6708 1-hop accuracy = 0.9411\n",
      "Epoch: 0156 loss_train: 0.9867 acc_train: 0.7022 loss_val: 0.9825 acc_val: 0.7035 time: 3.2093s\n",
      "Test set results: loss= 1.1742 accuracy= 0.6769 1-hop accuracy = 0.9433\n",
      "Epoch: 0157 loss_train: 0.9825 acc_train: 0.7035 loss_val: 0.9780 acc_val: 0.7008 time: 3.2459s\n",
      "Test set results: loss= 1.1729 accuracy= 0.6715 1-hop accuracy = 0.9426\n",
      "Epoch: 0158 loss_train: 0.9780 acc_train: 0.7008 loss_val: 0.9736 acc_val: 0.7036 time: 3.1844s\n",
      "Test set results: loss= 1.1702 accuracy= 0.6775 1-hop accuracy = 0.9453\n",
      "Epoch: 0159 loss_train: 0.9736 acc_train: 0.7036 loss_val: 0.9695 acc_val: 0.7062 time: 3.2448s\n",
      "Test set results: loss= 1.1687 accuracy= 0.6788 1-hop accuracy = 0.9452\n",
      "Epoch: 0160 loss_train: 0.9695 acc_train: 0.7062 loss_val: 0.9655 acc_val: 0.7044 time: 3.2586s\n",
      "Test set results: loss= 1.1683 accuracy= 0.6734 1-hop accuracy = 0.9460\n",
      "Epoch: 0161 loss_train: 0.9655 acc_train: 0.7044 loss_val: 0.9612 acc_val: 0.7040 time: 3.2086s\n",
      "Test set results: loss= 1.1645 accuracy= 0.6802 1-hop accuracy = 0.9474\n",
      "Epoch: 0162 loss_train: 0.9612 acc_train: 0.7040 loss_val: 0.9571 acc_val: 0.7069 time: 3.2373s\n",
      "Test set results: loss= 1.1632 accuracy= 0.6812 1-hop accuracy = 0.9474\n",
      "Epoch: 0163 loss_train: 0.9571 acc_train: 0.7069 loss_val: 0.9534 acc_val: 0.7087 time: 3.2375s\n",
      "Test set results: loss= 1.1633 accuracy= 0.6765 1-hop accuracy = 0.9484\n",
      "Epoch: 0164 loss_train: 0.9534 acc_train: 0.7087 loss_val: 0.9491 acc_val: 0.7096 time: 3.3422s\n",
      "Test set results: loss= 1.1590 accuracy= 0.6817 1-hop accuracy = 0.9501\n",
      "Epoch: 0165 loss_train: 0.9491 acc_train: 0.7096 loss_val: 0.9454 acc_val: 0.7100 time: 3.2366s\n",
      "Test set results: loss= 1.1589 accuracy= 0.6808 1-hop accuracy = 0.9509\n",
      "Epoch: 0166 loss_train: 0.9454 acc_train: 0.7100 loss_val: 0.9418 acc_val: 0.7139 time: 3.2383s\n",
      "Test set results: loss= 1.1584 accuracy= 0.6823 1-hop accuracy = 0.9500\n",
      "Epoch: 0167 loss_train: 0.9418 acc_train: 0.7139 loss_val: 0.9378 acc_val: 0.7096 time: 3.2373s\n",
      "Test set results: loss= 1.1563 accuracy= 0.6827 1-hop accuracy = 0.9514\n",
      "Epoch: 0168 loss_train: 0.9378 acc_train: 0.7096 loss_val: 0.9339 acc_val: 0.7127 time: 3.2957s\n",
      "Test set results: loss= 1.1535 accuracy= 0.6835 1-hop accuracy = 0.9537\n",
      "Epoch: 0169 loss_train: 0.9339 acc_train: 0.7127 loss_val: 0.9299 acc_val: 0.7136 time: 3.2909s\n",
      "Test set results: loss= 1.1555 accuracy= 0.6838 1-hop accuracy = 0.9529\n",
      "Epoch: 0170 loss_train: 0.9299 acc_train: 0.7136 loss_val: 0.9265 acc_val: 0.7132 time: 3.3402s\n",
      "Test set results: loss= 1.1541 accuracy= 0.6843 1-hop accuracy = 0.9529\n",
      "Epoch: 0171 loss_train: 0.9265 acc_train: 0.7132 loss_val: 0.9226 acc_val: 0.7124 time: 3.2889s\n",
      "Test set results: loss= 1.1516 accuracy= 0.6871 1-hop accuracy = 0.9562\n",
      "Epoch: 0172 loss_train: 0.9226 acc_train: 0.7124 loss_val: 0.9193 acc_val: 0.7133 time: 3.2659s\n",
      "Test set results: loss= 1.1486 accuracy= 0.6842 1-hop accuracy = 0.9538\n",
      "Epoch: 0173 loss_train: 0.9193 acc_train: 0.7133 loss_val: 0.9155 acc_val: 0.7145 time: 3.2899s\n",
      "Test set results: loss= 1.1517 accuracy= 0.6855 1-hop accuracy = 0.9530\n",
      "Epoch: 0174 loss_train: 0.9155 acc_train: 0.7145 loss_val: 0.9123 acc_val: 0.7163 time: 3.2619s\n",
      "Test set results: loss= 1.1485 accuracy= 0.6864 1-hop accuracy = 0.9552\n",
      "Epoch: 0175 loss_train: 0.9123 acc_train: 0.7163 loss_val: 0.9086 acc_val: 0.7129 time: 3.2724s\n",
      "Test set results: loss= 1.1471 accuracy= 0.6862 1-hop accuracy = 0.9553\n",
      "Epoch: 0176 loss_train: 0.9086 acc_train: 0.7129 loss_val: 0.9053 acc_val: 0.7167 time: 3.2627s\n",
      "Test set results: loss= 1.1457 accuracy= 0.6877 1-hop accuracy = 0.9560\n",
      "Epoch: 0177 loss_train: 0.9053 acc_train: 0.7167 loss_val: 0.9020 acc_val: 0.7152 time: 3.2616s\n",
      "Test set results: loss= 1.1475 accuracy= 0.6860 1-hop accuracy = 0.9560\n",
      "Epoch: 0178 loss_train: 0.9020 acc_train: 0.7152 loss_val: 0.8990 acc_val: 0.7163 time: 3.2684s\n",
      "Test set results: loss= 1.1443 accuracy= 0.6885 1-hop accuracy = 0.9569\n",
      "Epoch: 0179 loss_train: 0.8990 acc_train: 0.7163 loss_val: 0.8963 acc_val: 0.7129 time: 3.2422s\n",
      "Test set results: loss= 1.1437 accuracy= 0.6854 1-hop accuracy = 0.9554\n",
      "Epoch: 0180 loss_train: 0.8963 acc_train: 0.7129 loss_val: 0.8938 acc_val: 0.7145 time: 3.2612s\n",
      "Test set results: loss= 1.1392 accuracy= 0.6897 1-hop accuracy = 0.9553\n",
      "Epoch: 0181 loss_train: 0.8938 acc_train: 0.7145 loss_val: 0.8916 acc_val: 0.7131 time: 3.2526s\n",
      "Test set results: loss= 1.1455 accuracy= 0.6885 1-hop accuracy = 0.9572\n",
      "Epoch: 0182 loss_train: 0.8916 acc_train: 0.7131 loss_val: 0.8882 acc_val: 0.7148 time: 3.2600s\n",
      "Test set results: loss= 1.1397 accuracy= 0.6912 1-hop accuracy = 0.9574\n",
      "Epoch: 0183 loss_train: 0.8882 acc_train: 0.7148 loss_val: 0.8843 acc_val: 0.7160 time: 3.2765s\n",
      "Test set results: loss= 1.1429 accuracy= 0.6839 1-hop accuracy = 0.9565\n",
      "Epoch: 0184 loss_train: 0.8843 acc_train: 0.7160 loss_val: 0.8802 acc_val: 0.7174 time: 3.2495s\n",
      "Test set results: loss= 1.1364 accuracy= 0.6896 1-hop accuracy = 0.9567\n",
      "Epoch: 0185 loss_train: 0.8802 acc_train: 0.7174 loss_val: 0.8764 acc_val: 0.7153 time: 3.2854s\n",
      "Test set results: loss= 1.1405 accuracy= 0.6882 1-hop accuracy = 0.9592\n",
      "Epoch: 0186 loss_train: 0.8764 acc_train: 0.7153 loss_val: 0.8734 acc_val: 0.7163 time: 3.2322s\n",
      "Test set results: loss= 1.1368 accuracy= 0.6884 1-hop accuracy = 0.9566\n",
      "Epoch: 0187 loss_train: 0.8734 acc_train: 0.7163 loss_val: 0.8706 acc_val: 0.7173 time: 3.2835s\n",
      "Test set results: loss= 1.1374 accuracy= 0.6947 1-hop accuracy = 0.9622\n",
      "Epoch: 0188 loss_train: 0.8706 acc_train: 0.7173 loss_val: 0.8678 acc_val: 0.7167 time: 3.2336s\n",
      "Test set results: loss= 1.1359 accuracy= 0.6898 1-hop accuracy = 0.9582\n",
      "Epoch: 0189 loss_train: 0.8678 acc_train: 0.7167 loss_val: 0.8644 acc_val: 0.7189 time: 3.2650s\n",
      "Test set results: loss= 1.1336 accuracy= 0.6954 1-hop accuracy = 0.9607\n",
      "Epoch: 0190 loss_train: 0.8644 acc_train: 0.7189 loss_val: 0.8609 acc_val: 0.7215 time: 3.2700s\n",
      "Test set results: loss= 1.1343 accuracy= 0.6940 1-hop accuracy = 0.9600\n",
      "Epoch: 0191 loss_train: 0.8609 acc_train: 0.7215 loss_val: 0.8581 acc_val: 0.7203 time: 3.2638s\n",
      "Test set results: loss= 1.1320 accuracy= 0.6917 1-hop accuracy = 0.9574\n",
      "Epoch: 0192 loss_train: 0.8581 acc_train: 0.7203 loss_val: 0.8554 acc_val: 0.7208 time: 3.3156s\n",
      "Test set results: loss= 1.1340 accuracy= 0.6959 1-hop accuracy = 0.9636\n",
      "Epoch: 0193 loss_train: 0.8554 acc_train: 0.7208 loss_val: 0.8528 acc_val: 0.7193 time: 3.2804s\n",
      "Test set results: loss= 1.1302 accuracy= 0.6929 1-hop accuracy = 0.9600\n",
      "Epoch: 0194 loss_train: 0.8528 acc_train: 0.7193 loss_val: 0.8497 acc_val: 0.7225 time: 3.2610s\n",
      "Test set results: loss= 1.1332 accuracy= 0.6964 1-hop accuracy = 0.9630\n",
      "Epoch: 0195 loss_train: 0.8497 acc_train: 0.7225 loss_val: 0.8469 acc_val: 0.7212 time: 3.2849s\n",
      "Test set results: loss= 1.1295 accuracy= 0.6967 1-hop accuracy = 0.9622\n",
      "Epoch: 0196 loss_train: 0.8469 acc_train: 0.7212 loss_val: 0.8439 acc_val: 0.7215 time: 3.3308s\n",
      "Test set results: loss= 1.1308 accuracy= 0.6954 1-hop accuracy = 0.9612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0197 loss_train: 0.8439 acc_train: 0.7215 loss_val: 0.8412 acc_val: 0.7249 time: 3.3412s\n",
      "Test set results: loss= 1.1295 accuracy= 0.6958 1-hop accuracy = 0.9629\n",
      "Epoch: 0198 loss_train: 0.8412 acc_train: 0.7249 loss_val: 0.8386 acc_val: 0.7239 time: 3.3093s\n",
      "Test set results: loss= 1.1289 accuracy= 0.6940 1-hop accuracy = 0.9596\n",
      "Epoch: 0199 loss_train: 0.8386 acc_train: 0.7239 loss_val: 0.8358 acc_val: 0.7264 time: 3.3098s\n",
      "Test set results: loss= 1.1289 accuracy= 0.6972 1-hop accuracy = 0.9633\n",
      "Epoch: 0200 loss_train: 0.8358 acc_train: 0.7264 loss_val: 0.8332 acc_val: 0.7243 time: 3.3061s\n",
      "Test set results: loss= 1.1265 accuracy= 0.6955 1-hop accuracy = 0.9626\n",
      "Epoch: 0201 loss_train: 0.8332 acc_train: 0.7243 loss_val: 0.8306 acc_val: 0.7264 time: 3.3078s\n",
      "Test set results: loss= 1.1293 accuracy= 0.6954 1-hop accuracy = 0.9619\n",
      "Epoch: 0202 loss_train: 0.8306 acc_train: 0.7264 loss_val: 0.8281 acc_val: 0.7269 time: 3.2950s\n",
      "Test set results: loss= 1.1258 accuracy= 0.6959 1-hop accuracy = 0.9622\n",
      "Epoch: 0203 loss_train: 0.8281 acc_train: 0.7269 loss_val: 0.8257 acc_val: 0.7255 time: 3.3072s\n",
      "Test set results: loss= 1.1270 accuracy= 0.6962 1-hop accuracy = 0.9628\n",
      "Epoch: 0204 loss_train: 0.8257 acc_train: 0.7255 loss_val: 0.8231 acc_val: 0.7279 time: 3.3395s\n",
      "Test set results: loss= 1.1261 accuracy= 0.6967 1-hop accuracy = 0.9619\n",
      "Epoch: 0205 loss_train: 0.8231 acc_train: 0.7279 loss_val: 0.8206 acc_val: 0.7261 time: 3.3302s\n",
      "Test set results: loss= 1.1267 accuracy= 0.6947 1-hop accuracy = 0.9626\n",
      "Epoch: 0206 loss_train: 0.8206 acc_train: 0.7261 loss_val: 0.8183 acc_val: 0.7274 time: 3.3068s\n",
      "Test set results: loss= 1.1240 accuracy= 0.6955 1-hop accuracy = 0.9623\n",
      "Epoch: 0207 loss_train: 0.8183 acc_train: 0.7274 loss_val: 0.8160 acc_val: 0.7297 time: 3.2875s\n",
      "Test set results: loss= 1.1278 accuracy= 0.6956 1-hop accuracy = 0.9634\n",
      "Epoch: 0208 loss_train: 0.8160 acc_train: 0.7297 loss_val: 0.8139 acc_val: 0.7261 time: 3.2944s\n",
      "Test set results: loss= 1.1227 accuracy= 0.6966 1-hop accuracy = 0.9615\n",
      "Epoch: 0209 loss_train: 0.8139 acc_train: 0.7261 loss_val: 0.8118 acc_val: 0.7273 time: 3.2707s\n",
      "Test set results: loss= 1.1285 accuracy= 0.6930 1-hop accuracy = 0.9623\n",
      "Epoch: 0210 loss_train: 0.8118 acc_train: 0.7273 loss_val: 0.8100 acc_val: 0.7269 time: 3.3023s\n",
      "Test set results: loss= 1.1211 accuracy= 0.6972 1-hop accuracy = 0.9618\n",
      "Epoch: 0211 loss_train: 0.8100 acc_train: 0.7269 loss_val: 0.8087 acc_val: 0.7268 time: 3.2984s\n",
      "Test set results: loss= 1.1302 accuracy= 0.6874 1-hop accuracy = 0.9629\n",
      "Epoch: 0212 loss_train: 0.8087 acc_train: 0.7268 loss_val: 0.8076 acc_val: 0.7252 time: 3.2945s\n",
      "Test set results: loss= 1.1218 accuracy= 0.6939 1-hop accuracy = 0.9598\n",
      "Epoch: 0213 loss_train: 0.8076 acc_train: 0.7252 loss_val: 0.8062 acc_val: 0.7267 time: 3.4361s\n",
      "Test set results: loss= 1.1301 accuracy= 0.6874 1-hop accuracy = 0.9636\n",
      "Epoch: 0214 loss_train: 0.8062 acc_train: 0.7267 loss_val: 0.8037 acc_val: 0.7261 time: 3.3409s\n",
      "Test set results: loss= 1.1202 accuracy= 0.6956 1-hop accuracy = 0.9599\n",
      "Epoch: 0215 loss_train: 0.8037 acc_train: 0.7261 loss_val: 0.8000 acc_val: 0.7300 time: 3.2947s\n",
      "Test set results: loss= 1.1310 accuracy= 0.6886 1-hop accuracy = 0.9629\n",
      "Epoch: 0216 loss_train: 0.8000 acc_train: 0.7300 loss_val: 0.7960 acc_val: 0.7311 time: 3.3557s\n",
      "Test set results: loss= 1.1201 accuracy= 0.6964 1-hop accuracy = 0.9619\n",
      "Epoch: 0217 loss_train: 0.7960 acc_train: 0.7311 loss_val: 0.7931 acc_val: 0.7319 time: 3.2718s\n",
      "Test set results: loss= 1.1223 accuracy= 0.6962 1-hop accuracy = 0.9629\n",
      "Epoch: 0218 loss_train: 0.7931 acc_train: 0.7319 loss_val: 0.7914 acc_val: 0.7309 time: 3.3159s\n",
      "Test set results: loss= 1.1254 accuracy= 0.6957 1-hop accuracy = 0.9640\n",
      "Epoch: 0219 loss_train: 0.7914 acc_train: 0.7309 loss_val: 0.7903 acc_val: 0.7319 time: 3.3154s\n",
      "Test set results: loss= 1.1198 accuracy= 0.6948 1-hop accuracy = 0.9604\n",
      "Epoch: 0220 loss_train: 0.7903 acc_train: 0.7319 loss_val: 0.7889 acc_val: 0.7308 time: 3.3589s\n",
      "Test set results: loss= 1.1275 accuracy= 0.6922 1-hop accuracy = 0.9623\n",
      "Epoch: 0221 loss_train: 0.7889 acc_train: 0.7308 loss_val: 0.7867 acc_val: 0.7324 time: 3.3334s\n",
      "Test set results: loss= 1.1208 accuracy= 0.6973 1-hop accuracy = 0.9605\n",
      "Epoch: 0222 loss_train: 0.7867 acc_train: 0.7324 loss_val: 0.7837 acc_val: 0.7326 time: 3.3600s\n",
      "Test set results: loss= 1.1252 accuracy= 0.6939 1-hop accuracy = 0.9624\n",
      "Epoch: 0223 loss_train: 0.7837 acc_train: 0.7326 loss_val: 0.7810 acc_val: 0.7341 time: 3.3117s\n",
      "Test set results: loss= 1.1207 accuracy= 0.6956 1-hop accuracy = 0.9622\n",
      "Epoch: 0224 loss_train: 0.7810 acc_train: 0.7341 loss_val: 0.7789 acc_val: 0.7338 time: 3.3217s\n",
      "Test set results: loss= 1.1220 accuracy= 0.6975 1-hop accuracy = 0.9610\n",
      "Epoch: 0225 loss_train: 0.7789 acc_train: 0.7338 loss_val: 0.7775 acc_val: 0.7328 time: 3.2954s\n",
      "Test set results: loss= 1.1249 accuracy= 0.6953 1-hop accuracy = 0.9630\n",
      "Epoch: 0226 loss_train: 0.7775 acc_train: 0.7328 loss_val: 0.7761 acc_val: 0.7312 time: 3.3488s\n",
      "Test set results: loss= 1.1205 accuracy= 0.6964 1-hop accuracy = 0.9614\n",
      "Epoch: 0227 loss_train: 0.7761 acc_train: 0.7312 loss_val: 0.7741 acc_val: 0.7344 time: 3.3336s\n",
      "Test set results: loss= 1.1251 accuracy= 0.6938 1-hop accuracy = 0.9629\n",
      "Epoch: 0228 loss_train: 0.7741 acc_train: 0.7344 loss_val: 0.7718 acc_val: 0.7337 time: 3.3075s\n",
      "Test set results: loss= 1.1210 accuracy= 0.6969 1-hop accuracy = 0.9607\n",
      "Epoch: 0229 loss_train: 0.7718 acc_train: 0.7337 loss_val: 0.7693 acc_val: 0.7357 time: 3.3385s\n",
      "Test set results: loss= 1.1235 accuracy= 0.6971 1-hop accuracy = 0.9618\n",
      "Epoch: 0230 loss_train: 0.7693 acc_train: 0.7357 loss_val: 0.7673 acc_val: 0.7356 time: 3.3270s\n",
      "Test set results: loss= 1.1204 accuracy= 0.6964 1-hop accuracy = 0.9627\n",
      "Epoch: 0231 loss_train: 0.7673 acc_train: 0.7356 loss_val: 0.7655 acc_val: 0.7354 time: 3.3567s\n",
      "Test set results: loss= 1.1241 accuracy= 0.6963 1-hop accuracy = 0.9619\n",
      "Epoch: 0232 loss_train: 0.7655 acc_train: 0.7354 loss_val: 0.7640 acc_val: 0.7350 time: 3.3043s\n",
      "Test set results: loss= 1.1219 accuracy= 0.6953 1-hop accuracy = 0.9628\n",
      "Epoch: 0233 loss_train: 0.7640 acc_train: 0.7350 loss_val: 0.7624 acc_val: 0.7336 time: 3.3573s\n",
      "Test set results: loss= 1.1224 accuracy= 0.6959 1-hop accuracy = 0.9609\n",
      "Epoch: 0234 loss_train: 0.7624 acc_train: 0.7336 loss_val: 0.7606 acc_val: 0.7356 time: 3.3842s\n",
      "Test set results: loss= 1.1250 accuracy= 0.6943 1-hop accuracy = 0.9622\n",
      "Epoch: 0235 loss_train: 0.7606 acc_train: 0.7356 loss_val: 0.7587 acc_val: 0.7373 time: 3.3561s\n",
      "Test set results: loss= 1.1194 accuracy= 0.6967 1-hop accuracy = 0.9618\n",
      "Epoch: 0236 loss_train: 0.7587 acc_train: 0.7373 loss_val: 0.7566 acc_val: 0.7380 time: 3.3371s\n",
      "Test set results: loss= 1.1255 accuracy= 0.6949 1-hop accuracy = 0.9612\n",
      "Epoch: 0237 loss_train: 0.7566 acc_train: 0.7380 loss_val: 0.7546 acc_val: 0.7373 time: 3.3281s\n",
      "Test set results: loss= 1.1208 accuracy= 0.6962 1-hop accuracy = 0.9618\n",
      "Epoch: 0238 loss_train: 0.7546 acc_train: 0.7373 loss_val: 0.7528 acc_val: 0.7395 time: 3.4711s\n",
      "Test set results: loss= 1.1237 accuracy= 0.6966 1-hop accuracy = 0.9618\n",
      "Epoch: 0239 loss_train: 0.7528 acc_train: 0.7395 loss_val: 0.7510 acc_val: 0.7394 time: 3.4114s\n",
      "Test set results: loss= 1.1213 accuracy= 0.6960 1-hop accuracy = 0.9624\n",
      "Epoch: 0240 loss_train: 0.7510 acc_train: 0.7394 loss_val: 0.7494 acc_val: 0.7377 time: 3.3537s\n",
      "Test set results: loss= 1.1232 accuracy= 0.6963 1-hop accuracy = 0.9608\n",
      "Epoch: 0241 loss_train: 0.7494 acc_train: 0.7377 loss_val: 0.7479 acc_val: 0.7388 time: 3.3745s\n",
      "Test set results: loss= 1.1243 accuracy= 0.6946 1-hop accuracy = 0.9630\n",
      "Epoch: 0242 loss_train: 0.7479 acc_train: 0.7388 loss_val: 0.7465 acc_val: 0.7364 time: 3.3996s\n",
      "Test set results: loss= 1.1236 accuracy= 0.6961 1-hop accuracy = 0.9607\n",
      "Epoch: 0243 loss_train: 0.7465 acc_train: 0.7364 loss_val: 0.7452 acc_val: 0.7419 time: 3.3640s\n",
      "Test set results: loss= 1.1259 accuracy= 0.6952 1-hop accuracy = 0.9631\n",
      "Epoch: 0244 loss_train: 0.7452 acc_train: 0.7419 loss_val: 0.7439 acc_val: 0.7376 time: 3.3214s\n",
      "Test set results: loss= 1.1232 accuracy= 0.6967 1-hop accuracy = 0.9611\n",
      "Epoch: 0245 loss_train: 0.7439 acc_train: 0.7376 loss_val: 0.7424 acc_val: 0.7431 time: 3.3883s\n",
      "Test set results: loss= 1.1257 accuracy= 0.6951 1-hop accuracy = 0.9626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0246 loss_train: 0.7424 acc_train: 0.7431 loss_val: 0.7409 acc_val: 0.7394 time: 3.3536s\n",
      "Test set results: loss= 1.1258 accuracy= 0.6962 1-hop accuracy = 0.9609\n",
      "Epoch: 0247 loss_train: 0.7409 acc_train: 0.7394 loss_val: 0.7390 acc_val: 0.7408 time: 3.3552s\n",
      "Test set results: loss= 1.1246 accuracy= 0.6963 1-hop accuracy = 0.9629\n",
      "Epoch: 0248 loss_train: 0.7390 acc_train: 0.7408 loss_val: 0.7373 acc_val: 0.7400 time: 3.3431s\n",
      "Test set results: loss= 1.1283 accuracy= 0.6950 1-hop accuracy = 0.9605\n",
      "Epoch: 0249 loss_train: 0.7373 acc_train: 0.7400 loss_val: 0.7360 acc_val: 0.7392 time: 3.3314s\n",
      "Test set results: loss= 1.1226 accuracy= 0.6969 1-hop accuracy = 0.9620\n",
      "Epoch: 0250 loss_train: 0.7360 acc_train: 0.7392 loss_val: 0.7349 acc_val: 0.7422 time: 3.3618s\n",
      "Test set results: loss= 1.1318 accuracy= 0.6944 1-hop accuracy = 0.9597\n",
      "Epoch: 0251 loss_train: 0.7349 acc_train: 0.7422 loss_val: 0.7339 acc_val: 0.7381 time: 3.3420s\n",
      "Test set results: loss= 1.1220 accuracy= 0.6979 1-hop accuracy = 0.9611\n",
      "Epoch: 0252 loss_train: 0.7339 acc_train: 0.7381 loss_val: 0.7329 acc_val: 0.7424 time: 3.4152s\n",
      "Test set results: loss= 1.1342 accuracy= 0.6940 1-hop accuracy = 0.9609\n",
      "Epoch: 0253 loss_train: 0.7329 acc_train: 0.7424 loss_val: 0.7324 acc_val: 0.7382 time: 3.4288s\n",
      "Test set results: loss= 1.1232 accuracy= 0.6972 1-hop accuracy = 0.9604\n",
      "Epoch: 0254 loss_train: 0.7324 acc_train: 0.7382 loss_val: 0.7317 acc_val: 0.7404 time: 3.3360s\n",
      "Test set results: loss= 1.1349 accuracy= 0.6940 1-hop accuracy = 0.9648\n",
      "Epoch: 0255 loss_train: 0.7317 acc_train: 0.7404 loss_val: 0.7302 acc_val: 0.7426 time: 3.4194s\n",
      "Test set results: loss= 1.1266 accuracy= 0.6965 1-hop accuracy = 0.9614\n",
      "Epoch: 0256 loss_train: 0.7302 acc_train: 0.7426 loss_val: 0.7276 acc_val: 0.7424 time: 3.3993s\n",
      "Test set results: loss= 1.1352 accuracy= 0.6932 1-hop accuracy = 0.9656\n",
      "Epoch: 0257 loss_train: 0.7276 acc_train: 0.7424 loss_val: 0.7252 acc_val: 0.7403 time: 3.3725s\n",
      "Test set results: loss= 1.1292 accuracy= 0.6956 1-hop accuracy = 0.9594\n",
      "Epoch: 0258 loss_train: 0.7252 acc_train: 0.7403 loss_val: 0.7224 acc_val: 0.7422 time: 3.3537s\n",
      "Test set results: loss= 1.1270 accuracy= 0.6949 1-hop accuracy = 0.9620\n",
      "Epoch: 0259 loss_train: 0.7224 acc_train: 0.7422 loss_val: 0.7206 acc_val: 0.7428 time: 3.3887s\n",
      "Test set results: loss= 1.1317 accuracy= 0.6961 1-hop accuracy = 0.9595\n",
      "Epoch: 0260 loss_train: 0.7206 acc_train: 0.7428 loss_val: 0.7198 acc_val: 0.7432 time: 3.3430s\n",
      "Test set results: loss= 1.1273 accuracy= 0.6967 1-hop accuracy = 0.9610\n",
      "Epoch: 0261 loss_train: 0.7198 acc_train: 0.7432 loss_val: 0.7191 acc_val: 0.7435 time: 3.4511s\n",
      "Test set results: loss= 1.1355 accuracy= 0.6959 1-hop accuracy = 0.9619\n",
      "Epoch: 0262 loss_train: 0.7191 acc_train: 0.7435 loss_val: 0.7182 acc_val: 0.7435 time: 3.4393s\n",
      "Test set results: loss= 1.1283 accuracy= 0.6983 1-hop accuracy = 0.9609\n",
      "Epoch: 0263 loss_train: 0.7182 acc_train: 0.7435 loss_val: 0.7163 acc_val: 0.7478 time: 3.3544s\n",
      "Test set results: loss= 1.1336 accuracy= 0.6938 1-hop accuracy = 0.9635\n",
      "Epoch: 0264 loss_train: 0.7163 acc_train: 0.7478 loss_val: 0.7141 acc_val: 0.7433 time: 3.3528s\n",
      "Test set results: loss= 1.1293 accuracy= 0.6980 1-hop accuracy = 0.9614\n",
      "Epoch: 0265 loss_train: 0.7141 acc_train: 0.7433 loss_val: 0.7123 acc_val: 0.7442 time: 3.3958s\n",
      "Test set results: loss= 1.1321 accuracy= 0.6953 1-hop accuracy = 0.9610\n",
      "Epoch: 0266 loss_train: 0.7123 acc_train: 0.7442 loss_val: 0.7109 acc_val: 0.7445 time: 3.3477s\n",
      "Test set results: loss= 1.1326 accuracy= 0.6971 1-hop accuracy = 0.9615\n",
      "Epoch: 0267 loss_train: 0.7109 acc_train: 0.7445 loss_val: 0.7096 acc_val: 0.7460 time: 3.4054s\n",
      "Test set results: loss= 1.1301 accuracy= 0.6973 1-hop accuracy = 0.9622\n",
      "Epoch: 0268 loss_train: 0.7096 acc_train: 0.7460 loss_val: 0.7087 acc_val: 0.7453 time: 3.3980s\n",
      "Test set results: loss= 1.1333 accuracy= 0.6961 1-hop accuracy = 0.9611\n",
      "Epoch: 0269 loss_train: 0.7087 acc_train: 0.7453 loss_val: 0.7080 acc_val: 0.7457 time: 3.4344s\n",
      "Test set results: loss= 1.1330 accuracy= 0.6961 1-hop accuracy = 0.9597\n",
      "Epoch: 0270 loss_train: 0.7080 acc_train: 0.7457 loss_val: 0.7065 acc_val: 0.7470 time: 3.4002s\n",
      "Test set results: loss= 1.1362 accuracy= 0.6960 1-hop accuracy = 0.9615\n",
      "Epoch: 0271 loss_train: 0.7065 acc_train: 0.7470 loss_val: 0.7052 acc_val: 0.7484 time: 3.3576s\n",
      "Test set results: loss= 1.1308 accuracy= 0.6962 1-hop accuracy = 0.9607\n",
      "Epoch: 0272 loss_train: 0.7052 acc_train: 0.7484 loss_val: 0.7030 acc_val: 0.7482 time: 3.4074s\n",
      "Test set results: loss= 1.1367 accuracy= 0.6965 1-hop accuracy = 0.9636\n",
      "Epoch: 0273 loss_train: 0.7030 acc_train: 0.7482 loss_val: 0.7017 acc_val: 0.7457 time: 3.3743s\n",
      "Test set results: loss= 1.1344 accuracy= 0.6968 1-hop accuracy = 0.9614\n",
      "Epoch: 0274 loss_train: 0.7017 acc_train: 0.7457 loss_val: 0.7006 acc_val: 0.7489 time: 3.4283s\n",
      "Test set results: loss= 1.1346 accuracy= 0.6963 1-hop accuracy = 0.9618\n",
      "Epoch: 0275 loss_train: 0.7006 acc_train: 0.7489 loss_val: 0.6995 acc_val: 0.7473 time: 3.3548s\n",
      "Test set results: loss= 1.1372 accuracy= 0.6949 1-hop accuracy = 0.9607\n",
      "Epoch: 0276 loss_train: 0.6995 acc_train: 0.7473 loss_val: 0.6983 acc_val: 0.7469 time: 3.4243s\n",
      "Test set results: loss= 1.1359 accuracy= 0.6970 1-hop accuracy = 0.9617\n",
      "Epoch: 0277 loss_train: 0.6983 acc_train: 0.7469 loss_val: 0.6969 acc_val: 0.7493 time: 3.4506s\n",
      "Test set results: loss= 1.1392 accuracy= 0.6969 1-hop accuracy = 0.9616\n",
      "Epoch: 0278 loss_train: 0.6969 acc_train: 0.7493 loss_val: 0.6956 acc_val: 0.7472 time: 3.4115s\n",
      "Test set results: loss= 1.1363 accuracy= 0.6973 1-hop accuracy = 0.9625\n",
      "Epoch: 0279 loss_train: 0.6956 acc_train: 0.7472 loss_val: 0.6944 acc_val: 0.7511 time: 3.4339s\n",
      "Test set results: loss= 1.1411 accuracy= 0.6960 1-hop accuracy = 0.9617\n",
      "Epoch: 0280 loss_train: 0.6944 acc_train: 0.7511 loss_val: 0.6931 acc_val: 0.7480 time: 3.4207s\n",
      "Test set results: loss= 1.1351 accuracy= 0.6966 1-hop accuracy = 0.9624\n",
      "Epoch: 0281 loss_train: 0.6931 acc_train: 0.7480 loss_val: 0.6918 acc_val: 0.7501 time: 3.4152s\n",
      "Test set results: loss= 1.1414 accuracy= 0.6958 1-hop accuracy = 0.9605\n",
      "Epoch: 0282 loss_train: 0.6918 acc_train: 0.7501 loss_val: 0.6907 acc_val: 0.7505 time: 3.3897s\n",
      "Test set results: loss= 1.1392 accuracy= 0.6969 1-hop accuracy = 0.9609\n",
      "Epoch: 0283 loss_train: 0.6907 acc_train: 0.7505 loss_val: 0.6896 acc_val: 0.7477 time: 3.4817s\n",
      "Test set results: loss= 1.1415 accuracy= 0.6971 1-hop accuracy = 0.9623\n",
      "Epoch: 0284 loss_train: 0.6896 acc_train: 0.7477 loss_val: 0.6886 acc_val: 0.7498 time: 3.5443s\n",
      "Test set results: loss= 1.1399 accuracy= 0.6957 1-hop accuracy = 0.9633\n",
      "Epoch: 0285 loss_train: 0.6886 acc_train: 0.7498 loss_val: 0.6878 acc_val: 0.7516 time: 3.5029s\n",
      "Test set results: loss= 1.1428 accuracy= 0.6953 1-hop accuracy = 0.9600\n",
      "Epoch: 0286 loss_train: 0.6878 acc_train: 0.7516 loss_val: 0.6871 acc_val: 0.7509 time: 3.5101s\n",
      "Test set results: loss= 1.1413 accuracy= 0.6949 1-hop accuracy = 0.9634\n",
      "Epoch: 0287 loss_train: 0.6871 acc_train: 0.7509 loss_val: 0.6869 acc_val: 0.7515 time: 3.3837s\n",
      "Test set results: loss= 1.1443 accuracy= 0.6962 1-hop accuracy = 0.9611\n",
      "Epoch: 0288 loss_train: 0.6869 acc_train: 0.7515 loss_val: 0.6868 acc_val: 0.7516 time: 3.4113s\n",
      "Test set results: loss= 1.1447 accuracy= 0.6975 1-hop accuracy = 0.9645\n",
      "Epoch: 0289 loss_train: 0.6868 acc_train: 0.7516 loss_val: 0.6868 acc_val: 0.7510 time: 3.3901s\n",
      "Test set results: loss= 1.1459 accuracy= 0.6962 1-hop accuracy = 0.9618\n",
      "Epoch: 0290 loss_train: 0.6868 acc_train: 0.7510 loss_val: 0.6860 acc_val: 0.7488 time: 3.4471s\n",
      "Test set results: loss= 1.1461 accuracy= 0.6975 1-hop accuracy = 0.9633\n",
      "Epoch: 0291 loss_train: 0.6860 acc_train: 0.7488 loss_val: 0.6840 acc_val: 0.7511 time: 3.4108s\n",
      "Test set results: loss= 1.1463 accuracy= 0.6971 1-hop accuracy = 0.9609\n",
      "Epoch: 0292 loss_train: 0.6840 acc_train: 0.7511 loss_val: 0.6811 acc_val: 0.7505 time: 3.4132s\n",
      "Test set results: loss= 1.1477 accuracy= 0.6966 1-hop accuracy = 0.9616\n",
      "Epoch: 0293 loss_train: 0.6811 acc_train: 0.7505 loss_val: 0.6792 acc_val: 0.7528 time: 3.4844s\n",
      "Test set results: loss= 1.1437 accuracy= 0.6976 1-hop accuracy = 0.9619\n",
      "Epoch: 0294 loss_train: 0.6792 acc_train: 0.7528 loss_val: 0.6782 acc_val: 0.7511 time: 3.4140s\n",
      "Test set results: loss= 1.1492 accuracy= 0.6964 1-hop accuracy = 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0295 loss_train: 0.6782 acc_train: 0.7511 loss_val: 0.6778 acc_val: 0.7538 time: 3.4276s\n",
      "Test set results: loss= 1.1468 accuracy= 0.6981 1-hop accuracy = 0.9643\n",
      "Epoch: 0296 loss_train: 0.6778 acc_train: 0.7538 loss_val: 0.6776 acc_val: 0.7536 time: 3.4259s\n",
      "Test set results: loss= 1.1497 accuracy= 0.6953 1-hop accuracy = 0.9613\n",
      "Epoch: 0297 loss_train: 0.6776 acc_train: 0.7536 loss_val: 0.6760 acc_val: 0.7485 time: 3.5029s\n",
      "Test set results: loss= 1.1484 accuracy= 0.6993 1-hop accuracy = 0.9627\n",
      "Epoch: 0298 loss_train: 0.6760 acc_train: 0.7485 loss_val: 0.6740 acc_val: 0.7553 time: 3.4243s\n",
      "Test set results: loss= 1.1485 accuracy= 0.6961 1-hop accuracy = 0.9604\n",
      "Epoch: 0299 loss_train: 0.6740 acc_train: 0.7553 loss_val: 0.6728 acc_val: 0.7502 time: 3.4089s\n",
      "Test set results: loss= 1.1501 accuracy= 0.6967 1-hop accuracy = 0.9620\n",
      "Epoch: 0300 loss_train: 0.6728 acc_train: 0.7502 loss_val: 0.6720 acc_val: 0.7571 time: 3.4711s\n",
      "Test set results: loss= 1.1496 accuracy= 0.6967 1-hop accuracy = 0.9649\n",
      "Optimization Finished!\n",
      "The max acc is 0.6993\n",
      "The test max acc is 0.6993\n",
      "The test argmax acc is 296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a68464a30>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model  \n",
    "t_total = time.time()\n",
    "acc_epoch = []\n",
    "best_acc = 0\n",
    "start = time.time()\n",
    "for epoch in range(300):  \n",
    "    acc_train = train( model2,optimizer2,  features_X    , labels   , Adj, ind_train, fastmode, ind_train, batch_size)\n",
    "    acc_test, acc_hop = test(model2, features_X , labels ,A_119, Adj, ind_test)\n",
    "    acc_epoch.append(acc_test.numpy())\n",
    "    if acc_test.numpy() > best_acc:\n",
    "        best_acc = acc_test.numpy() \n",
    "        state = {'epoch': epoch + 1, 'state_dict': model2.state_dict() ,\n",
    "             'optimizer': optimizer2.state_dict() }\n",
    "        torch.save(state, savepath2)\n",
    "print(\"Optimization Finished!\") \n",
    "print('The max acc is %.4f' %np.max(acc_epoch))  \n",
    "print('The test max acc is %.4f' %np.max(acc_epoch))\n",
    "print('The test argmax acc is %2d' %np.argmax(acc_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
