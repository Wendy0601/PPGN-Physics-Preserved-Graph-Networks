{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os, scipy\n",
    "import torch   \n",
    "import scipy.sparse as sp \n",
    "import numpy as np  \n",
    "import torch.nn.functional as F \n",
    "import pickle\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt \n",
    "from math import *\n",
    "from numpy import transpose,matrix,exp,conj\n",
    "from numpy.linalg import inv \n",
    "from src_lwt.util_final import *\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import f1_score, accuracy_score,roc_auc_score\n",
    "import sklearn.metrics\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module \n",
    "from PPGN import *\n",
    "from load_data_37 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "root = \"./data/\"\n",
    "phase = 'ALL'\n",
    "dropout = 0 \n",
    "dropout_outlayer = 0  \n",
    "num_node = 36   \n",
    "dim_input = 6\n",
    "num_labelper =90\n",
    "agg_func = \"MEAN\"\n",
    "epochs =  100 \n",
    "b_sz = 32\n",
    "seed = 842 \n",
    "learn_method = 'sup' \n",
    "max_vali_f1 = 0  \n",
    "hidden_emb_size = [32,32,32] \n",
    "num_layers = len(hidden_emb_size) \n",
    "dataSet = 'loc'\n",
    "ds = dataSet\n",
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\") \n",
    "random.seed( seed)\n",
    "np.random.seed( seed)\n",
    "torch.manual_seed( seed)\n",
    "torch.cuda.manual_seed_all( seed)\n",
    "lr = 0.001\n",
    "weight_name = 'A_short' \n",
    "k=3 \n",
    "measured_index = [3,   31, 21, 27,7,23,2, 14,  15,17,35, 11,12,32 , 33]# [3, 4, 10, 34, 27,7,23,2, 14, 4,15,17,35, 1,32,33 ]#[0, 1, 3, 4, 7, 8, 9, 10, 13, 19, 26, 30, 34, 35]\n",
    "name_train = 'train_set_allV_37nodes_1pu.npz' \n",
    "name_test = 'test_set_allV_37nodes_1pu.npz'  \n",
    "dataC = dataCenter( num_labelper,  measured_index , seed = seed)     \n",
    "A , prob_A = select_A_prob(k, 'A_short')\n",
    "nodes_layers  = dic_nodes_neib(num_layers,A,prob_A )   \n",
    "modelname = str(num_labelper) + '_' + phase + '_' +str(len(measured_index)) + '_37node_suptrain.pkl'\n",
    "saveroot = \"./00_saved_final\"\n",
    "savepath = os.path.join(saveroot, modelname ) \n",
    "savebest = os.path.join(saveroot, learn_method + '_'+ str(num_labelper) + '_' + phase + '_' +str(len(measured_index)) + '_37node_suptrain.pkl') \n",
    "retrain =     False \n",
    "features, labels, ind_train ,  ind_test,     ind_measured= load_all_types( num_labelper ,measured_index,  seed = seed )\n",
    "dist_graph = np.load(os.path.join(root,'dist_graph_37nodes.npy')) \n",
    "neib = {}\n",
    "for node in range(dist_graph.shape[0]):\n",
    "    neib[node] =   np.array(np.where(dist_graph[node, :] > 0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "\n",
    "graphSage = GraphSage(num_layers, dim_input,hidden_emb_size  ,   A, prob_A, dropout = dropout ,device = device,    agg_method= agg_func  )\n",
    "graphSage.to(device) \n",
    "classification = Outlayer_fully( hidden_emb_size[-1],  num_node,  dropout = dropout_outlayer) \n",
    "classification.to(device)\n",
    "models = [graphSage, classification]\n",
    "params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params.append(param)\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001, weight_decay = 5e-3)  \n",
    "retrain = False\n",
    "if retrain:\n",
    "    models, optimizer, start_epoch = load_checkpoint(models, optimizer, savepath) \n",
    "    graphSage, classification  = models[0], models[1]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------EPOCH 0-----------------------\n",
      "Update the Whole GraphSage\n",
      " Step [0/304], Loss: 3.6253, Dealed Nodes [32/9720] \n",
      " Step [5/304], Loss: 3.6894, Dealed Nodes [192/9720] \n",
      " Step [10/304], Loss: 3.6293, Dealed Nodes [352/9720] \n",
      " Step [15/304], Loss: 3.5677, Dealed Nodes [512/9720] \n",
      " Step [20/304], Loss: 3.5181, Dealed Nodes [672/9720] \n",
      " Step [25/304], Loss: 3.5699, Dealed Nodes [832/9720] \n",
      " Step [30/304], Loss: 3.5062, Dealed Nodes [992/9720] \n",
      " Step [35/304], Loss: 3.5144, Dealed Nodes [1152/9720] \n",
      " Step [40/304], Loss: 3.5594, Dealed Nodes [1312/9720] \n",
      " Step [45/304], Loss: 3.4584, Dealed Nodes [1472/9720] \n",
      " Step [50/304], Loss: 3.4522, Dealed Nodes [1632/9720] \n",
      " Step [55/304], Loss: 3.3962, Dealed Nodes [1792/9720] \n",
      " Step [60/304], Loss: 3.3719, Dealed Nodes [1952/9720] \n",
      " Step [65/304], Loss: 3.2801, Dealed Nodes [2112/9720] \n",
      " Step [70/304], Loss: 3.1024, Dealed Nodes [2272/9720] \n",
      " Step [75/304], Loss: 3.1192, Dealed Nodes [2432/9720] \n",
      " Step [80/304], Loss: 2.8997, Dealed Nodes [2592/9720] \n",
      " Step [85/304], Loss: 3.2233, Dealed Nodes [2752/9720] \n",
      " Step [90/304], Loss: 3.1482, Dealed Nodes [2912/9720] \n",
      " Step [95/304], Loss: 3.0222, Dealed Nodes [3072/9720] \n",
      " Step [100/304], Loss: 3.4147, Dealed Nodes [3232/9720] \n",
      " Step [105/304], Loss: 3.0498, Dealed Nodes [3392/9720] \n",
      " Step [110/304], Loss: 3.1332, Dealed Nodes [3552/9720] \n",
      " Step [115/304], Loss: 3.2054, Dealed Nodes [3712/9720] \n",
      " Step [120/304], Loss: 2.8329, Dealed Nodes [3872/9720] \n",
      " Step [125/304], Loss: 3.2007, Dealed Nodes [4032/9720] \n",
      " Step [130/304], Loss: 3.0349, Dealed Nodes [4192/9720] \n",
      " Step [135/304], Loss: 2.9680, Dealed Nodes [4352/9720] \n",
      " Step [140/304], Loss: 2.8437, Dealed Nodes [4512/9720] \n",
      " Step [145/304], Loss: 2.8591, Dealed Nodes [4672/9720] \n",
      " Step [150/304], Loss: 2.9277, Dealed Nodes [4832/9720] \n",
      " Step [155/304], Loss: 2.6604, Dealed Nodes [4992/9720] \n",
      " Step [160/304], Loss: 2.9925, Dealed Nodes [5152/9720] \n",
      " Step [165/304], Loss: 2.8965, Dealed Nodes [5312/9720] \n",
      " Step [170/304], Loss: 2.5537, Dealed Nodes [5472/9720] \n",
      " Step [175/304], Loss: 2.4741, Dealed Nodes [5632/9720] \n",
      " Step [180/304], Loss: 2.6249, Dealed Nodes [5792/9720] \n",
      " Step [185/304], Loss: 2.7910, Dealed Nodes [5952/9720] \n",
      " Step [190/304], Loss: 2.3093, Dealed Nodes [6112/9720] \n",
      " Step [195/304], Loss: 2.3822, Dealed Nodes [6272/9720] \n",
      " Step [200/304], Loss: 2.4566, Dealed Nodes [6432/9720] \n",
      " Step [205/304], Loss: 2.5399, Dealed Nodes [6592/9720] \n",
      " Step [210/304], Loss: 2.5883, Dealed Nodes [6752/9720] \n",
      " Step [215/304], Loss: 2.2444, Dealed Nodes [6912/9720] \n",
      " Step [220/304], Loss: 2.5581, Dealed Nodes [7072/9720] \n",
      " Step [225/304], Loss: 2.2621, Dealed Nodes [7232/9720] \n",
      " Step [230/304], Loss: 2.1480, Dealed Nodes [7392/9720] \n",
      " Step [235/304], Loss: 2.2645, Dealed Nodes [7552/9720] \n",
      " Step [240/304], Loss: 2.5739, Dealed Nodes [7712/9720] \n",
      " Step [245/304], Loss: 2.1597, Dealed Nodes [7872/9720] \n",
      " Step [250/304], Loss: 2.0679, Dealed Nodes [8032/9720] \n",
      " Step [255/304], Loss: 2.1408, Dealed Nodes [8192/9720] \n",
      " Step [260/304], Loss: 2.0771, Dealed Nodes [8352/9720] \n",
      " Step [265/304], Loss: 2.4826, Dealed Nodes [8512/9720] \n",
      " Step [270/304], Loss: 2.1640, Dealed Nodes [8672/9720] \n",
      " Step [275/304], Loss: 1.9003, Dealed Nodes [8832/9720] \n",
      " Step [280/304], Loss: 1.8892, Dealed Nodes [8992/9720] \n",
      " Step [285/304], Loss: 2.2060, Dealed Nodes [9152/9720] \n",
      " Step [290/304], Loss: 2.0171, Dealed Nodes [9312/9720] \n",
      " Step [295/304], Loss: 2.0313, Dealed Nodes [9472/9720] \n",
      " Step [300/304], Loss: 1.9313, Dealed Nodes [9632/9720] \n",
      "Validation F1: 0.348559670781893\n",
      "Test F1: 0.3472222222222222\n",
      "----------------------EPOCH 1-----------------------\n",
      "Update the Whole GraphSage\n",
      " Step [0/304], Loss: 1.8810, Dealed Nodes [32/9720] \n",
      " Step [5/304], Loss: 1.9300, Dealed Nodes [192/9720] \n",
      " Step [10/304], Loss: 1.5183, Dealed Nodes [352/9720] \n",
      " Step [15/304], Loss: 1.9226, Dealed Nodes [512/9720] \n",
      " Step [20/304], Loss: 1.5574, Dealed Nodes [672/9720] \n",
      " Step [25/304], Loss: 1.7924, Dealed Nodes [832/9720] \n",
      " Step [30/304], Loss: 1.9366, Dealed Nodes [992/9720] \n",
      " Step [35/304], Loss: 1.7786, Dealed Nodes [1152/9720] \n",
      " Step [40/304], Loss: 1.6625, Dealed Nodes [1312/9720] \n",
      " Step [45/304], Loss: 1.5572, Dealed Nodes [1472/9720] \n",
      " Step [50/304], Loss: 1.7577, Dealed Nodes [1632/9720] \n",
      " Step [55/304], Loss: 1.6020, Dealed Nodes [1792/9720] \n",
      " Step [60/304], Loss: 1.6519, Dealed Nodes [1952/9720] \n",
      " Step [65/304], Loss: 1.7127, Dealed Nodes [2112/9720] \n",
      " Step [70/304], Loss: 1.5013, Dealed Nodes [2272/9720] \n",
      " Step [75/304], Loss: 1.5071, Dealed Nodes [2432/9720] \n",
      " Step [80/304], Loss: 1.6121, Dealed Nodes [2592/9720] \n",
      " Step [85/304], Loss: 1.7908, Dealed Nodes [2752/9720] \n",
      " Step [90/304], Loss: 1.5111, Dealed Nodes [2912/9720] \n",
      " Step [95/304], Loss: 1.8184, Dealed Nodes [3072/9720] \n",
      " Step [100/304], Loss: 1.4407, Dealed Nodes [3232/9720] \n",
      " Step [105/304], Loss: 1.9114, Dealed Nodes [3392/9720] \n",
      " Step [110/304], Loss: 1.6160, Dealed Nodes [3552/9720] \n",
      " Step [115/304], Loss: 1.4706, Dealed Nodes [3712/9720] \n",
      " Step [120/304], Loss: 1.5664, Dealed Nodes [3872/9720] \n",
      " Step [125/304], Loss: 1.5257, Dealed Nodes [4032/9720] \n",
      " Step [130/304], Loss: 1.2448, Dealed Nodes [4192/9720] \n",
      " Step [135/304], Loss: 1.2142, Dealed Nodes [4352/9720] \n",
      " Step [140/304], Loss: 1.5693, Dealed Nodes [4512/9720] \n",
      " Step [145/304], Loss: 1.3770, Dealed Nodes [4672/9720] \n",
      " Step [150/304], Loss: 1.4660, Dealed Nodes [4832/9720] \n",
      " Step [155/304], Loss: 1.5448, Dealed Nodes [4992/9720] \n",
      " Step [160/304], Loss: 1.3467, Dealed Nodes [5152/9720] \n",
      " Step [165/304], Loss: 1.4452, Dealed Nodes [5312/9720] \n",
      " Step [170/304], Loss: 1.3587, Dealed Nodes [5472/9720] \n",
      " Step [175/304], Loss: 1.3478, Dealed Nodes [5632/9720] \n",
      " Step [180/304], Loss: 1.4972, Dealed Nodes [5792/9720] \n",
      " Step [185/304], Loss: 1.4086, Dealed Nodes [5952/9720] \n",
      " Step [190/304], Loss: 1.4555, Dealed Nodes [6112/9720] \n",
      " Step [195/304], Loss: 1.3849, Dealed Nodes [6272/9720] \n",
      " Step [200/304], Loss: 1.3763, Dealed Nodes [6432/9720] \n",
      " Step [205/304], Loss: 1.3604, Dealed Nodes [6592/9720] \n",
      " Step [210/304], Loss: 1.4114, Dealed Nodes [6752/9720] \n",
      " Step [215/304], Loss: 1.5685, Dealed Nodes [6912/9720] \n",
      " Step [220/304], Loss: 1.3368, Dealed Nodes [7072/9720] \n",
      " Step [225/304], Loss: 1.5282, Dealed Nodes [7232/9720] \n",
      " Step [230/304], Loss: 1.5436, Dealed Nodes [7392/9720] \n",
      " Step [235/304], Loss: 1.2556, Dealed Nodes [7552/9720] \n",
      " Step [240/304], Loss: 1.2634, Dealed Nodes [7712/9720] \n",
      " Step [245/304], Loss: 1.6064, Dealed Nodes [7872/9720] \n",
      " Step [250/304], Loss: 1.4601, Dealed Nodes [8032/9720] \n",
      " Step [255/304], Loss: 1.5512, Dealed Nodes [8192/9720] \n",
      " Step [260/304], Loss: 1.5048, Dealed Nodes [8352/9720] \n",
      " Step [265/304], Loss: 1.2698, Dealed Nodes [8512/9720] \n",
      " Step [270/304], Loss: 1.2598, Dealed Nodes [8672/9720] \n",
      " Step [275/304], Loss: 1.3447, Dealed Nodes [8832/9720] \n",
      " Step [280/304], Loss: 1.2656, Dealed Nodes [8992/9720] \n",
      " Step [285/304], Loss: 1.3122, Dealed Nodes [9152/9720] \n",
      " Step [290/304], Loss: 1.3207, Dealed Nodes [9312/9720] \n",
      " Step [295/304], Loss: 1.2932, Dealed Nodes [9472/9720] \n",
      " Step [300/304], Loss: 1.2206, Dealed Nodes [9632/9720] \n",
      "Validation F1: 0.5349794238683128\n",
      "Test F1: 0.5268518518518519\n",
      "----------------------EPOCH 2-----------------------\n",
      "Update the Whole GraphSage\n",
      " Step [0/304], Loss: 1.2107, Dealed Nodes [32/9720] \n",
      " Step [5/304], Loss: 1.3333, Dealed Nodes [192/9720] \n",
      " Step [10/304], Loss: 1.3241, Dealed Nodes [352/9720] \n",
      " Step [15/304], Loss: 1.2592, Dealed Nodes [512/9720] \n",
      " Step [20/304], Loss: 1.1421, Dealed Nodes [672/9720] \n",
      " Step [25/304], Loss: 1.2203, Dealed Nodes [832/9720] \n",
      " Step [30/304], Loss: 1.3254, Dealed Nodes [992/9720] \n",
      " Step [35/304], Loss: 1.2716, Dealed Nodes [1152/9720] \n",
      " Step [40/304], Loss: 0.9790, Dealed Nodes [1312/9720] \n",
      " Step [45/304], Loss: 1.2820, Dealed Nodes [1472/9720] \n",
      " Step [50/304], Loss: 1.0836, Dealed Nodes [1632/9720] \n",
      " Step [55/304], Loss: 1.2185, Dealed Nodes [1792/9720] \n",
      " Step [60/304], Loss: 1.1380, Dealed Nodes [1952/9720] \n",
      " Step [65/304], Loss: 1.2750, Dealed Nodes [2112/9720] \n",
      " Step [70/304], Loss: 1.2040, Dealed Nodes [2272/9720] \n",
      " Step [75/304], Loss: 1.3329, Dealed Nodes [2432/9720] \n",
      " Step [80/304], Loss: 1.1959, Dealed Nodes [2592/9720] \n",
      " Step [85/304], Loss: 1.3031, Dealed Nodes [2752/9720] \n",
      " Step [90/304], Loss: 1.1516, Dealed Nodes [2912/9720] \n",
      " Step [95/304], Loss: 1.5129, Dealed Nodes [3072/9720] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step [100/304], Loss: 1.0029, Dealed Nodes [3232/9720] \n",
      " Step [105/304], Loss: 0.9008, Dealed Nodes [3392/9720] \n",
      " Step [110/304], Loss: 1.2458, Dealed Nodes [3552/9720] \n",
      " Step [115/304], Loss: 1.0611, Dealed Nodes [3712/9720] \n",
      " Step [120/304], Loss: 1.1359, Dealed Nodes [3872/9720] \n",
      " Step [125/304], Loss: 1.1664, Dealed Nodes [4032/9720] \n",
      " Step [130/304], Loss: 1.1832, Dealed Nodes [4192/9720] \n",
      " Step [135/304], Loss: 1.1602, Dealed Nodes [4352/9720] \n",
      " Step [140/304], Loss: 1.1132, Dealed Nodes [4512/9720] \n",
      " Step [145/304], Loss: 1.2785, Dealed Nodes [4672/9720] \n",
      " Step [150/304], Loss: 1.2909, Dealed Nodes [4832/9720] \n",
      " Step [155/304], Loss: 1.4284, Dealed Nodes [4992/9720] \n",
      " Step [160/304], Loss: 0.8497, Dealed Nodes [5152/9720] \n",
      " Step [165/304], Loss: 1.2425, Dealed Nodes [5312/9720] \n",
      " Step [170/304], Loss: 0.8875, Dealed Nodes [5472/9720] \n",
      " Step [175/304], Loss: 1.0500, Dealed Nodes [5632/9720] \n",
      " Step [180/304], Loss: 1.1430, Dealed Nodes [5792/9720] \n",
      " Step [185/304], Loss: 1.3845, Dealed Nodes [5952/9720] \n",
      " Step [190/304], Loss: 1.2089, Dealed Nodes [6112/9720] \n",
      " Step [195/304], Loss: 0.7846, Dealed Nodes [6272/9720] \n",
      " Step [200/304], Loss: 1.0437, Dealed Nodes [6432/9720] \n",
      " Step [205/304], Loss: 1.0914, Dealed Nodes [6592/9720] \n",
      " Step [210/304], Loss: 1.3458, Dealed Nodes [6752/9720] \n",
      " Step [215/304], Loss: 1.1510, Dealed Nodes [6912/9720] \n",
      " Step [220/304], Loss: 0.9365, Dealed Nodes [7072/9720] \n",
      " Step [225/304], Loss: 1.0272, Dealed Nodes [7232/9720] \n",
      " Step [230/304], Loss: 1.2277, Dealed Nodes [7392/9720] \n",
      " Step [235/304], Loss: 1.1838, Dealed Nodes [7552/9720] \n",
      " Step [240/304], Loss: 1.0259, Dealed Nodes [7712/9720] \n",
      " Step [245/304], Loss: 1.1722, Dealed Nodes [7872/9720] \n",
      " Step [250/304], Loss: 1.2833, Dealed Nodes [8032/9720] \n",
      " Step [255/304], Loss: 0.9724, Dealed Nodes [8192/9720] \n",
      " Step [260/304], Loss: 1.1137, Dealed Nodes [8352/9720] \n",
      " Step [265/304], Loss: 0.9611, Dealed Nodes [8512/9720] \n",
      " Step [270/304], Loss: 0.9552, Dealed Nodes [8672/9720] \n",
      " Step [275/304], Loss: 0.8660, Dealed Nodes [8832/9720] \n",
      " Step [280/304], Loss: 1.0837, Dealed Nodes [8992/9720] \n",
      " Step [285/304], Loss: 1.1688, Dealed Nodes [9152/9720] \n",
      " Step [290/304], Loss: 0.9936, Dealed Nodes [9312/9720] \n",
      " Step [295/304], Loss: 0.8844, Dealed Nodes [9472/9720] \n",
      " Step [300/304], Loss: 0.9419, Dealed Nodes [9632/9720] \n",
      "Validation F1: 0.577366255144033\n",
      "Test F1: 0.5703703703703704\n",
      "max valid acc 0.577366255144033\n",
      "test acc 0.5703703703703704\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1_list = []\n",
    "test_acc = 0 \n",
    "T1 = 10\n",
    "T2 =  0\n",
    "epochs = 3#600\n",
    "r_sup = [range(i,i+T1) for i in range(0, epochs, int(T1 + T2))]\n",
    "r_unsup = [range( i , i   + T2) for i in range(T1, epochs, int(T1 + T2))] \n",
    "r_unsup = [i for sub in r_unsup for i in sub]\n",
    "r_sup = [i for sub in r_sup for i in sub]\n",
    "for epoch in range(epochs ):\n",
    "    print('----------------------EPOCH %d-----------------------' % epoch) \n",
    "    if epoch in r_sup:\n",
    "        learn_method = 'sup'\n",
    "        print('Update the Whole GraphSage')\n",
    "        graphSage, classification, optimizer = apply_model(  optimizer, nodes_layers ,   dataC , 'loc', graphSage, classification,     b_sz,  device  , learn_method )\n",
    "        max_vali_f1, test_f1 = evaluate(nodes_layers , dataC , ds, graphSage, classification, device,  max_vali_f1,    epoch)\n",
    "    elif epoch in r_unsup:  \n",
    "        print('Update only the fully connect layers')\n",
    "        classification, max_vali_f1, test_f1 =train_classification( nodes_layers ,dataC , graphSage, classification, ds, device, max_vali_f1  )\n",
    "    f1_list.append(max_vali_f1)\n",
    "    if test_f1 > test_acc:\n",
    "        test_acc = test_f1    \n",
    "        state = {'epoch': epoch + 1, 'state_dict': [graphSage.state_dict(), classification.state_dict()],\n",
    "             'optimizer': optimizer.state_dict() }\n",
    "        torch.save(state, savepath) \n",
    "plt.plot(f1_list, '*-')      \n",
    "print('max valid acc', max(f1_list))\n",
    "print('test acc', test_acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './00_saved_final/sup_90_ALL_15_37nodes_suptrain.pkl'\n",
      "=> loaded checkpoint './00_saved_final/sup_90_ALL_15_37nodes_suptrain.pkl' (epoch 6)\n",
      "Test F1:0.6968,  Acc:0.7003, Acc 1 hop: 0.8954 \n"
     ]
    }
   ],
   "source": [
    "savebest = './00_saved_final/sup_90_ALL_15_37nodes_suptrain.pkl'\n",
    "models, optimizer, start_epoch = load_checkpoint(models, optimizer , savebest)\n",
    "graphSage, classification = models[0], models[1]\n",
    "labels_neib = one_hot_neib(labels, neib )\n",
    "models = [graphSage, classification]\n",
    "\n",
    "params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.requires_grad = False\n",
    "            params.append(param)\n",
    "\n",
    "embs = graphSage( nodes_layers ,features)\n",
    "logists =  classification(embs)\n",
    "predicts = torch.max(logists, 1)[1]\n",
    "labels_test = labels[ind_test] \n",
    "labels_neib_test = labels_neib[ind_test]\n",
    "assert len(labels[ind_test] ) == len(predicts[ind_test]  ) \n",
    "test_f1 = f1_score(labels[ind_test] , predicts[ind_test].cpu().data, average=\"macro\")\n",
    "acc = accuracy_score(labels_test, predicts[ind_test].cpu().data)\n",
    "acc_neib = hop_acc(labels_neib_test, predicts[ind_test].cpu().data)\n",
    "print(\"Test F1:%.4f,  Acc:%.4f, Acc 1 hop: %.4f \" %(test_f1, acc, acc_neib))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj = constructW_stageI(embs , logists  ,A  , 120 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_X = torch.reshape(features, [features.shape[0],features.shape[1] *features.shape[2] ])\n",
    "seed = 42; epochs =300\n",
    "lr = 0.001\n",
    "weight_decay = 5e-5\n",
    "batch_size = 32\n",
    "hidden = [128*3,128*3 ]\n",
    "dropout =0 \n",
    "fastmode = False\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed( seed)   \n",
    "\n",
    " \n",
    "model2 = GCN(nfeat=features_X.shape[1],\n",
    "            nhid= hidden,\n",
    "            nclass=int(len(list(set(labels.numpy()))) ), \n",
    "            dropout= dropout)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(),\n",
    "                       lr= lr, weight_decay= weight_decay) \n",
    "model_name =  'StageII_all37' + str(num_labelper) + '_1.pkl'\n",
    "newname =  'StageII_all37' + str(num_labelper) + '_1.pkl'\n",
    "savepath2 = os.path.join(saveroot, model_name) \n",
    "savebest2 = os.path.join(saveroot, newname) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 3.5881 acc_train: 0.0426 loss_val: 3.5023 acc_val: 0.0765 time: 1.1605s\n",
      "Test set results: loss= 3.5047 accuracy= 0.0725 1-hop accuracy = 0.2731\n",
      "Epoch: 0002 loss_train: 3.5023 acc_train: 0.0765 loss_val: 3.4292 acc_val: 0.1616 time: 1.1369s\n",
      "Test set results: loss= 3.4335 accuracy= 0.1540 1-hop accuracy = 0.3593\n",
      "Epoch: 0003 loss_train: 3.4292 acc_train: 0.1616 loss_val: 3.3627 acc_val: 0.1975 time: 1.1469s\n",
      "Test set results: loss= 3.3690 accuracy= 0.1886 1-hop accuracy = 0.4086\n",
      "Epoch: 0004 loss_train: 3.3627 acc_train: 0.1975 loss_val: 3.2990 acc_val: 0.2319 time: 1.1261s\n",
      "Test set results: loss= 3.3074 accuracy= 0.2278 1-hop accuracy = 0.4377\n",
      "Epoch: 0005 loss_train: 3.2990 acc_train: 0.2319 loss_val: 3.2365 acc_val: 0.2731 time: 1.1424s\n",
      "Test set results: loss= 3.2470 accuracy= 0.2673 1-hop accuracy = 0.4836\n",
      "Epoch: 0006 loss_train: 3.2365 acc_train: 0.2731 loss_val: 3.1744 acc_val: 0.3002 time: 1.2162s\n",
      "Test set results: loss= 3.1869 accuracy= 0.2895 1-hop accuracy = 0.5077\n",
      "Epoch: 0007 loss_train: 3.1744 acc_train: 0.3002 loss_val: 3.1114 acc_val: 0.3160 time: 1.1574s\n",
      "Test set results: loss= 3.1260 accuracy= 0.3043 1-hop accuracy = 0.5151\n",
      "Epoch: 0008 loss_train: 3.1114 acc_train: 0.3160 loss_val: 3.0472 acc_val: 0.3298 time: 1.1494s\n",
      "Test set results: loss= 3.0638 accuracy= 0.3256 1-hop accuracy = 0.5472\n",
      "Epoch: 0009 loss_train: 3.0472 acc_train: 0.3298 loss_val: 2.9820 acc_val: 0.3506 time: 1.1452s\n",
      "Test set results: loss= 3.0004 accuracy= 0.3414 1-hop accuracy = 0.5756\n",
      "Epoch: 0010 loss_train: 2.9820 acc_train: 0.3506 loss_val: 2.9157 acc_val: 0.3708 time: 1.1173s\n",
      "Test set results: loss= 2.9357 accuracy= 0.3590 1-hop accuracy = 0.6068\n",
      "Epoch: 0011 loss_train: 2.9157 acc_train: 0.3708 loss_val: 2.8485 acc_val: 0.3862 time: 1.1063s\n",
      "Test set results: loss= 2.8700 accuracy= 0.3750 1-hop accuracy = 0.6219\n",
      "Epoch: 0012 loss_train: 2.8485 acc_train: 0.3862 loss_val: 2.7804 acc_val: 0.4102 time: 1.1576s\n",
      "Test set results: loss= 2.8032 accuracy= 0.3988 1-hop accuracy = 0.6367\n",
      "Epoch: 0013 loss_train: 2.7804 acc_train: 0.4102 loss_val: 2.7115 acc_val: 0.4308 time: 1.1665s\n",
      "Test set results: loss= 2.7355 accuracy= 0.4167 1-hop accuracy = 0.6605\n",
      "Epoch: 0014 loss_train: 2.7115 acc_train: 0.4308 loss_val: 2.6417 acc_val: 0.4442 time: 1.1223s\n",
      "Test set results: loss= 2.6667 accuracy= 0.4336 1-hop accuracy = 0.6775\n",
      "Epoch: 0015 loss_train: 2.6417 acc_train: 0.4442 loss_val: 2.5708 acc_val: 0.4626 time: 1.1098s\n",
      "Test set results: loss= 2.5968 accuracy= 0.4540 1-hop accuracy = 0.7031\n",
      "Epoch: 0016 loss_train: 2.5708 acc_train: 0.4626 loss_val: 2.4995 acc_val: 0.4730 time: 1.1258s\n",
      "Test set results: loss= 2.5263 accuracy= 0.4642 1-hop accuracy = 0.7120\n",
      "Epoch: 0017 loss_train: 2.4995 acc_train: 0.4730 loss_val: 2.4280 acc_val: 0.4884 time: 1.1256s\n",
      "Test set results: loss= 2.4555 accuracy= 0.4796 1-hop accuracy = 0.7265\n",
      "Epoch: 0018 loss_train: 2.4280 acc_train: 0.4884 loss_val: 2.3563 acc_val: 0.4997 time: 1.1162s\n",
      "Test set results: loss= 2.3844 accuracy= 0.4886 1-hop accuracy = 0.7373\n",
      "Epoch: 0019 loss_train: 2.3563 acc_train: 0.4997 loss_val: 2.2845 acc_val: 0.5192 time: 1.1217s\n",
      "Test set results: loss= 2.3132 accuracy= 0.5077 1-hop accuracy = 0.7525\n",
      "Epoch: 0020 loss_train: 2.2845 acc_train: 0.5192 loss_val: 2.2129 acc_val: 0.5357 time: 1.1570s\n",
      "Test set results: loss= 2.2421 accuracy= 0.5247 1-hop accuracy = 0.7676\n",
      "Epoch: 0021 loss_train: 2.2129 acc_train: 0.5357 loss_val: 2.1419 acc_val: 0.5458 time: 1.1241s\n",
      "Test set results: loss= 2.1717 accuracy= 0.5358 1-hop accuracy = 0.7759\n",
      "Epoch: 0022 loss_train: 2.1419 acc_train: 0.5458 loss_val: 2.0719 acc_val: 0.5660 time: 1.1081s\n",
      "Test set results: loss= 2.1022 accuracy= 0.5617 1-hop accuracy = 0.7969\n",
      "Epoch: 0023 loss_train: 2.0719 acc_train: 0.5660 loss_val: 2.0029 acc_val: 0.5867 time: 1.1178s\n",
      "Test set results: loss= 2.0340 accuracy= 0.5843 1-hop accuracy = 0.8151\n",
      "Epoch: 0024 loss_train: 2.0029 acc_train: 0.5867 loss_val: 1.9351 acc_val: 0.6055 time: 1.1089s\n",
      "Test set results: loss= 1.9669 accuracy= 0.6012 1-hop accuracy = 0.8309\n",
      "Epoch: 0025 loss_train: 1.9351 acc_train: 0.6055 loss_val: 1.8686 acc_val: 0.6317 time: 1.1114s\n",
      "Test set results: loss= 1.9012 accuracy= 0.6287 1-hop accuracy = 0.8549\n",
      "Epoch: 0026 loss_train: 1.8686 acc_train: 0.6317 loss_val: 1.8037 acc_val: 0.6443 time: 1.1341s\n",
      "Test set results: loss= 1.8370 accuracy= 0.6392 1-hop accuracy = 0.8685\n",
      "Epoch: 0027 loss_train: 1.8037 acc_train: 0.6443 loss_val: 1.7401 acc_val: 0.6631 time: 1.1223s\n",
      "Test set results: loss= 1.7742 accuracy= 0.6599 1-hop accuracy = 0.8904\n",
      "Epoch: 0028 loss_train: 1.7401 acc_train: 0.6631 loss_val: 1.6784 acc_val: 0.6668 time: 1.0979s\n",
      "Test set results: loss= 1.7132 accuracy= 0.6642 1-hop accuracy = 0.8997\n",
      "Epoch: 0029 loss_train: 1.6784 acc_train: 0.6668 loss_val: 1.6184 acc_val: 0.6770 time: 1.1182s\n",
      "Test set results: loss= 1.6540 accuracy= 0.6759 1-hop accuracy = 0.9090\n",
      "Epoch: 0030 loss_train: 1.6184 acc_train: 0.6770 loss_val: 1.5604 acc_val: 0.6850 time: 1.1376s\n",
      "Test set results: loss= 1.5966 accuracy= 0.6840 1-hop accuracy = 0.9142\n",
      "Epoch: 0031 loss_train: 1.5604 acc_train: 0.6850 loss_val: 1.5042 acc_val: 0.6926 time: 1.1381s\n",
      "Test set results: loss= 1.5412 accuracy= 0.6895 1-hop accuracy = 0.9160\n",
      "Epoch: 0032 loss_train: 1.5042 acc_train: 0.6926 loss_val: 1.4501 acc_val: 0.7029 time: 1.1138s\n",
      "Test set results: loss= 1.4879 accuracy= 0.6972 1-hop accuracy = 0.9170\n",
      "Epoch: 0033 loss_train: 1.4501 acc_train: 0.7029 loss_val: 1.3981 acc_val: 0.7121 time: 1.1050s\n",
      "Test set results: loss= 1.4368 accuracy= 0.7059 1-hop accuracy = 0.9194\n",
      "Epoch: 0034 loss_train: 1.3981 acc_train: 0.7121 loss_val: 1.3482 acc_val: 0.7179 time: 1.1102s\n",
      "Test set results: loss= 1.3877 accuracy= 0.7154 1-hop accuracy = 0.9238\n",
      "Epoch: 0035 loss_train: 1.3482 acc_train: 0.7179 loss_val: 1.3004 acc_val: 0.7206 time: 1.1157s\n",
      "Test set results: loss= 1.3407 accuracy= 0.7191 1-hop accuracy = 0.9247\n",
      "Epoch: 0036 loss_train: 1.3004 acc_train: 0.7206 loss_val: 1.2546 acc_val: 0.7226 time: 1.1206s\n",
      "Test set results: loss= 1.2955 accuracy= 0.7210 1-hop accuracy = 0.9262\n",
      "Epoch: 0037 loss_train: 1.2546 acc_train: 0.7226 loss_val: 1.2108 acc_val: 0.7276 time: 1.1446s\n",
      "Test set results: loss= 1.2520 accuracy= 0.7228 1-hop accuracy = 0.9293\n",
      "Epoch: 0038 loss_train: 1.2108 acc_train: 0.7276 loss_val: 1.1688 acc_val: 0.7316 time: 1.1161s\n",
      "Test set results: loss= 1.2106 accuracy= 0.7235 1-hop accuracy = 0.9290\n",
      "Epoch: 0039 loss_train: 1.1688 acc_train: 0.7316 loss_val: 1.1288 acc_val: 0.7394 time: 1.1332s\n",
      "Test set results: loss= 1.1710 accuracy= 0.7346 1-hop accuracy = 0.9336\n",
      "Epoch: 0040 loss_train: 1.1288 acc_train: 0.7394 loss_val: 1.0906 acc_val: 0.7399 time: 1.1267s\n",
      "Test set results: loss= 1.1332 accuracy= 0.7364 1-hop accuracy = 0.9340\n",
      "Epoch: 0041 loss_train: 1.0906 acc_train: 0.7399 loss_val: 1.0541 acc_val: 0.7425 time: 1.1052s\n",
      "Test set results: loss= 1.0970 accuracy= 0.7373 1-hop accuracy = 0.9349\n",
      "Epoch: 0042 loss_train: 1.0541 acc_train: 0.7425 loss_val: 1.0194 acc_val: 0.7444 time: 1.1301s\n",
      "Test set results: loss= 1.0625 accuracy= 0.7386 1-hop accuracy = 0.9383\n",
      "Epoch: 0043 loss_train: 1.0194 acc_train: 0.7444 loss_val: 0.9864 acc_val: 0.7492 time: 1.1359s\n",
      "Test set results: loss= 1.0295 accuracy= 0.7426 1-hop accuracy = 0.9401\n",
      "Epoch: 0044 loss_train: 0.9864 acc_train: 0.7492 loss_val: 0.9549 acc_val: 0.7536 time: 1.1264s\n",
      "Test set results: loss= 0.9980 accuracy= 0.7460 1-hop accuracy = 0.9441\n",
      "Epoch: 0045 loss_train: 0.9549 acc_train: 0.7536 loss_val: 0.9250 acc_val: 0.7580 time: 1.1143s\n",
      "Test set results: loss= 0.9679 accuracy= 0.7512 1-hop accuracy = 0.9460\n",
      "Epoch: 0046 loss_train: 0.9250 acc_train: 0.7580 loss_val: 0.8966 acc_val: 0.7612 time: 1.1214s\n",
      "Test set results: loss= 0.9395 accuracy= 0.7515 1-hop accuracy = 0.9500\n",
      "Epoch: 0047 loss_train: 0.8966 acc_train: 0.7612 loss_val: 0.8696 acc_val: 0.7650 time: 1.1170s\n",
      "Test set results: loss= 0.9125 accuracy= 0.7562 1-hop accuracy = 0.9534\n",
      "Epoch: 0048 loss_train: 0.8696 acc_train: 0.7650 loss_val: 0.8440 acc_val: 0.7669 time: 1.1284s\n",
      "Test set results: loss= 0.8868 accuracy= 0.7565 1-hop accuracy = 0.9552\n",
      "Epoch: 0049 loss_train: 0.8440 acc_train: 0.7669 loss_val: 0.8198 acc_val: 0.7695 time: 1.1057s\n",
      "Test set results: loss= 0.8624 accuracy= 0.7596 1-hop accuracy = 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 loss_train: 0.8198 acc_train: 0.7695 loss_val: 0.7970 acc_val: 0.7743 time: 1.1388s\n",
      "Test set results: loss= 0.8393 accuracy= 0.7642 1-hop accuracy = 0.9623\n",
      "Epoch: 0051 loss_train: 0.7970 acc_train: 0.7743 loss_val: 0.7755 acc_val: 0.7751 time: 1.1195s\n",
      "Test set results: loss= 0.8176 accuracy= 0.7660 1-hop accuracy = 0.9630\n",
      "Epoch: 0052 loss_train: 0.7755 acc_train: 0.7751 loss_val: 0.7554 acc_val: 0.7754 time: 1.1243s\n",
      "Test set results: loss= 0.7973 accuracy= 0.7657 1-hop accuracy = 0.9642\n",
      "Epoch: 0053 loss_train: 0.7554 acc_train: 0.7754 loss_val: 0.7365 acc_val: 0.7771 time: 1.1377s\n",
      "Test set results: loss= 0.7784 accuracy= 0.7694 1-hop accuracy = 0.9654\n",
      "Epoch: 0054 loss_train: 0.7365 acc_train: 0.7771 loss_val: 0.7187 acc_val: 0.7800 time: 1.1176s\n",
      "Test set results: loss= 0.7606 accuracy= 0.7710 1-hop accuracy = 0.9676\n",
      "Epoch: 0055 loss_train: 0.7187 acc_train: 0.7800 loss_val: 0.7020 acc_val: 0.7803 time: 1.1215s\n",
      "Test set results: loss= 0.7440 accuracy= 0.7710 1-hop accuracy = 0.9688\n",
      "Epoch: 0056 loss_train: 0.7020 acc_train: 0.7803 loss_val: 0.6863 acc_val: 0.7835 time: 1.1505s\n",
      "Test set results: loss= 0.7284 accuracy= 0.7747 1-hop accuracy = 0.9704\n",
      "Epoch: 0057 loss_train: 0.6863 acc_train: 0.7835 loss_val: 0.6716 acc_val: 0.7863 time: 1.1367s\n",
      "Test set results: loss= 0.7139 accuracy= 0.7762 1-hop accuracy = 0.9707\n",
      "Epoch: 0058 loss_train: 0.6716 acc_train: 0.7863 loss_val: 0.6577 acc_val: 0.7883 time: 1.1272s\n",
      "Test set results: loss= 0.7003 accuracy= 0.7784 1-hop accuracy = 0.9701\n",
      "Epoch: 0059 loss_train: 0.6577 acc_train: 0.7883 loss_val: 0.6447 acc_val: 0.7897 time: 1.1068s\n",
      "Test set results: loss= 0.6874 accuracy= 0.7802 1-hop accuracy = 0.9716\n",
      "Epoch: 0060 loss_train: 0.6447 acc_train: 0.7897 loss_val: 0.6324 acc_val: 0.7904 time: 1.1208s\n",
      "Test set results: loss= 0.6749 accuracy= 0.7824 1-hop accuracy = 0.9713\n",
      "Epoch: 0061 loss_train: 0.6324 acc_train: 0.7904 loss_val: 0.6208 acc_val: 0.7922 time: 1.1310s\n",
      "Test set results: loss= 0.6631 accuracy= 0.7855 1-hop accuracy = 0.9716\n",
      "Epoch: 0062 loss_train: 0.6208 acc_train: 0.7922 loss_val: 0.6098 acc_val: 0.7930 time: 1.1366s\n",
      "Test set results: loss= 0.6521 accuracy= 0.7870 1-hop accuracy = 0.9716\n",
      "Epoch: 0063 loss_train: 0.6098 acc_train: 0.7930 loss_val: 0.5995 acc_val: 0.7937 time: 1.1097s\n",
      "Test set results: loss= 0.6417 accuracy= 0.7858 1-hop accuracy = 0.9719\n",
      "Epoch: 0064 loss_train: 0.5995 acc_train: 0.7937 loss_val: 0.5898 acc_val: 0.7953 time: 1.1061s\n",
      "Test set results: loss= 0.6319 accuracy= 0.7861 1-hop accuracy = 0.9722\n",
      "Epoch: 0065 loss_train: 0.5898 acc_train: 0.7953 loss_val: 0.5806 acc_val: 0.7969 time: 1.1267s\n",
      "Test set results: loss= 0.6227 accuracy= 0.7870 1-hop accuracy = 0.9725\n",
      "Epoch: 0066 loss_train: 0.5806 acc_train: 0.7969 loss_val: 0.5718 acc_val: 0.7986 time: 1.1112s\n",
      "Test set results: loss= 0.6139 accuracy= 0.7889 1-hop accuracy = 0.9722\n",
      "Epoch: 0067 loss_train: 0.5718 acc_train: 0.7986 loss_val: 0.5635 acc_val: 0.8000 time: 1.1378s\n",
      "Test set results: loss= 0.6056 accuracy= 0.7883 1-hop accuracy = 0.9725\n",
      "Epoch: 0068 loss_train: 0.5635 acc_train: 0.8000 loss_val: 0.5556 acc_val: 0.8034 time: 1.1357s\n",
      "Test set results: loss= 0.5975 accuracy= 0.7917 1-hop accuracy = 0.9722\n",
      "Epoch: 0069 loss_train: 0.5556 acc_train: 0.8034 loss_val: 0.5481 acc_val: 0.8038 time: 1.1280s\n",
      "Test set results: loss= 0.5902 accuracy= 0.7910 1-hop accuracy = 0.9722\n",
      "Epoch: 0070 loss_train: 0.5481 acc_train: 0.8038 loss_val: 0.5410 acc_val: 0.8071 time: 1.1292s\n",
      "Test set results: loss= 0.5828 accuracy= 0.7935 1-hop accuracy = 0.9716\n",
      "Epoch: 0071 loss_train: 0.5410 acc_train: 0.8071 loss_val: 0.5343 acc_val: 0.8077 time: 1.1204s\n",
      "Test set results: loss= 0.5765 accuracy= 0.7910 1-hop accuracy = 0.9722\n",
      "Epoch: 0072 loss_train: 0.5343 acc_train: 0.8077 loss_val: 0.5278 acc_val: 0.8122 time: 1.1217s\n",
      "Test set results: loss= 0.5696 accuracy= 0.7969 1-hop accuracy = 0.9719\n",
      "Epoch: 0073 loss_train: 0.5278 acc_train: 0.8122 loss_val: 0.5217 acc_val: 0.8129 time: 1.1294s\n",
      "Test set results: loss= 0.5644 accuracy= 0.7966 1-hop accuracy = 0.9716\n",
      "Epoch: 0074 loss_train: 0.5217 acc_train: 0.8129 loss_val: 0.5159 acc_val: 0.8172 time: 1.1168s\n",
      "Test set results: loss= 0.5579 accuracy= 0.8040 1-hop accuracy = 0.9738\n",
      "Epoch: 0075 loss_train: 0.5159 acc_train: 0.8172 loss_val: 0.5103 acc_val: 0.8176 time: 1.1250s\n",
      "Test set results: loss= 0.5533 accuracy= 0.7988 1-hop accuracy = 0.9722\n",
      "Epoch: 0076 loss_train: 0.5103 acc_train: 0.8176 loss_val: 0.5049 acc_val: 0.8202 time: 1.1210s\n",
      "Test set results: loss= 0.5475 accuracy= 0.8052 1-hop accuracy = 0.9728\n",
      "Epoch: 0077 loss_train: 0.5049 acc_train: 0.8202 loss_val: 0.4996 acc_val: 0.8229 time: 1.1516s\n",
      "Test set results: loss= 0.5427 accuracy= 0.8093 1-hop accuracy = 0.9725\n",
      "Epoch: 0078 loss_train: 0.4996 acc_train: 0.8229 loss_val: 0.4944 acc_val: 0.8218 time: 1.1242s\n",
      "Test set results: loss= 0.5376 accuracy= 0.8083 1-hop accuracy = 0.9728\n",
      "Epoch: 0079 loss_train: 0.4944 acc_train: 0.8218 loss_val: 0.4896 acc_val: 0.8255 time: 1.1432s\n",
      "Test set results: loss= 0.5328 accuracy= 0.8114 1-hop accuracy = 0.9725\n",
      "Epoch: 0080 loss_train: 0.4896 acc_train: 0.8255 loss_val: 0.4851 acc_val: 0.8249 time: 1.1209s\n",
      "Test set results: loss= 0.5285 accuracy= 0.8120 1-hop accuracy = 0.9731\n",
      "Epoch: 0081 loss_train: 0.4851 acc_train: 0.8249 loss_val: 0.4807 acc_val: 0.8242 time: 1.1198s\n",
      "Test set results: loss= 0.5238 accuracy= 0.8120 1-hop accuracy = 0.9738\n",
      "Epoch: 0082 loss_train: 0.4807 acc_train: 0.8242 loss_val: 0.4763 acc_val: 0.8263 time: 1.1150s\n",
      "Test set results: loss= 0.5202 accuracy= 0.8130 1-hop accuracy = 0.9738\n",
      "Epoch: 0083 loss_train: 0.4763 acc_train: 0.8263 loss_val: 0.4720 acc_val: 0.8275 time: 1.1016s\n",
      "Test set results: loss= 0.5155 accuracy= 0.8151 1-hop accuracy = 0.9731\n",
      "Epoch: 0084 loss_train: 0.4720 acc_train: 0.8275 loss_val: 0.4680 acc_val: 0.8290 time: 1.1087s\n",
      "Test set results: loss= 0.5120 accuracy= 0.8145 1-hop accuracy = 0.9731\n",
      "Epoch: 0085 loss_train: 0.4680 acc_train: 0.8290 loss_val: 0.4642 acc_val: 0.8302 time: 1.1168s\n",
      "Test set results: loss= 0.5083 accuracy= 0.8182 1-hop accuracy = 0.9738\n",
      "Epoch: 0086 loss_train: 0.4642 acc_train: 0.8302 loss_val: 0.4605 acc_val: 0.8314 time: 1.1114s\n",
      "Test set results: loss= 0.5045 accuracy= 0.8176 1-hop accuracy = 0.9731\n",
      "Epoch: 0087 loss_train: 0.4605 acc_train: 0.8314 loss_val: 0.4568 acc_val: 0.8326 time: 1.1095s\n",
      "Test set results: loss= 0.5016 accuracy= 0.8191 1-hop accuracy = 0.9731\n",
      "Epoch: 0088 loss_train: 0.4568 acc_train: 0.8326 loss_val: 0.4533 acc_val: 0.8328 time: 1.1088s\n",
      "Test set results: loss= 0.4978 accuracy= 0.8231 1-hop accuracy = 0.9731\n",
      "Epoch: 0089 loss_train: 0.4533 acc_train: 0.8328 loss_val: 0.4499 acc_val: 0.8337 time: 1.1336s\n",
      "Test set results: loss= 0.4953 accuracy= 0.8204 1-hop accuracy = 0.9738\n",
      "Epoch: 0090 loss_train: 0.4499 acc_train: 0.8337 loss_val: 0.4466 acc_val: 0.8362 time: 1.1433s\n",
      "Test set results: loss= 0.4916 accuracy= 0.8231 1-hop accuracy = 0.9735\n",
      "Epoch: 0091 loss_train: 0.4466 acc_train: 0.8362 loss_val: 0.4434 acc_val: 0.8389 time: 1.1076s\n",
      "Test set results: loss= 0.4891 accuracy= 0.8219 1-hop accuracy = 0.9744\n",
      "Epoch: 0092 loss_train: 0.4434 acc_train: 0.8389 loss_val: 0.4402 acc_val: 0.8403 time: 1.1107s\n",
      "Test set results: loss= 0.4856 accuracy= 0.8272 1-hop accuracy = 0.9738\n",
      "Epoch: 0093 loss_train: 0.4402 acc_train: 0.8403 loss_val: 0.4372 acc_val: 0.8418 time: 1.1072s\n",
      "Test set results: loss= 0.4832 accuracy= 0.8265 1-hop accuracy = 0.9735\n",
      "Epoch: 0094 loss_train: 0.4372 acc_train: 0.8418 loss_val: 0.4343 acc_val: 0.8415 time: 1.1299s\n",
      "Test set results: loss= 0.4800 accuracy= 0.8287 1-hop accuracy = 0.9747\n",
      "Epoch: 0095 loss_train: 0.4343 acc_train: 0.8415 loss_val: 0.4314 acc_val: 0.8426 time: 1.1179s\n",
      "Test set results: loss= 0.4778 accuracy= 0.8299 1-hop accuracy = 0.9744\n",
      "Epoch: 0096 loss_train: 0.4314 acc_train: 0.8426 loss_val: 0.4286 acc_val: 0.8430 time: 1.1196s\n",
      "Test set results: loss= 0.4748 accuracy= 0.8302 1-hop accuracy = 0.9744\n",
      "Epoch: 0097 loss_train: 0.4286 acc_train: 0.8430 loss_val: 0.4258 acc_val: 0.8470 time: 1.1043s\n",
      "Test set results: loss= 0.4724 accuracy= 0.8318 1-hop accuracy = 0.9756\n",
      "Epoch: 0098 loss_train: 0.4258 acc_train: 0.8470 loss_val: 0.4232 acc_val: 0.8453 time: 1.1300s\n",
      "Test set results: loss= 0.4699 accuracy= 0.8336 1-hop accuracy = 0.9747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0099 loss_train: 0.4232 acc_train: 0.8453 loss_val: 0.4208 acc_val: 0.8467 time: 1.1366s\n",
      "Test set results: loss= 0.4674 accuracy= 0.8312 1-hop accuracy = 0.9750\n",
      "Epoch: 0100 loss_train: 0.4208 acc_train: 0.8467 loss_val: 0.4186 acc_val: 0.8465 time: 1.1255s\n",
      "Test set results: loss= 0.4659 accuracy= 0.8346 1-hop accuracy = 0.9747\n",
      "Epoch: 0101 loss_train: 0.4186 acc_train: 0.8465 loss_val: 0.4163 acc_val: 0.8478 time: 1.1048s\n",
      "Test set results: loss= 0.4629 accuracy= 0.8312 1-hop accuracy = 0.9753\n",
      "Epoch: 0102 loss_train: 0.4163 acc_train: 0.8478 loss_val: 0.4138 acc_val: 0.8477 time: 1.1323s\n",
      "Test set results: loss= 0.4617 accuracy= 0.8349 1-hop accuracy = 0.9744\n",
      "Epoch: 0103 loss_train: 0.4138 acc_train: 0.8477 loss_val: 0.4110 acc_val: 0.8527 time: 1.1020s\n",
      "Test set results: loss= 0.4579 accuracy= 0.8380 1-hop accuracy = 0.9762\n",
      "Epoch: 0104 loss_train: 0.4110 acc_train: 0.8527 loss_val: 0.4085 acc_val: 0.8489 time: 1.1285s\n",
      "Test set results: loss= 0.4568 accuracy= 0.8358 1-hop accuracy = 0.9744\n",
      "Epoch: 0105 loss_train: 0.4085 acc_train: 0.8489 loss_val: 0.4065 acc_val: 0.8505 time: 1.1086s\n",
      "Test set results: loss= 0.4546 accuracy= 0.8404 1-hop accuracy = 0.9747\n",
      "Epoch: 0106 loss_train: 0.4065 acc_train: 0.8505 loss_val: 0.4050 acc_val: 0.8501 time: 1.1228s\n",
      "Test set results: loss= 0.4536 accuracy= 0.8370 1-hop accuracy = 0.9756\n",
      "Epoch: 0107 loss_train: 0.4050 acc_train: 0.8501 loss_val: 0.4030 acc_val: 0.8520 time: 1.1134s\n",
      "Test set results: loss= 0.4513 accuracy= 0.8420 1-hop accuracy = 0.9750\n",
      "Epoch: 0108 loss_train: 0.4030 acc_train: 0.8520 loss_val: 0.4005 acc_val: 0.8562 time: 1.1350s\n",
      "Test set results: loss= 0.4494 accuracy= 0.8410 1-hop accuracy = 0.9775\n",
      "Epoch: 0109 loss_train: 0.4005 acc_train: 0.8562 loss_val: 0.3979 acc_val: 0.8546 time: 1.1326s\n",
      "Test set results: loss= 0.4463 accuracy= 0.8426 1-hop accuracy = 0.9756\n",
      "Epoch: 0110 loss_train: 0.3979 acc_train: 0.8546 loss_val: 0.3960 acc_val: 0.8543 time: 1.1348s\n",
      "Test set results: loss= 0.4451 accuracy= 0.8410 1-hop accuracy = 0.9756\n",
      "Epoch: 0111 loss_train: 0.3960 acc_train: 0.8543 loss_val: 0.3945 acc_val: 0.8547 time: 1.1297s\n",
      "Test set results: loss= 0.4434 accuracy= 0.8414 1-hop accuracy = 0.9759\n",
      "Epoch: 0112 loss_train: 0.3945 acc_train: 0.8547 loss_val: 0.3924 acc_val: 0.8558 time: 1.1251s\n",
      "Test set results: loss= 0.4414 accuracy= 0.8438 1-hop accuracy = 0.9756\n",
      "Epoch: 0113 loss_train: 0.3924 acc_train: 0.8558 loss_val: 0.3900 acc_val: 0.8585 time: 1.1187s\n",
      "Test set results: loss= 0.4393 accuracy= 0.8444 1-hop accuracy = 0.9762\n",
      "Epoch: 0114 loss_train: 0.3900 acc_train: 0.8585 loss_val: 0.3883 acc_val: 0.8605 time: 1.1128s\n",
      "Test set results: loss= 0.4376 accuracy= 0.8472 1-hop accuracy = 0.9772\n",
      "Epoch: 0115 loss_train: 0.3883 acc_train: 0.8605 loss_val: 0.3870 acc_val: 0.8599 time: 1.1392s\n",
      "Test set results: loss= 0.4367 accuracy= 0.8435 1-hop accuracy = 0.9759\n",
      "Epoch: 0116 loss_train: 0.3870 acc_train: 0.8599 loss_val: 0.3852 acc_val: 0.8566 time: 1.1069s\n",
      "Test set results: loss= 0.4343 accuracy= 0.8423 1-hop accuracy = 0.9772\n",
      "Epoch: 0117 loss_train: 0.3852 acc_train: 0.8566 loss_val: 0.3830 acc_val: 0.8612 time: 1.1273s\n",
      "Test set results: loss= 0.4333 accuracy= 0.8463 1-hop accuracy = 0.9762\n",
      "Epoch: 0118 loss_train: 0.3830 acc_train: 0.8612 loss_val: 0.3810 acc_val: 0.8605 time: 1.1007s\n",
      "Test set results: loss= 0.4308 accuracy= 0.8460 1-hop accuracy = 0.9762\n",
      "Epoch: 0119 loss_train: 0.3810 acc_train: 0.8605 loss_val: 0.3795 acc_val: 0.8618 time: 1.2116s\n",
      "Test set results: loss= 0.4298 accuracy= 0.8488 1-hop accuracy = 0.9775\n",
      "Epoch: 0120 loss_train: 0.3795 acc_train: 0.8618 loss_val: 0.3781 acc_val: 0.8615 time: 1.2162s\n",
      "Test set results: loss= 0.4284 accuracy= 0.8472 1-hop accuracy = 0.9769\n",
      "Epoch: 0121 loss_train: 0.3781 acc_train: 0.8615 loss_val: 0.3762 acc_val: 0.8619 time: 1.2125s\n",
      "Test set results: loss= 0.4266 accuracy= 0.8472 1-hop accuracy = 0.9765\n",
      "Epoch: 0122 loss_train: 0.3762 acc_train: 0.8619 loss_val: 0.3743 acc_val: 0.8629 time: 1.1337s\n",
      "Test set results: loss= 0.4251 accuracy= 0.8503 1-hop accuracy = 0.9772\n",
      "Epoch: 0123 loss_train: 0.3743 acc_train: 0.8629 loss_val: 0.3728 acc_val: 0.8614 time: 1.1331s\n",
      "Test set results: loss= 0.4236 accuracy= 0.8463 1-hop accuracy = 0.9772\n",
      "Epoch: 0124 loss_train: 0.3728 acc_train: 0.8614 loss_val: 0.3715 acc_val: 0.8663 time: 1.1345s\n",
      "Test set results: loss= 0.4224 accuracy= 0.8506 1-hop accuracy = 0.9787\n",
      "Epoch: 0125 loss_train: 0.3715 acc_train: 0.8663 loss_val: 0.3701 acc_val: 0.8622 time: 1.1093s\n",
      "Test set results: loss= 0.4212 accuracy= 0.8503 1-hop accuracy = 0.9769\n",
      "Epoch: 0126 loss_train: 0.3701 acc_train: 0.8622 loss_val: 0.3682 acc_val: 0.8649 time: 1.1464s\n",
      "Test set results: loss= 0.4192 accuracy= 0.8488 1-hop accuracy = 0.9778\n",
      "Epoch: 0127 loss_train: 0.3682 acc_train: 0.8649 loss_val: 0.3667 acc_val: 0.8652 time: 1.1140s\n",
      "Test set results: loss= 0.4183 accuracy= 0.8509 1-hop accuracy = 0.9781\n",
      "Epoch: 0128 loss_train: 0.3667 acc_train: 0.8652 loss_val: 0.3654 acc_val: 0.8654 time: 1.1354s\n",
      "Test set results: loss= 0.4168 accuracy= 0.8528 1-hop accuracy = 0.9781\n",
      "Epoch: 0129 loss_train: 0.3654 acc_train: 0.8654 loss_val: 0.3642 acc_val: 0.8672 time: 1.1333s\n",
      "Test set results: loss= 0.4159 accuracy= 0.8528 1-hop accuracy = 0.9790\n",
      "Epoch: 0130 loss_train: 0.3642 acc_train: 0.8672 loss_val: 0.3627 acc_val: 0.8672 time: 1.1010s\n",
      "Test set results: loss= 0.4149 accuracy= 0.8525 1-hop accuracy = 0.9784\n",
      "Epoch: 0131 loss_train: 0.3627 acc_train: 0.8672 loss_val: 0.3614 acc_val: 0.8671 time: 1.1313s\n",
      "Test set results: loss= 0.4134 accuracy= 0.8531 1-hop accuracy = 0.9778\n",
      "Epoch: 0132 loss_train: 0.3614 acc_train: 0.8671 loss_val: 0.3602 acc_val: 0.8660 time: 1.1183s\n",
      "Test set results: loss= 0.4122 accuracy= 0.8485 1-hop accuracy = 0.9778\n",
      "Epoch: 0133 loss_train: 0.3602 acc_train: 0.8660 loss_val: 0.3593 acc_val: 0.8657 time: 1.1317s\n",
      "Test set results: loss= 0.4118 accuracy= 0.8525 1-hop accuracy = 0.9778\n",
      "Epoch: 0134 loss_train: 0.3593 acc_train: 0.8657 loss_val: 0.3584 acc_val: 0.8686 time: 1.1316s\n",
      "Test set results: loss= 0.4102 accuracy= 0.8512 1-hop accuracy = 0.9793\n",
      "Epoch: 0135 loss_train: 0.3584 acc_train: 0.8686 loss_val: 0.3575 acc_val: 0.8669 time: 1.1151s\n",
      "Test set results: loss= 0.4110 accuracy= 0.8500 1-hop accuracy = 0.9772\n",
      "Epoch: 0136 loss_train: 0.3575 acc_train: 0.8669 loss_val: 0.3557 acc_val: 0.8707 time: 1.1272s\n",
      "Test set results: loss= 0.4075 accuracy= 0.8546 1-hop accuracy = 0.9796\n",
      "Epoch: 0137 loss_train: 0.3557 acc_train: 0.8707 loss_val: 0.3543 acc_val: 0.8699 time: 1.1127s\n",
      "Test set results: loss= 0.4080 accuracy= 0.8534 1-hop accuracy = 0.9772\n",
      "Epoch: 0138 loss_train: 0.3543 acc_train: 0.8699 loss_val: 0.3531 acc_val: 0.8678 time: 1.1519s\n",
      "Test set results: loss= 0.4055 accuracy= 0.8559 1-hop accuracy = 0.9781\n",
      "Epoch: 0139 loss_train: 0.3531 acc_train: 0.8678 loss_val: 0.3512 acc_val: 0.8733 time: 1.1451s\n",
      "Test set results: loss= 0.4049 accuracy= 0.8543 1-hop accuracy = 0.9781\n",
      "Epoch: 0140 loss_train: 0.3512 acc_train: 0.8733 loss_val: 0.3502 acc_val: 0.8711 time: 1.1328s\n",
      "Test set results: loss= 0.4035 accuracy= 0.8549 1-hop accuracy = 0.9772\n",
      "Epoch: 0141 loss_train: 0.3502 acc_train: 0.8711 loss_val: 0.3493 acc_val: 0.8691 time: 1.1347s\n",
      "Test set results: loss= 0.4024 accuracy= 0.8549 1-hop accuracy = 0.9781\n",
      "Epoch: 0142 loss_train: 0.3493 acc_train: 0.8691 loss_val: 0.3480 acc_val: 0.8716 time: 1.1293s\n",
      "Test set results: loss= 0.4026 accuracy= 0.8543 1-hop accuracy = 0.9762\n",
      "Epoch: 0143 loss_train: 0.3480 acc_train: 0.8716 loss_val: 0.3467 acc_val: 0.8714 time: 1.1251s\n",
      "Test set results: loss= 0.3999 accuracy= 0.8556 1-hop accuracy = 0.9772\n",
      "Epoch: 0144 loss_train: 0.3467 acc_train: 0.8714 loss_val: 0.3453 acc_val: 0.8722 time: 1.0996s\n",
      "Test set results: loss= 0.3998 accuracy= 0.8571 1-hop accuracy = 0.9769\n",
      "Epoch: 0145 loss_train: 0.3453 acc_train: 0.8722 loss_val: 0.3440 acc_val: 0.8731 time: 1.1124s\n",
      "Test set results: loss= 0.3983 accuracy= 0.8543 1-hop accuracy = 0.9772\n",
      "Epoch: 0146 loss_train: 0.3440 acc_train: 0.8731 loss_val: 0.3433 acc_val: 0.8703 time: 1.1109s\n",
      "Test set results: loss= 0.3968 accuracy= 0.8562 1-hop accuracy = 0.9765\n",
      "Epoch: 0147 loss_train: 0.3433 acc_train: 0.8703 loss_val: 0.3425 acc_val: 0.8723 time: 1.1032s\n",
      "Test set results: loss= 0.3983 accuracy= 0.8537 1-hop accuracy = 0.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0148 loss_train: 0.3425 acc_train: 0.8723 loss_val: 0.3409 acc_val: 0.8718 time: 1.1365s\n",
      "Test set results: loss= 0.3948 accuracy= 0.8580 1-hop accuracy = 0.9772\n",
      "Epoch: 0149 loss_train: 0.3409 acc_train: 0.8718 loss_val: 0.3396 acc_val: 0.8725 time: 1.1243s\n",
      "Test set results: loss= 0.3943 accuracy= 0.8568 1-hop accuracy = 0.9765\n",
      "Epoch: 0150 loss_train: 0.3396 acc_train: 0.8725 loss_val: 0.3386 acc_val: 0.8744 time: 1.1194s\n",
      "Test set results: loss= 0.3940 accuracy= 0.8559 1-hop accuracy = 0.9769\n",
      "Epoch: 0151 loss_train: 0.3386 acc_train: 0.8744 loss_val: 0.3377 acc_val: 0.8730 time: 1.1035s\n",
      "Test set results: loss= 0.3920 accuracy= 0.8605 1-hop accuracy = 0.9769\n",
      "Epoch: 0152 loss_train: 0.3377 acc_train: 0.8730 loss_val: 0.3367 acc_val: 0.8721 time: 1.1168s\n",
      "Test set results: loss= 0.3926 accuracy= 0.8552 1-hop accuracy = 0.9765\n",
      "Epoch: 0153 loss_train: 0.3367 acc_train: 0.8721 loss_val: 0.3358 acc_val: 0.8726 time: 1.1115s\n",
      "Test set results: loss= 0.3908 accuracy= 0.8571 1-hop accuracy = 0.9765\n",
      "Epoch: 0154 loss_train: 0.3358 acc_train: 0.8726 loss_val: 0.3348 acc_val: 0.8730 time: 1.1332s\n",
      "Test set results: loss= 0.3903 accuracy= 0.8556 1-hop accuracy = 0.9756\n",
      "Epoch: 0155 loss_train: 0.3348 acc_train: 0.8730 loss_val: 0.3336 acc_val: 0.8758 time: 1.1024s\n",
      "Test set results: loss= 0.3893 accuracy= 0.8568 1-hop accuracy = 0.9769\n",
      "Epoch: 0156 loss_train: 0.3336 acc_train: 0.8758 loss_val: 0.3327 acc_val: 0.8745 time: 1.1389s\n",
      "Test set results: loss= 0.3885 accuracy= 0.8586 1-hop accuracy = 0.9759\n",
      "Epoch: 0157 loss_train: 0.3327 acc_train: 0.8745 loss_val: 0.3317 acc_val: 0.8766 time: 1.1120s\n",
      "Test set results: loss= 0.3878 accuracy= 0.8605 1-hop accuracy = 0.9781\n",
      "Epoch: 0158 loss_train: 0.3317 acc_train: 0.8766 loss_val: 0.3308 acc_val: 0.8758 time: 1.1341s\n",
      "Test set results: loss= 0.3867 accuracy= 0.8596 1-hop accuracy = 0.9762\n",
      "Epoch: 0159 loss_train: 0.3308 acc_train: 0.8758 loss_val: 0.3300 acc_val: 0.8753 time: 1.1325s\n",
      "Test set results: loss= 0.3866 accuracy= 0.8593 1-hop accuracy = 0.9781\n",
      "Epoch: 0160 loss_train: 0.3300 acc_train: 0.8753 loss_val: 0.3294 acc_val: 0.8759 time: 1.1316s\n",
      "Test set results: loss= 0.3858 accuracy= 0.8596 1-hop accuracy = 0.9769\n",
      "Epoch: 0161 loss_train: 0.3294 acc_train: 0.8759 loss_val: 0.3290 acc_val: 0.8757 time: 1.1018s\n",
      "Test set results: loss= 0.3850 accuracy= 0.8639 1-hop accuracy = 0.9787\n",
      "Epoch: 0162 loss_train: 0.3290 acc_train: 0.8757 loss_val: 0.3288 acc_val: 0.8770 time: 1.1311s\n",
      "Test set results: loss= 0.3863 accuracy= 0.8562 1-hop accuracy = 0.9769\n",
      "Epoch: 0163 loss_train: 0.3288 acc_train: 0.8770 loss_val: 0.3281 acc_val: 0.8737 time: 1.1348s\n",
      "Test set results: loss= 0.3834 accuracy= 0.8614 1-hop accuracy = 0.9778\n",
      "Epoch: 0164 loss_train: 0.3281 acc_train: 0.8737 loss_val: 0.3271 acc_val: 0.8750 time: 1.1257s\n",
      "Test set results: loss= 0.3851 accuracy= 0.8549 1-hop accuracy = 0.9765\n",
      "Epoch: 0165 loss_train: 0.3271 acc_train: 0.8750 loss_val: 0.3254 acc_val: 0.8770 time: 1.1158s\n",
      "Test set results: loss= 0.3819 accuracy= 0.8630 1-hop accuracy = 0.9781\n",
      "Epoch: 0166 loss_train: 0.3254 acc_train: 0.8770 loss_val: 0.3245 acc_val: 0.8753 time: 1.1293s\n",
      "Test set results: loss= 0.3819 accuracy= 0.8602 1-hop accuracy = 0.9759\n",
      "Epoch: 0167 loss_train: 0.3245 acc_train: 0.8753 loss_val: 0.3239 acc_val: 0.8762 time: 1.1396s\n",
      "Test set results: loss= 0.3818 accuracy= 0.8559 1-hop accuracy = 0.9759\n",
      "Epoch: 0168 loss_train: 0.3239 acc_train: 0.8762 loss_val: 0.3232 acc_val: 0.8753 time: 1.1410s\n",
      "Test set results: loss= 0.3805 accuracy= 0.8577 1-hop accuracy = 0.9772\n",
      "Epoch: 0169 loss_train: 0.3232 acc_train: 0.8753 loss_val: 0.3222 acc_val: 0.8767 time: 1.1315s\n",
      "Test set results: loss= 0.3800 accuracy= 0.8571 1-hop accuracy = 0.9765\n",
      "Epoch: 0170 loss_train: 0.3222 acc_train: 0.8767 loss_val: 0.3208 acc_val: 0.8772 time: 1.1240s\n",
      "Test set results: loss= 0.3790 accuracy= 0.8627 1-hop accuracy = 0.9775\n",
      "Epoch: 0171 loss_train: 0.3208 acc_train: 0.8772 loss_val: 0.3198 acc_val: 0.8773 time: 1.1144s\n",
      "Test set results: loss= 0.3778 accuracy= 0.8599 1-hop accuracy = 0.9765\n",
      "Epoch: 0172 loss_train: 0.3198 acc_train: 0.8773 loss_val: 0.3193 acc_val: 0.8751 time: 1.1364s\n",
      "Test set results: loss= 0.3777 accuracy= 0.8571 1-hop accuracy = 0.9765\n",
      "Epoch: 0173 loss_train: 0.3193 acc_train: 0.8751 loss_val: 0.3185 acc_val: 0.8808 time: 1.1053s\n",
      "Test set results: loss= 0.3768 accuracy= 0.8627 1-hop accuracy = 0.9784\n",
      "Epoch: 0174 loss_train: 0.3185 acc_train: 0.8808 loss_val: 0.3176 acc_val: 0.8769 time: 1.1292s\n",
      "Test set results: loss= 0.3766 accuracy= 0.8574 1-hop accuracy = 0.9756\n",
      "Epoch: 0175 loss_train: 0.3176 acc_train: 0.8769 loss_val: 0.3167 acc_val: 0.8765 time: 1.1262s\n",
      "Test set results: loss= 0.3760 accuracy= 0.8605 1-hop accuracy = 0.9765\n",
      "Epoch: 0176 loss_train: 0.3167 acc_train: 0.8765 loss_val: 0.3159 acc_val: 0.8787 time: 1.1297s\n",
      "Test set results: loss= 0.3743 accuracy= 0.8602 1-hop accuracy = 0.9765\n",
      "Epoch: 0177 loss_train: 0.3159 acc_train: 0.8787 loss_val: 0.3150 acc_val: 0.8779 time: 1.0952s\n",
      "Test set results: loss= 0.3747 accuracy= 0.8571 1-hop accuracy = 0.9756\n",
      "Epoch: 0178 loss_train: 0.3150 acc_train: 0.8779 loss_val: 0.3143 acc_val: 0.8805 time: 1.1312s\n",
      "Test set results: loss= 0.3734 accuracy= 0.8614 1-hop accuracy = 0.9784\n",
      "Epoch: 0179 loss_train: 0.3143 acc_train: 0.8805 loss_val: 0.3140 acc_val: 0.8813 time: 1.1320s\n",
      "Test set results: loss= 0.3737 accuracy= 0.8614 1-hop accuracy = 0.9765\n",
      "Epoch: 0180 loss_train: 0.3140 acc_train: 0.8813 loss_val: 0.3134 acc_val: 0.8773 time: 1.1212s\n",
      "Test set results: loss= 0.3729 accuracy= 0.8580 1-hop accuracy = 0.9772\n",
      "Epoch: 0181 loss_train: 0.3134 acc_train: 0.8773 loss_val: 0.3124 acc_val: 0.8798 time: 1.1332s\n",
      "Test set results: loss= 0.3723 accuracy= 0.8614 1-hop accuracy = 0.9762\n",
      "Epoch: 0182 loss_train: 0.3124 acc_train: 0.8798 loss_val: 0.3114 acc_val: 0.8813 time: 1.1370s\n",
      "Test set results: loss= 0.3709 accuracy= 0.8614 1-hop accuracy = 0.9784\n",
      "Epoch: 0183 loss_train: 0.3114 acc_train: 0.8813 loss_val: 0.3108 acc_val: 0.8763 time: 1.1234s\n",
      "Test set results: loss= 0.3713 accuracy= 0.8605 1-hop accuracy = 0.9759\n",
      "Epoch: 0184 loss_train: 0.3108 acc_train: 0.8763 loss_val: 0.3103 acc_val: 0.8798 time: 1.1213s\n",
      "Test set results: loss= 0.3699 accuracy= 0.8583 1-hop accuracy = 0.9765\n",
      "Epoch: 0185 loss_train: 0.3103 acc_train: 0.8798 loss_val: 0.3097 acc_val: 0.8799 time: 1.0996s\n",
      "Test set results: loss= 0.3707 accuracy= 0.8648 1-hop accuracy = 0.9781\n",
      "Epoch: 0186 loss_train: 0.3097 acc_train: 0.8799 loss_val: 0.3090 acc_val: 0.8777 time: 1.1354s\n",
      "Test set results: loss= 0.3689 accuracy= 0.8577 1-hop accuracy = 0.9765\n",
      "Epoch: 0187 loss_train: 0.3090 acc_train: 0.8777 loss_val: 0.3081 acc_val: 0.8807 time: 1.1167s\n",
      "Test set results: loss= 0.3692 accuracy= 0.8642 1-hop accuracy = 0.9772\n",
      "Epoch: 0188 loss_train: 0.3081 acc_train: 0.8807 loss_val: 0.3072 acc_val: 0.8814 time: 1.0970s\n",
      "Test set results: loss= 0.3680 accuracy= 0.8583 1-hop accuracy = 0.9775\n",
      "Epoch: 0189 loss_train: 0.3072 acc_train: 0.8814 loss_val: 0.3063 acc_val: 0.8807 time: 1.0953s\n",
      "Test set results: loss= 0.3673 accuracy= 0.8636 1-hop accuracy = 0.9765\n",
      "Epoch: 0190 loss_train: 0.3063 acc_train: 0.8807 loss_val: 0.3057 acc_val: 0.8823 time: 1.1251s\n",
      "Test set results: loss= 0.3667 accuracy= 0.8657 1-hop accuracy = 0.9784\n",
      "Epoch: 0191 loss_train: 0.3057 acc_train: 0.8823 loss_val: 0.3052 acc_val: 0.8811 time: 1.1219s\n",
      "Test set results: loss= 0.3671 accuracy= 0.8605 1-hop accuracy = 0.9759\n",
      "Epoch: 0192 loss_train: 0.3052 acc_train: 0.8811 loss_val: 0.3047 acc_val: 0.8796 time: 1.1233s\n",
      "Test set results: loss= 0.3653 accuracy= 0.8608 1-hop accuracy = 0.9784\n",
      "Epoch: 0193 loss_train: 0.3047 acc_train: 0.8796 loss_val: 0.3040 acc_val: 0.8816 time: 1.1116s\n",
      "Test set results: loss= 0.3664 accuracy= 0.8617 1-hop accuracy = 0.9762\n",
      "Epoch: 0194 loss_train: 0.3040 acc_train: 0.8816 loss_val: 0.3029 acc_val: 0.8830 time: 1.1483s\n",
      "Test set results: loss= 0.3643 accuracy= 0.8620 1-hop accuracy = 0.9787\n",
      "Epoch: 0195 loss_train: 0.3029 acc_train: 0.8830 loss_val: 0.3021 acc_val: 0.8825 time: 1.1220s\n",
      "Test set results: loss= 0.3639 accuracy= 0.8639 1-hop accuracy = 0.9772\n",
      "Epoch: 0196 loss_train: 0.3021 acc_train: 0.8825 loss_val: 0.3016 acc_val: 0.8821 time: 1.1354s\n",
      "Test set results: loss= 0.3643 accuracy= 0.8611 1-hop accuracy = 0.9765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0197 loss_train: 0.3016 acc_train: 0.8821 loss_val: 0.3012 acc_val: 0.8828 time: 1.1305s\n",
      "Test set results: loss= 0.3631 accuracy= 0.8642 1-hop accuracy = 0.9787\n",
      "Epoch: 0198 loss_train: 0.3012 acc_train: 0.8828 loss_val: 0.3007 acc_val: 0.8821 time: 1.1389s\n",
      "Test set results: loss= 0.3633 accuracy= 0.8590 1-hop accuracy = 0.9769\n",
      "Epoch: 0199 loss_train: 0.3007 acc_train: 0.8821 loss_val: 0.3002 acc_val: 0.8840 time: 1.1096s\n",
      "Test set results: loss= 0.3633 accuracy= 0.8664 1-hop accuracy = 0.9790\n",
      "Epoch: 0200 loss_train: 0.3002 acc_train: 0.8840 loss_val: 0.2997 acc_val: 0.8821 time: 1.1028s\n",
      "Test set results: loss= 0.3617 accuracy= 0.8605 1-hop accuracy = 0.9775\n",
      "Epoch: 0201 loss_train: 0.2997 acc_train: 0.8821 loss_val: 0.2989 acc_val: 0.8824 time: 1.1182s\n",
      "Test set results: loss= 0.3626 accuracy= 0.8654 1-hop accuracy = 0.9765\n",
      "Epoch: 0202 loss_train: 0.2989 acc_train: 0.8824 loss_val: 0.2983 acc_val: 0.8837 time: 1.1146s\n",
      "Test set results: loss= 0.3606 accuracy= 0.8642 1-hop accuracy = 0.9793\n",
      "Epoch: 0203 loss_train: 0.2983 acc_train: 0.8837 loss_val: 0.2983 acc_val: 0.8808 time: 1.1651s\n",
      "Test set results: loss= 0.3616 accuracy= 0.8651 1-hop accuracy = 0.9772\n",
      "Epoch: 0204 loss_train: 0.2983 acc_train: 0.8808 loss_val: 0.2980 acc_val: 0.8817 time: 1.1216s\n",
      "Test set results: loss= 0.3604 accuracy= 0.8623 1-hop accuracy = 0.9781\n",
      "Epoch: 0205 loss_train: 0.2980 acc_train: 0.8817 loss_val: 0.2973 acc_val: 0.8835 time: 1.1187s\n",
      "Test set results: loss= 0.3611 accuracy= 0.8636 1-hop accuracy = 0.9765\n",
      "Epoch: 0206 loss_train: 0.2973 acc_train: 0.8835 loss_val: 0.2964 acc_val: 0.8855 time: 1.1279s\n",
      "Test set results: loss= 0.3593 accuracy= 0.8670 1-hop accuracy = 0.9790\n",
      "Epoch: 0207 loss_train: 0.2964 acc_train: 0.8855 loss_val: 0.2952 acc_val: 0.8843 time: 1.1039s\n",
      "Test set results: loss= 0.3596 accuracy= 0.8645 1-hop accuracy = 0.9772\n",
      "Epoch: 0208 loss_train: 0.2952 acc_train: 0.8843 loss_val: 0.2943 acc_val: 0.8837 time: 1.1271s\n",
      "Test set results: loss= 0.3571 accuracy= 0.8651 1-hop accuracy = 0.9778\n",
      "Epoch: 0209 loss_train: 0.2943 acc_train: 0.8837 loss_val: 0.2935 acc_val: 0.8869 time: 1.1385s\n",
      "Test set results: loss= 0.3579 accuracy= 0.8657 1-hop accuracy = 0.9787\n",
      "Epoch: 0210 loss_train: 0.2935 acc_train: 0.8869 loss_val: 0.2930 acc_val: 0.8850 time: 1.1359s\n",
      "Test set results: loss= 0.3567 accuracy= 0.8660 1-hop accuracy = 0.9775\n",
      "Epoch: 0211 loss_train: 0.2930 acc_train: 0.8850 loss_val: 0.2925 acc_val: 0.8870 time: 1.1288s\n",
      "Test set results: loss= 0.3563 accuracy= 0.8664 1-hop accuracy = 0.9796\n",
      "Epoch: 0212 loss_train: 0.2925 acc_train: 0.8870 loss_val: 0.2922 acc_val: 0.8852 time: 1.1338s\n",
      "Test set results: loss= 0.3571 accuracy= 0.8651 1-hop accuracy = 0.9775\n",
      "Epoch: 0213 loss_train: 0.2922 acc_train: 0.8852 loss_val: 0.2920 acc_val: 0.8867 time: 1.1318s\n",
      "Test set results: loss= 0.3556 accuracy= 0.8679 1-hop accuracy = 0.9796\n",
      "Epoch: 0214 loss_train: 0.2920 acc_train: 0.8867 loss_val: 0.2916 acc_val: 0.8848 time: 1.1128s\n",
      "Test set results: loss= 0.3572 accuracy= 0.8636 1-hop accuracy = 0.9775\n",
      "Epoch: 0215 loss_train: 0.2916 acc_train: 0.8848 loss_val: 0.2908 acc_val: 0.8844 time: 1.1091s\n",
      "Test set results: loss= 0.3549 accuracy= 0.8636 1-hop accuracy = 0.9772\n",
      "Epoch: 0216 loss_train: 0.2908 acc_train: 0.8844 loss_val: 0.2898 acc_val: 0.8878 time: 1.1109s\n",
      "Test set results: loss= 0.3549 accuracy= 0.8667 1-hop accuracy = 0.9790\n",
      "Epoch: 0217 loss_train: 0.2898 acc_train: 0.8878 loss_val: 0.2890 acc_val: 0.8858 time: 1.1197s\n",
      "Test set results: loss= 0.3541 accuracy= 0.8664 1-hop accuracy = 0.9769\n",
      "Epoch: 0218 loss_train: 0.2890 acc_train: 0.8858 loss_val: 0.2884 acc_val: 0.8896 time: 1.1130s\n",
      "Test set results: loss= 0.3529 accuracy= 0.8688 1-hop accuracy = 0.9790\n",
      "Epoch: 0219 loss_train: 0.2884 acc_train: 0.8896 loss_val: 0.2879 acc_val: 0.8871 time: 1.1135s\n",
      "Test set results: loss= 0.3540 accuracy= 0.8657 1-hop accuracy = 0.9778\n",
      "Epoch: 0220 loss_train: 0.2879 acc_train: 0.8871 loss_val: 0.2877 acc_val: 0.8887 time: 1.1216s\n",
      "Test set results: loss= 0.3522 accuracy= 0.8691 1-hop accuracy = 0.9793\n",
      "Epoch: 0221 loss_train: 0.2877 acc_train: 0.8887 loss_val: 0.2875 acc_val: 0.8858 time: 1.1841s\n",
      "Test set results: loss= 0.3537 accuracy= 0.8676 1-hop accuracy = 0.9778\n",
      "Epoch: 0222 loss_train: 0.2875 acc_train: 0.8858 loss_val: 0.2873 acc_val: 0.8880 time: 1.1767s\n",
      "Test set results: loss= 0.3523 accuracy= 0.8694 1-hop accuracy = 0.9793\n",
      "Epoch: 0223 loss_train: 0.2873 acc_train: 0.8880 loss_val: 0.2869 acc_val: 0.8857 time: 1.1470s\n",
      "Test set results: loss= 0.3530 accuracy= 0.8676 1-hop accuracy = 0.9781\n",
      "Epoch: 0224 loss_train: 0.2869 acc_train: 0.8857 loss_val: 0.2861 acc_val: 0.8854 time: 1.3067s\n",
      "Test set results: loss= 0.3515 accuracy= 0.8667 1-hop accuracy = 0.9787\n",
      "Epoch: 0225 loss_train: 0.2861 acc_train: 0.8854 loss_val: 0.2857 acc_val: 0.8854 time: 1.0605s\n",
      "Test set results: loss= 0.3517 accuracy= 0.8701 1-hop accuracy = 0.9772\n",
      "Epoch: 0226 loss_train: 0.2857 acc_train: 0.8854 loss_val: 0.2848 acc_val: 0.8889 time: 1.1395s\n",
      "Test set results: loss= 0.3508 accuracy= 0.8673 1-hop accuracy = 0.9802\n",
      "Epoch: 0227 loss_train: 0.2848 acc_train: 0.8889 loss_val: 0.2842 acc_val: 0.8870 time: 1.1353s\n",
      "Test set results: loss= 0.3501 accuracy= 0.8688 1-hop accuracy = 0.9781\n",
      "Epoch: 0228 loss_train: 0.2842 acc_train: 0.8870 loss_val: 0.2836 acc_val: 0.8882 time: 1.1220s\n",
      "Test set results: loss= 0.3502 accuracy= 0.8673 1-hop accuracy = 0.9778\n",
      "Epoch: 0229 loss_train: 0.2836 acc_train: 0.8882 loss_val: 0.2829 acc_val: 0.8915 time: 1.1390s\n",
      "Test set results: loss= 0.3488 accuracy= 0.8722 1-hop accuracy = 0.9802\n",
      "Epoch: 0230 loss_train: 0.2829 acc_train: 0.8915 loss_val: 0.2823 acc_val: 0.8894 time: 1.1465s\n",
      "Test set results: loss= 0.3493 accuracy= 0.8694 1-hop accuracy = 0.9781\n",
      "Epoch: 0231 loss_train: 0.2823 acc_train: 0.8894 loss_val: 0.2817 acc_val: 0.8897 time: 1.1189s\n",
      "Test set results: loss= 0.3477 accuracy= 0.8673 1-hop accuracy = 0.9793\n",
      "Epoch: 0232 loss_train: 0.2817 acc_train: 0.8897 loss_val: 0.2813 acc_val: 0.8884 time: 1.1336s\n",
      "Test set results: loss= 0.3483 accuracy= 0.8701 1-hop accuracy = 0.9784\n",
      "Epoch: 0233 loss_train: 0.2813 acc_train: 0.8884 loss_val: 0.2807 acc_val: 0.8921 time: 1.1453s\n",
      "Test set results: loss= 0.3478 accuracy= 0.8682 1-hop accuracy = 0.9799\n",
      "Epoch: 0234 loss_train: 0.2807 acc_train: 0.8921 loss_val: 0.2803 acc_val: 0.8874 time: 1.1422s\n",
      "Test set results: loss= 0.3468 accuracy= 0.8710 1-hop accuracy = 0.9790\n",
      "Epoch: 0235 loss_train: 0.2803 acc_train: 0.8874 loss_val: 0.2799 acc_val: 0.8898 time: 1.1341s\n",
      "Test set results: loss= 0.3477 accuracy= 0.8682 1-hop accuracy = 0.9787\n",
      "Epoch: 0236 loss_train: 0.2799 acc_train: 0.8898 loss_val: 0.2795 acc_val: 0.8901 time: 1.1377s\n",
      "Test set results: loss= 0.3461 accuracy= 0.8722 1-hop accuracy = 0.9806\n",
      "Epoch: 0237 loss_train: 0.2795 acc_train: 0.8901 loss_val: 0.2790 acc_val: 0.8901 time: 1.1374s\n",
      "Test set results: loss= 0.3468 accuracy= 0.8676 1-hop accuracy = 0.9781\n",
      "Epoch: 0238 loss_train: 0.2790 acc_train: 0.8901 loss_val: 0.2786 acc_val: 0.8902 time: 1.1200s\n",
      "Test set results: loss= 0.3464 accuracy= 0.8707 1-hop accuracy = 0.9802\n",
      "Epoch: 0239 loss_train: 0.2786 acc_train: 0.8902 loss_val: 0.2781 acc_val: 0.8892 time: 1.1093s\n",
      "Test set results: loss= 0.3453 accuracy= 0.8704 1-hop accuracy = 0.9790\n",
      "Epoch: 0240 loss_train: 0.2781 acc_train: 0.8892 loss_val: 0.2775 acc_val: 0.8909 time: 1.1053s\n",
      "Test set results: loss= 0.3462 accuracy= 0.8691 1-hop accuracy = 0.9793\n",
      "Epoch: 0241 loss_train: 0.2775 acc_train: 0.8909 loss_val: 0.2768 acc_val: 0.8890 time: 1.1340s\n",
      "Test set results: loss= 0.3439 accuracy= 0.8716 1-hop accuracy = 0.9796\n",
      "Epoch: 0242 loss_train: 0.2768 acc_train: 0.8890 loss_val: 0.2761 acc_val: 0.8915 time: 1.1228s\n",
      "Test set results: loss= 0.3448 accuracy= 0.8691 1-hop accuracy = 0.9802\n",
      "Epoch: 0243 loss_train: 0.2761 acc_train: 0.8915 loss_val: 0.2755 acc_val: 0.8915 time: 1.1139s\n",
      "Test set results: loss= 0.3435 accuracy= 0.8738 1-hop accuracy = 0.9796\n",
      "Epoch: 0244 loss_train: 0.2755 acc_train: 0.8915 loss_val: 0.2750 acc_val: 0.8921 time: 1.1055s\n",
      "Test set results: loss= 0.3433 accuracy= 0.8713 1-hop accuracy = 0.9799\n",
      "Epoch: 0245 loss_train: 0.2750 acc_train: 0.8921 loss_val: 0.2747 acc_val: 0.8934 time: 1.1384s\n",
      "Test set results: loss= 0.3435 accuracy= 0.8741 1-hop accuracy = 0.9809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0246 loss_train: 0.2747 acc_train: 0.8934 loss_val: 0.2745 acc_val: 0.8913 time: 1.1630s\n",
      "Test set results: loss= 0.3427 accuracy= 0.8713 1-hop accuracy = 0.9793\n",
      "Epoch: 0247 loss_train: 0.2745 acc_train: 0.8913 loss_val: 0.2742 acc_val: 0.8938 time: 1.1291s\n",
      "Test set results: loss= 0.3433 accuracy= 0.8747 1-hop accuracy = 0.9812\n",
      "Epoch: 0248 loss_train: 0.2742 acc_train: 0.8938 loss_val: 0.2741 acc_val: 0.8904 time: 1.1373s\n",
      "Test set results: loss= 0.3426 accuracy= 0.8707 1-hop accuracy = 0.9793\n",
      "Epoch: 0249 loss_train: 0.2741 acc_train: 0.8904 loss_val: 0.2739 acc_val: 0.8924 time: 1.1231s\n",
      "Test set results: loss= 0.3430 accuracy= 0.8744 1-hop accuracy = 0.9809\n",
      "Epoch: 0250 loss_train: 0.2739 acc_train: 0.8924 loss_val: 0.2740 acc_val: 0.8924 time: 1.1058s\n",
      "Test set results: loss= 0.3431 accuracy= 0.8728 1-hop accuracy = 0.9812\n",
      "Epoch: 0251 loss_train: 0.2740 acc_train: 0.8924 loss_val: 0.2738 acc_val: 0.8907 time: 1.1379s\n",
      "Test set results: loss= 0.3431 accuracy= 0.8728 1-hop accuracy = 0.9787\n",
      "Epoch: 0252 loss_train: 0.2738 acc_train: 0.8907 loss_val: 0.2738 acc_val: 0.8884 time: 1.1214s\n",
      "Test set results: loss= 0.3425 accuracy= 0.8710 1-hop accuracy = 0.9802\n",
      "Epoch: 0253 loss_train: 0.2738 acc_train: 0.8884 loss_val: 0.2739 acc_val: 0.8907 time: 1.0922s\n",
      "Test set results: loss= 0.3434 accuracy= 0.8756 1-hop accuracy = 0.9790\n",
      "Epoch: 0254 loss_train: 0.2739 acc_train: 0.8907 loss_val: 0.2731 acc_val: 0.8869 time: 1.1601s\n",
      "Test set results: loss= 0.3415 accuracy= 0.8691 1-hop accuracy = 0.9793\n",
      "Epoch: 0255 loss_train: 0.2731 acc_train: 0.8869 loss_val: 0.2720 acc_val: 0.8912 time: 1.1449s\n",
      "Test set results: loss= 0.3420 accuracy= 0.8762 1-hop accuracy = 0.9790\n",
      "Epoch: 0256 loss_train: 0.2720 acc_train: 0.8912 loss_val: 0.2707 acc_val: 0.8933 time: 1.1359s\n",
      "Test set results: loss= 0.3401 accuracy= 0.8747 1-hop accuracy = 0.9806\n",
      "Epoch: 0257 loss_train: 0.2707 acc_train: 0.8933 loss_val: 0.2699 acc_val: 0.8936 time: 1.1166s\n",
      "Test set results: loss= 0.3400 accuracy= 0.8744 1-hop accuracy = 0.9806\n",
      "Epoch: 0258 loss_train: 0.2699 acc_train: 0.8936 loss_val: 0.2695 acc_val: 0.8936 time: 1.1266s\n",
      "Test set results: loss= 0.3388 accuracy= 0.8747 1-hop accuracy = 0.9802\n",
      "Epoch: 0259 loss_train: 0.2695 acc_train: 0.8936 loss_val: 0.2695 acc_val: 0.8943 time: 1.1445s\n",
      "Test set results: loss= 0.3395 accuracy= 0.8765 1-hop accuracy = 0.9815\n",
      "Epoch: 0260 loss_train: 0.2695 acc_train: 0.8943 loss_val: 0.2695 acc_val: 0.8922 time: 1.0926s\n",
      "Test set results: loss= 0.3396 accuracy= 0.8769 1-hop accuracy = 0.9790\n",
      "Epoch: 0261 loss_train: 0.2695 acc_train: 0.8922 loss_val: 0.2688 acc_val: 0.8909 time: 1.1146s\n",
      "Test set results: loss= 0.3381 accuracy= 0.8719 1-hop accuracy = 0.9812\n",
      "Epoch: 0262 loss_train: 0.2688 acc_train: 0.8909 loss_val: 0.2681 acc_val: 0.8928 time: 1.1349s\n",
      "Test set results: loss= 0.3389 accuracy= 0.8753 1-hop accuracy = 0.9802\n",
      "Epoch: 0263 loss_train: 0.2681 acc_train: 0.8928 loss_val: 0.2678 acc_val: 0.8940 time: 1.1325s\n",
      "Test set results: loss= 0.3374 accuracy= 0.8759 1-hop accuracy = 0.9809\n",
      "Epoch: 0264 loss_train: 0.2678 acc_train: 0.8940 loss_val: 0.2676 acc_val: 0.8949 time: 1.1306s\n",
      "Test set results: loss= 0.3387 accuracy= 0.8753 1-hop accuracy = 0.9809\n",
      "Epoch: 0265 loss_train: 0.2676 acc_train: 0.8949 loss_val: 0.2670 acc_val: 0.8949 time: 1.1585s\n",
      "Test set results: loss= 0.3368 accuracy= 0.8762 1-hop accuracy = 0.9793\n",
      "Epoch: 0266 loss_train: 0.2670 acc_train: 0.8949 loss_val: 0.2661 acc_val: 0.8951 time: 1.1506s\n",
      "Test set results: loss= 0.3372 accuracy= 0.8772 1-hop accuracy = 0.9818\n",
      "Epoch: 0267 loss_train: 0.2661 acc_train: 0.8951 loss_val: 0.2655 acc_val: 0.8966 time: 1.1322s\n",
      "Test set results: loss= 0.3359 accuracy= 0.8781 1-hop accuracy = 0.9812\n",
      "Epoch: 0268 loss_train: 0.2655 acc_train: 0.8966 loss_val: 0.2656 acc_val: 0.8937 time: 1.1322s\n",
      "Test set results: loss= 0.3368 accuracy= 0.8741 1-hop accuracy = 0.9812\n",
      "Epoch: 0269 loss_train: 0.2656 acc_train: 0.8937 loss_val: 0.2660 acc_val: 0.8965 time: 1.1334s\n",
      "Test set results: loss= 0.3371 accuracy= 0.8769 1-hop accuracy = 0.9809\n",
      "Epoch: 0270 loss_train: 0.2660 acc_train: 0.8965 loss_val: 0.2661 acc_val: 0.8929 time: 1.1411s\n",
      "Test set results: loss= 0.3371 accuracy= 0.8744 1-hop accuracy = 0.9815\n",
      "Epoch: 0271 loss_train: 0.2661 acc_train: 0.8929 loss_val: 0.2654 acc_val: 0.8956 time: 1.1480s\n",
      "Test set results: loss= 0.3371 accuracy= 0.8796 1-hop accuracy = 0.9815\n",
      "Epoch: 0272 loss_train: 0.2654 acc_train: 0.8956 loss_val: 0.2642 acc_val: 0.8945 time: 1.1337s\n",
      "Test set results: loss= 0.3356 accuracy= 0.8747 1-hop accuracy = 0.9818\n",
      "Epoch: 0273 loss_train: 0.2642 acc_train: 0.8945 loss_val: 0.2634 acc_val: 0.8959 time: 1.1362s\n",
      "Test set results: loss= 0.3346 accuracy= 0.8778 1-hop accuracy = 0.9815\n",
      "Epoch: 0274 loss_train: 0.2634 acc_train: 0.8959 loss_val: 0.2631 acc_val: 0.8957 time: 1.1280s\n",
      "Test set results: loss= 0.3350 accuracy= 0.8781 1-hop accuracy = 0.9821\n",
      "Epoch: 0275 loss_train: 0.2631 acc_train: 0.8957 loss_val: 0.2628 acc_val: 0.8964 time: 1.1421s\n",
      "Test set results: loss= 0.3335 accuracy= 0.8765 1-hop accuracy = 0.9806\n",
      "Epoch: 0276 loss_train: 0.2628 acc_train: 0.8964 loss_val: 0.2623 acc_val: 0.8949 time: 1.1248s\n",
      "Test set results: loss= 0.3347 accuracy= 0.8775 1-hop accuracy = 0.9815\n",
      "Epoch: 0277 loss_train: 0.2623 acc_train: 0.8949 loss_val: 0.2617 acc_val: 0.8975 time: 1.1361s\n",
      "Test set results: loss= 0.3327 accuracy= 0.8772 1-hop accuracy = 0.9821\n",
      "Epoch: 0278 loss_train: 0.2617 acc_train: 0.8975 loss_val: 0.2616 acc_val: 0.8966 time: 1.1553s\n",
      "Test set results: loss= 0.3334 accuracy= 0.8821 1-hop accuracy = 0.9809\n",
      "Epoch: 0279 loss_train: 0.2616 acc_train: 0.8966 loss_val: 0.2614 acc_val: 0.8962 time: 1.1333s\n",
      "Test set results: loss= 0.3337 accuracy= 0.8784 1-hop accuracy = 0.9818\n",
      "Epoch: 0280 loss_train: 0.2614 acc_train: 0.8962 loss_val: 0.2612 acc_val: 0.8936 time: 1.1332s\n",
      "Test set results: loss= 0.3322 accuracy= 0.8784 1-hop accuracy = 0.9809\n",
      "Epoch: 0281 loss_train: 0.2612 acc_train: 0.8936 loss_val: 0.2607 acc_val: 0.8987 time: 1.1334s\n",
      "Test set results: loss= 0.3342 accuracy= 0.8796 1-hop accuracy = 0.9821\n",
      "Epoch: 0282 loss_train: 0.2607 acc_train: 0.8987 loss_val: 0.2600 acc_val: 0.8968 time: 1.1330s\n",
      "Test set results: loss= 0.3311 accuracy= 0.8781 1-hop accuracy = 0.9812\n",
      "Epoch: 0283 loss_train: 0.2600 acc_train: 0.8968 loss_val: 0.2597 acc_val: 0.8962 time: 1.1219s\n",
      "Test set results: loss= 0.3324 accuracy= 0.8778 1-hop accuracy = 0.9802\n",
      "Epoch: 0284 loss_train: 0.2597 acc_train: 0.8962 loss_val: 0.2595 acc_val: 0.8948 time: 1.1287s\n",
      "Test set results: loss= 0.3318 accuracy= 0.8759 1-hop accuracy = 0.9802\n",
      "Epoch: 0285 loss_train: 0.2595 acc_train: 0.8948 loss_val: 0.2595 acc_val: 0.8963 time: 1.1289s\n",
      "Test set results: loss= 0.3318 accuracy= 0.8762 1-hop accuracy = 0.9806\n",
      "Epoch: 0286 loss_train: 0.2595 acc_train: 0.8963 loss_val: 0.2592 acc_val: 0.8935 time: 1.1357s\n",
      "Test set results: loss= 0.3319 accuracy= 0.8762 1-hop accuracy = 0.9812\n",
      "Epoch: 0287 loss_train: 0.2592 acc_train: 0.8935 loss_val: 0.2589 acc_val: 0.8963 time: 1.1314s\n",
      "Test set results: loss= 0.3314 accuracy= 0.8793 1-hop accuracy = 0.9812\n",
      "Epoch: 0288 loss_train: 0.2589 acc_train: 0.8963 loss_val: 0.2584 acc_val: 0.8972 time: 1.1304s\n",
      "Test set results: loss= 0.3311 accuracy= 0.8818 1-hop accuracy = 0.9821\n",
      "Epoch: 0289 loss_train: 0.2584 acc_train: 0.8972 loss_val: 0.2579 acc_val: 0.8965 time: 1.1152s\n",
      "Test set results: loss= 0.3313 accuracy= 0.8784 1-hop accuracy = 0.9809\n",
      "Epoch: 0290 loss_train: 0.2579 acc_train: 0.8965 loss_val: 0.2574 acc_val: 0.8989 time: 1.1338s\n",
      "Test set results: loss= 0.3298 accuracy= 0.8809 1-hop accuracy = 0.9827\n",
      "Epoch: 0291 loss_train: 0.2574 acc_train: 0.8989 loss_val: 0.2571 acc_val: 0.8969 time: 1.1396s\n",
      "Test set results: loss= 0.3308 accuracy= 0.8772 1-hop accuracy = 0.9827\n",
      "Epoch: 0292 loss_train: 0.2571 acc_train: 0.8969 loss_val: 0.2567 acc_val: 0.8998 time: 1.1276s\n",
      "Test set results: loss= 0.3294 accuracy= 0.8809 1-hop accuracy = 0.9824\n",
      "Epoch: 0293 loss_train: 0.2567 acc_train: 0.8998 loss_val: 0.2563 acc_val: 0.8974 time: 1.1114s\n",
      "Test set results: loss= 0.3299 accuracy= 0.8809 1-hop accuracy = 0.9833\n",
      "Epoch: 0294 loss_train: 0.2563 acc_train: 0.8974 loss_val: 0.2559 acc_val: 0.8988 time: 1.1304s\n",
      "Test set results: loss= 0.3297 accuracy= 0.8802 1-hop accuracy = 0.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0295 loss_train: 0.2559 acc_train: 0.8988 loss_val: 0.2556 acc_val: 0.8983 time: 1.1332s\n",
      "Test set results: loss= 0.3288 accuracy= 0.8824 1-hop accuracy = 0.9830\n",
      "Epoch: 0296 loss_train: 0.2556 acc_train: 0.8983 loss_val: 0.2553 acc_val: 0.8980 time: 1.1227s\n",
      "Test set results: loss= 0.3294 accuracy= 0.8799 1-hop accuracy = 0.9821\n",
      "Epoch: 0297 loss_train: 0.2553 acc_train: 0.8980 loss_val: 0.2546 acc_val: 0.8993 time: 1.1132s\n",
      "Test set results: loss= 0.3284 accuracy= 0.8809 1-hop accuracy = 0.9830\n",
      "Epoch: 0298 loss_train: 0.2546 acc_train: 0.8993 loss_val: 0.2540 acc_val: 0.9011 time: 1.1321s\n",
      "Test set results: loss= 0.3276 accuracy= 0.8840 1-hop accuracy = 0.9830\n",
      "Epoch: 0299 loss_train: 0.2540 acc_train: 0.9011 loss_val: 0.2533 acc_val: 0.8995 time: 1.1202s\n",
      "Test set results: loss= 0.3277 accuracy= 0.8799 1-hop accuracy = 0.9833\n",
      "Epoch: 0300 loss_train: 0.2533 acc_train: 0.8995 loss_val: 0.2529 acc_val: 0.9021 time: 1.1480s\n",
      "Test set results: loss= 0.3265 accuracy= 0.8849 1-hop accuracy = 0.9830\n",
      "Optimization Finished!\n",
      "The max acc is 0.8849\n",
      "The test max acc is 0.8849\n",
      "The test argmax acc is 299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fef80a02ac0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXScd33v8fdXo33fLVvyvsQxSRxsxdkJWyAOi9laAu1lKblpCmlLaXsJl5ZD23PuYWm5hZJeN9AUaAGXlkBcMAkhUBLI4iV2vDuWZVu7rX0fSaP53j9mYhRZssbOyKMZfV7n6Hie5/l55vvkST756Te/5/mZuyMiIskvLdEFiIhIfCjQRURShAJdRCRFKNBFRFKEAl1EJEWkJ+qDy8vLfdmyZYn6eBGRpLRnz54Od6+Y6ljCAn3ZsmXs3r07UR8vIpKUzOz0dMc05CIikiJiCnQzu8PMjplZnZndP8XxEjP7gZntN7OdZnZV/EsVEZELmTHQzSwAPABsBtYB7zOzdZOa/W9gn7tfA3wA+HK8CxURkQuLpYe+Cahz93p3HwW2AVsmtVkHPAHg7keBZWa2IK6ViojIBcUS6NVA44Ttpui+iV4A3gVgZpuApUBNPAoUEZHYxBLoNsW+yU/0+hxQYmb7gD8E9gKh897I7B4z221mu9vb2y+6WBERmV4s0xabgMUTtmuAlokN3L0P+DCAmRlwMvrDpHYPAg8C1NbW6jGPIiJxFEsPfRew2syWm1kmcBewfWIDMyuOHgO4G3gyGvIiIgIcP9PP8Og4X/7ZcQ63zE48zthDd/eQmd0HPAYEgIfc/ZCZ3Rs9vhW4EviWmY0Dh4GPzEq1IiJxEhoP0zk4yoLC7POOPXOik3/87zo+89Z1LC7NpaFriFUV+aSlvXwE+ukTHXzr6dO8Z2MNpzoHefOrqnj0YBtdQ6PkZgT42dGzbFhSzGgozLefa2BhUTatvUFGQuOsW1QY93OyRC1wUVtb67pTVESm4+5ERnBfrnNghMz0NJq6h+kZGmPT8lIAfnH0LPsae1i/uJhbV5ezt6GHkdA4AHsbeni+oZu8zHTee91iTrQPsOtUFz8/epb/eesKflXXAcD6mmLO9AV5pr6T/mCIjIBhGKPjYcrzs1hRnscdV1VRmJPB3oZuvv1cA2kG4WiMBtKM8bCTnmaEws6K8jxOdg5iwOarFvL4kTPctqaCrb+7kUDaVF9PzszM9rh77ZTHFOgiEi+joTCPHz7Da6+oIC/rNwMA/cEx8jLT+enhM9y0qoycjAABM9LSjFMdg+RmBfir/zrM0tJcdp/uZlVlPk8db+eammI6B0YIjTvrFhWytCyPf/j5caoKsznTF6R7aIyXcvGlUDWDhYXZtPQGz31+msGaBQXUdwwyGgqf21+Uk0Hv8BivWlRIVnoaB1v6WFqaS2FOBv/rzVfwxNGzGLC0LI9dp7o40trH0bb+c3//Qzct46OvW8m/72xkVWU+X//VST5w41Levn4RPUNjFOdmcLZ/hLysdPKz0ukcGKE4N/OSwzxyfgp0EYkaDYUZHQ+Tn3X+iOuZviANXUNctyzS6x0cCbG/qZdn6zvZuLSEJ46coaIgi8Otfdxx1UKGR0P8y69P8a4N1XQOjnK6Y4hHD7VRXZzDb9XW8MSRsywozOKJo2dZVpbHyY5B1izIp7l7mGAozLKyXE60DxJIM8LuuP8mZEvzMukaHCU/K52akhyae4bpD4bIz0pnYCREmsFfvnUdXYOjuMNV1YXcsrqCD/zzczR0DfHXW66iPD+LUDjMNTXF5Gel81x9J8/Wd7FxaQkNXUO8fm0lz9Z38rb1iwik2bS/FbzE3WnoGgKgMDuDkrzMadvOFgW6yCzoGRrlxTMD537ld3e6hyJBNNFIaJys9AAQCdPM9NgeoRQOO197qp43rlvAyfZBrqou4mTHIA/8oo7NV1fxO9cvPdfu2ZOdVBVm09g9TFleJl/9eR2Z6WmsrMjn+Nl+CrIzqCzI4jVrKvjk9/fT1hvkTesW0D4wQlP3MG29QbIy0giNOwMjIT5x+xoONvfy08NnXlaTGS8LXYDC7HT6gr+ZpXzXdYvZ19jD0bZ+lpfncbYvyA0rynimvpPXr63kxwdauW5ZKa9eUsz+xl42LC3mYHMf79pQzarKfGpKcmnrDVJVlM0LjT1Ul+SwsiIfd2fP6W4qC7L5990NlORmcvetK8775zY2HiY07uRkBmK8kslFgS5yEXae7KK1d5gt1/7m/rnQeJifHj7D0dY+Vi0oIDcjwBceO8qLZwb48zdfwdG2ftZWFfC3Pz3Gh25axv++80oyAmn8554m7v/+fj540zKuri7iUw8f4FN3ruVs3whZ6WncsrqcLz3+IoMjId69sYbHDp2hZ2iUVRX5FOZk8I2nT1FVmE1bX5Alpbk09wwTMCMUDvOWaxZx4uwAjd1D9AdD58ZvM9PTXjasUBDtifePREK3JDeDq2uKOdLax6LiHGpKcqgqzKZ7aJTRUJi+YIgnX2wnI2B8+OblLC7NZXlZHk8eb+f3X7OCYChMVWE2e053094/whuurKS+fZAlZbnR//EU4g4n2gdYUZF/bnghHHbS0oyzfUHK8rNe0bDDfKZAl3mnPzhGRiCNZ+o7yQykUZ6fxcBIiOLcDE51DPLYoTYqC7JZvSCfzEAajx1qY09DNxuWlPDowTZGQmHe9epIoC8qzuEHe5tp7hl+2WdkZ6SRkxGge2js3L4FhVmc6RthfU0RvcNjtPQGKc/LPDee+9IXaGkWuTvPPdLDLS/Ior59kJLcDK6qLuJgcy/dQ2NUF0eGGioKsmjvH2FFRR7fufsG/vQ/9nGqY4gVFXksK8vj1UuK2Xmyi8KcDL63u5HKgiyuqSnGgM+9+xrSDPac7qaha4jXXVF5waGCcNg50T5AUW4GlQXnzwCRxFKgS9J6abiivX+E8bCzoDCL/U29fOuZ07QPjDAWChN258M3L6OpOxK4V1QV8Eff3Ut/MEQoPPW/3wVZ6ed6rC+5dXU5B5t7KcvPoiA7nRcaeyjIzqA/GJlJ8eGbl3Pr6nIau4YJjo2zuDSXX9V18A9PHGftwkJ+tL+FH370Zp6t7+TvHn+RW1aVU5CdzqffciV1Zwb4zs4G7r51BQ8/38T7r1+CO3z1F3X8wW0rWVWZz3d3NnD7ugXUlOQSHBvn0YNt3Lq6nP96oYVb11TQ2DXE2qpCqoouHLJn+oJkBtISMr4rs0+BLnNScGyc5093c8OKMtLSjN6hMeraB/jOcw28alEhXYOj/NOTJ3j92koeP3yGsHNu+KEwO53lFflkBdJoi36RN1F5fhZvW7+QtVUFFOVkMhIapygng86BUcryM7lpZTlH2/oYGAnR1hskI5DG29YvIhx2nMhYd//IGCW5mYyEpv4CcaJw2GnuGWZxaS4A42HXkILMCgW6JNRoKIzjtPQEWVYWuUnjuzsb+emhNuo7BnnL1QtZVZnPV35+HHfISk9jJDoGvLIijxPtg9ywopTb11XxdF0H1y0v5XdvWHouZHuHxviT7+3jtjUVvH5tJb+q6+C6ZSWsqixI5GmLzAoFulw27s5IKMzB5l5+tL+VrPQ0HnyqnpyMAEOj45TnZ9E7PApEhkZql5byzWdO4Q5vWreAW9dU8Pb1i+gPjuEO1cU50YAuTdlZCyIX40KBnrA1RSV1DIyE6OgfoSA7nU987wWeO9lJwIzB0chdereuLmdJaS4rK/I52NxLRWEWv3fz8nO3XN9720rqOwa4fnnZuWGKopyMc+//mjVTrocrIpMo0OWiBMfGOd05xNMnOvjOcw0MjITICKTR3DNMeX4mPUNj3Lq6nNbeIPe9bhVH2vq573WrLjj3uqooe8Yv+kRkZgp0mdGLZ/rZfaqb3ae72HGgleBYZHx7fU0ReVnpNPcMc/Oqcg639LHtnht49ZKSc39389ULE1W2yLyjQJfzPF3XgRO5K/BE+yBfePQo/cEQWelpvGdjDdevKGN1ZT5XLizE3Qn7bx5KpJkdIomjQBcAWnsjt3/vOtXF535ylInTt0tyM3jkYzeztCyX4tyXz202MwLRDFeYiySWAn0eC46N88O9zbT0DPO1p04yPBb5EvM1ayq4bmkJaWnGbWsqKMzOYElZboKrFZGZKNDnibHxMKOhMP/0yxPsPt3N4EiI+vbBc3dLXlNTxEdfu5KFRTlcU1N0wSfOicjcFFOgm9kdwJeJrFj0dXf/3KTjRcC/AUui7/m37v4vca5VLtGe09185Ju76B2OzO2+dnExRbmZvO3aIt5y9UKurimiICtdIS6S5GYMdDMLAA8AtxNZMHqXmW1398MTmn0MOOzubzOzCuCYmX3b3UdnpWqZUXBsnD/9jxe4fnkpX3z0GKX5mfzWxhpuXV2hed0iKSqWHvomoM7d6wHMbBuwhcjaoS9xoMAiXbx8oAsITX4juTyCY+N87cl6fry/lR/vbyU7I41v/d4mlpblJbo0EZlFsQR6NdA4YbsJuH5Sm68C24EWoAB4r7uHJ7XBzO4B7gFYsmTJpdQr03B36jsGGR4d5/1fe5a+YIgbV5QxNDbO+zctVpiLzAOxBPpUA6uTHwDzZmAf8HpgJfC4mT3l7n0v+0vuDwIPQuRZLhdfrkx0tj9Ia0+QLzx2lJ6hMQ61RP5xl+Vl8hdvuZK3rV805YrmIpKaYgn0JmDxhO0aIj3xiT4MfM4jT/qqM7OTwFpgZ1yqnOdC42F+sLeZX9d1sKaqgJGxMM/Ud7LzZBcQeVTs4tIc/uj1q9jb2MPvv2Ylt6wuT3DVInK5xRLou4DVZrYcaAbuAt4/qU0D8AbgKTNbAFwB1Mez0PnqUEsvn91+iF2nuinOzeCH+yL/L11dmc8nbl9DbmaAt1+7SCvLiMjMge7uITO7D3iMyLTFh9z9kJndGz2+Ffgb4BtmdoDIEM0n3b1jFutOefubevjKE3X87MgZCrLT+b/vXc87rq2mbzhEVkYa2Rl6lKyIvFxM89DdfQewY9K+rRNetwBvim9p89OBpl7+8pGD7GvsoTA7nT954xo+dPOyc4+TLcrNmOEdRGS+0p2ic0DP0CgPPlnP0yc6OdbWT3FuBn/xlit573WLKchWgItIbBToCRIOO08cPcuv6zp48sV2TncNsXFpCZuvquL+zWup1OwUEblICvQEcHfuf3g/39vdRHZGGiW5mWy75wauW1aa6NJEJIkp0C+j8bBzsLmXbz59iof3NvPR167kE7evIT0w/Wo+IiKxUqBfJiOhcT76b8/zxNGzmMHH37iaP37Daj0QS0TiRoE+y9ydv/7RYb7xdGRl+z970xruvHohKyryE12aiKQYBfoscnc+88gh/vXZ07zr1dXcefVC3rhuQaLLEpEUpUCfJe7OZ7dHwvye16zgU5vXanhFRGaVAj3OjrT28ZMDrfzyeAcvNPZw9y3LFeYiclko0OPoK08c50uPv0iawdU1xfzFW67kI7csV5iLyGWhQI+T4Ng4X3+qntvWVPDlu66lODcz0SWJyDyjCdBxMB52frC3mb5giLtvXa4wF5GEUA/9FegdHuO7Oxv41tOnaOkNsqQ0l5tW6jnkIpIYCvRLdKJ9gA8+tJOm7mFuXFHGx29fw2vXVBBI03i5iCSGAv0SuDsf+/bzDI+O85/33kitnsEiInNATGPoZnaHmR0zszozu3+K439uZvuiPwfNbNzMUjblnjnRydG2fj65ea3CXETmjBkD3cwCwAPAZmAd8D4zWzexjbt/0d2vdfdrgU8Bv3T3rtkoeC742lP1lOVl8vb1ixJdiojIObH00DcBde5e7+6jwDZgywXavw/4bjyKm4v2NnTzi2PtfPjmZVoGTkTmlFgCvRponLDdFN13HjPLBe4Avj/N8XvMbLeZ7W5vb7/YWueELz3+IiW5GXzo5uWJLkVE5GViCfSppm34NG3fBvx6uuEWd3/Q3WvdvbaioiLWGueMXae6eOp4B/fetpL8LH2fLCJzSyyB3gQsnrBdA7RM0/YuUnS4JTQe5v/sOEJ5fhYfuHFZossRETlPLIG+C1htZsvNLJNIaG+f3MjMioDbgEfiW+Lc8Pc/O87ehh7+8q1XkpOpsXMRmXtmHDdw95CZ3Qc8BgSAh9z9kJndGz2+Ndr0ncBP3X1w1qpNkMauIf7pyRO8a0M1W66d8usDEZGEi2kg2N13ADsm7ds6afsbwDfiVdhc8pUnjmNm/Pmbr0h0KSIi09LDuWbQFxzjkRda+O3aGhYW5SS6HBGRaSnQZ/DogTZGQ2HevaEm0aWIiFyQAv0CRkNhvr2zgWVluVy7uDjR5YiIXJAC/QI+/YMDvNDYwx++frVWHRKROU+BPo2x8TCPvNDC+zYt4d0bNdwiInOfAn0aL57pZzQU5saVZYkuRUQkJgr0aRxo6gXgmuqiBFciIhIbBfo09jf3UpCdztKy3ESXIiISEwX6NA409XJ1dZG+DBWRpKFAn8KpjkEONPdyk8bPRSSJKNCnsG1XI4E04z0bF8/cWERkjlCgT2H7vmZed0UFVUXZiS5FRCRmCvRJ+oNjtPQG2bC0JNGliIhcFAX6JPXtkaf/rijPT3AlIiIXR4E+SX3HAACrKvMSXImIyMWJKdDN7A4zO2ZmdWZ2/zRtXmtm+8zskJn9Mr5lXj717YME0owlpQp0EUkuMy5wYWYB4AHgdiLri+4ys+3ufnhCm2LgH4E73L3BzCpnq+DZVt8+yOKSHDLT9cuLiCSXWFJrE1Dn7vXuPgpsA7ZMavN+4GF3bwBw97PxLfPyOdE+wIoKjZ+LSPKJJdCrgcYJ203RfROtAUrM7L/NbI+ZfSBeBV5OwbFx6tsHWb1AgS4iySeWNUWnuvfdp3ifjcAbgBzgGTN71t1ffNkbmd0D3AOwZMmSi692lu1t6GF0PMymZaWJLkVE5KLF0kNvAibeMlkDtEzR5lF3H3T3DuBJYP3kN3L3B9291t1rKyoqLrXmWfPcyU7MoFaBLiJJKJZA3wWsNrPlZpYJ3AVsn9TmEeBWM0s3s1zgeuBIfEudfc/Wd7JuYSFFORmJLkVE5KLNOOTi7iEzuw94DAgAD7n7ITO7N3p8q7sfMbNHgf1AGPi6ux+czcLjLTQeZl9jD3ddN/eGgkREYhHLGDruvgPYMWnf1knbXwS+GL/SLq9TnYMEx8JcpQUtRCRJabJ11OHWfgDWLSxMcCUiIpdGgR51uKWPjICxqlJTFkUkOSnQo4609rGqskB3iIpI0lJ6RR1u7ePKhQWJLkNE5JIp0IGeoVHa+0e4YoECXUSSlwIdqDsbeWSubvkXkWSmQGdCoFeqhy4iyUuBDhw/O0B2RhrVxTmJLkVE5JIp0In00FeU55OWNtVzyEREkoMCnUiga/xcRJLdvA/0+vYBmnuGuVq3/ItIkpv3gf7955tIM3j7+kWJLkVE5BWZ14Hu7jz8fDO3ramgsjA70eWIiLwi8zrQj58doLU3yOarFia6FBGRV2xeB/pz9Z0AXL9CKxSJSPKb14H+7MkuqgqzWVKam+hSREResZgC3czuMLNjZlZnZvdPcfy1ZtZrZvuiP5+Jf6nx5e7sPNnF9StKMdP8cxFJfjOuWGRmAeAB4HYii0HvMrPt7n54UtOn3P2ts1DjrGjsGqa9f4TrtCC0iKSIWHrom4A6d69391FgG7BldsuafXsbuwHYsKQkwZWIiMRHLIFeDTRO2G6K7pvsRjN7wcx+YmavmuqNzOweM9ttZrvb29svodz42dvQQ25mgDW6Q1REUkQsgT7VALNP2n4eWOru64F/AH441Ru5+4PuXuvutRUVFRdXaZztbejmmpoi0gPz+nthEUkhsaRZE7B4wnYN0DKxgbv3uftA9PUOIMPMyuNWZZx1D45yuLWPaxdruEVEUkcsgb4LWG1my80sE7gL2D6xgZlVWXSqiJltir5vZ7yLjZdvPXOasXHnna+eauRIRCQ5zTjLxd1DZnYf8BgQAB5y90Nmdm/0+FbgPcAfmFkIGAbucvfJwzJzwmgozDefOcUbr6zkiiotaCEiqWPGQIdzwyg7Ju3bOuH1V4Gvxre02bHndDddg6P8du3imRuLiCSRefeN4K/q2gmkGTeuLEt0KSIicTXvAv2p4x1sWFJMQXZGoksREYmreRXoZ/qCHGju5ZZViZ0yKSIyG+ZVoG/b2Yg7bLlWi1mISOqZN4E+Hna27Wrg1tXlLCvPS3Q5IiJxN28C/UhrH629Qd69oSbRpYiIzIp5E+h7TkcexnXdcj1dUURS07wJ9N2nu6kqzGZRkdYOFZHUNG8C/fnT3WxcVqLFLEQkZc2LQG/tHaa5Z5iNeva5iKSweRHoz5/uAWDjUgW6iKSueRHoe053k52RxrpFhYkuRURk1syPQG/o5pqaYjK0mIWIpLCUT7jh0XEONfdquEVEUl7KB/r+ph5CYdcXoiKS8mIKdDO7w8yOmVmdmd1/gXbXmdm4mb0nfiW+MnsaIjcUbVAPXURS3IyBbmYB4AFgM7AOeJ+ZrZum3eeJrGw0Zzx/upsVFXmU5mUmuhQRkVkVSw99E1Dn7vXuPgpsA7ZM0e4Pge8DZ+NY3yvi7uw53a3hFhGZF2IJ9GqgccJ2U3TfOWZWDbwT2MoFmNk9ZrbbzHa3t7dfbK0X7XTnEN1DYxpuEZF5IZZAn+pe+ckLQP898El3H7/QG7n7g+5e6+61FRWzv8jE0bY+ANYt1PxzEUl9sSwS3QRMXFG5BmiZ1KYW2BZ9Tko5cKeZhdz9h3Gp8hIdbevHDNYsKEhkGSIil0Usgb4LWG1my4Fm4C7g/RMbuPvyl16b2TeAHyU6zAGOtfWztDSXnMxAoksREZl1Mwa6u4fM7D4is1cCwEPufsjM7o0ev+C4eSIda+vniir1zkVkfoilh4677wB2TNo3ZZC7+4deeVmvXHBsnFOdg7x1vdYPFZH5IWXvFD3U0kvY9YWoiMwfKRvoz9Z3AXDdMk1ZFJH5IYUDvZMrFhRQlp+V6FJERC6LlAz0sfEwe053c/0KLQgtIvNHSgb6geZehkbHuWFFWaJLERG5bFIy0J+t7wRg03L10EVk/kjJQH+uvovVlfmUa/xcROaRlAv0sfEwu091abhFROadlAv0A829DI6O6wtREZl3Ui7Qn67rAOCmleUJrkRE5PJKuUD/VV0Hr1pUqBWKRGTeSalAHxoN8fzpHm5Zpd65iMw/KRXo+xp7GB0Pc+NKfSEqIvNPSgX6weZeAK6pKU5wJSIil1+KBXof1cU5Gj8XkXkppkA3szvM7JiZ1ZnZ/VMc32Jm+81sX3QR6FviX+rMDrb08qpFelyuiMxPMwa6mQWAB4DNwDrgfWa2blKzJ4D17n4t8HvA1+Nd6EwGRkKc7Bjkquqiy/3RIiJzQiw99E1AnbvXu/sosA3YMrGBuw+4u0c38wDnMtvf1IM7XFWtHrqIzE+xBHo10Dhhuym672XM7J1mdhT4MZFe+mX1yxfbyQgYm5ZrhouIzE+xBLpNse+8Hri7/8Dd1wLvAP5myjcyuyc6xr67vb394iqdwS+PtVO7tJT8rJiWSRURSTmxBHoTsHjCdg3QMl1jd38SWGlm593d4+4Punutu9dWVFRcdLHTOdMX5GhbP7ddEb/3FBFJNrEE+i5gtZktN7NM4C5g+8QGZrbKzCz6egOQCXTGu9jp7D7VDcBNuqFIROaxGccn3D1kZvcBjwEB4CF3P2Rm90aPbwXeDXzAzMaAYeC9E74knXWHW3tJTzPWLCi4XB8pIjLnxDTg7O47gB2T9m2d8PrzwOfjW1rsDrf0saoyn+yMQKJKEBFJuJS4U/Rwax/rFmq6oojMb0kf6B0DI5zpG2Gd7hAVkXku6QP9xbZ+ANZWKdBFZH5L+kBv6Q0CUFOSk+BKREQSK+kDva13GICqouwEVyIikljJH+h9QYpyMjTDRUTmveQP9N4RqgrVOxcRSfpAP9MX1HCLiAgpEOhtfUH10EVESPJAHxsP0zEwwgL10EVEkjvQz/aP4I566CIiJHmgt0XnoC9UD11EJLkD/UxfJNAXqIcuIpLcgd4a7aFrlouISJIH+pm+IJnpaZTkZiS6FBGRhEvqQG/rjUxZjC6WJCIyr8UU6GZ2h5kdM7M6M7t/iuO/Y2b7oz9Pm9n6+Jd6Ps1BFxH5jRkD3cwCwAPAZmAd8D4zWzep2UngNne/Bvgb4MF4FzqVM31BzUEXEYmKpYe+Cahz93p3HwW2AVsmNnD3p929O7r5LFAT3zLP5+609gapKsya7Y8SEUkKsQR6NdA4Ybspum86HwF+MtUBM7vHzHab2e729vbYq5xCz9AYo6EwVUV6DrqICMQW6FN94+hTNjR7HZFA/+RUx939QXevdffaioqK2KucQlt0DrrG0EVEItJjaNMELJ6wXQO0TG5kZtcAXwc2u3tnfMqb3rlAL9KQi4gIxNZD3wWsNrPlZpYJ3AVsn9jAzJYADwP/w91fjH+Z52vujqxUtKhYQy4iIhBDD93dQ2Z2H/AYEAAecvdDZnZv9PhW4DNAGfCP0TnhIXevnb2yoblnmPQ0o7JAQy4iIhDbkAvuvgPYMWnf1gmv7wbujm9pF9bcPczC4mwCabqpSEQEkvhO0eaeYao13CIick7yBnr3MNXFuYkuQ0RkzkjKQB8NhTnTH6S6RD10EZGXJGWgt/UGcYcaDbmIiJyTlIHe1DMEoB66iMgESRnonQOjAFQU6KYiEZGXJGWgD46EAMjPimnWpYjIvJCUgT4QDfQ8BbqIyDnJHeiZgQRXIiIydyRloA+OhMjOSCM9kJTli4jMiqRMxIGRcY2fi4hMkpSBPjgS0vi5iMgkSRvo6qGLiLxcUgb6gHroIiLnSdpAVw9dROTlYgp0M7vDzI6ZWZ2Z3T/F8bVm9oyZjZjZn8W/zJfTGLqIyPlmTEUzCwAPALcTWV90l5ltd/fDE5p1AX8EvGNWqpwkMstFc9BFRCaKpYe+Cahz93p3HwW2AVsmNnD3s+6+CxibhRrPMzgSIi9TPXQRkYliCfRqoHHCdlN030Uzs3vMbLeZ7W5vb7+Ut2A87AyPjWvIRURkklgCfapFO/1SPszdH3rgSpAAAATFSURBVHT3WnevraiouJS3YHA0ctt/QbYCXURkolgCvQlYPGG7BmiZnXJmNhDUg7lERKYSS6DvAlab2XIzywTuArbPblnTG9STFkVEpjRjKrp7yMzuAx4DAsBD7n7IzO6NHt9qZlXAbqAQCJvZx4F17t4X74IHzj0LXbNcREQmiqmb6+47gB2T9m2d8LqNyFDMrBscGQfQLBcRkUmS7k5RLW4hIjK1pAv0ioJM7ry6ivJ8rScqIjJR0nVzNy4tZePS0kSXISIy5yRdD11ERKamQBcRSREKdBGRFKFAFxFJEQp0EZEUoUAXEUkRCnQRkRShQBcRSRHmfkmPNn/lH2zWDpy+xL9eDnTEsZxE0rnMTTqXuUnnAkvdfcoFJRIW6K+Eme1299pE1xEPOpe5SecyN+lcLkxDLiIiKUKBLiKSIpI10B9MdAFxpHOZm3Quc5PO5QKScgxdRETOl6w9dBERmUSBLiKSIpIu0M3sDjM7ZmZ1ZnZ/ouu5WGZ2yswOmNk+M9sd3VdqZo+b2fHonyWJrnMqZvaQmZ01s4MT9k1bu5l9KnqdjpnZmxNT9dSmOZfPmllz9NrsM7M7Jxybk+diZovN7BdmdsTMDpnZH0f3J911ucC5JON1yTaznWb2QvRc/iq6f3avi7snzQ8QAE4AK4BM4AVgXaLrushzOAWUT9r3BeD+6Ov7gc8nus5pan8NsAE4OFPtwLro9ckClkevWyDR5zDDuXwW+LMp2s7ZcwEWAhuirwuAF6P1Jt11ucC5JON1MSA/+joDeA64YbavS7L10DcBde5e7+6jwDZgS4JrioctwDejr78JvCOBtUzL3Z8Euibtnq72LcA2dx9x95NAHZHrNydMcy7TmbPn4u6t7v589HU/cASoJgmvywXOZTpz+Vzc3QeimxnRH2eWr0uyBXo10Dhhu4kLX/C5yIGfmtkeM7snum+Bu7dC5F9qoDJh1V286WpP1mt1n5ntjw7JvPTrcFKci5ktA15NpDeY1Ndl0rlAEl4XMwuY2T7gLPC4u8/6dUm2QLcp9iXbvMub3X0DsBn4mJm9JtEFzZJkvFb/D1gJXAu0An8X3T/nz8XM8oHvAx93974LNZ1i31w/l6S8Lu4+7u7XAjXAJjO76gLN43IuyRboTcDiCds1QEuCarkk7t4S/fMs8AMiv1adMbOFANE/zyauwos2Xe1Jd63c/Uz0P8Iw8DV+8yvvnD4XM8sgEoDfdveHo7uT8rpMdS7Jel1e4u49wH8DdzDL1yXZAn0XsNrMlptZJnAXsD3BNcXMzPLMrOCl18CbgINEzuGD0WYfBB5JTIWXZLratwN3mVmWmS0HVgM7E1BfzF76Dy3qnUSuDczhczEzA/4ZOOLuX5pwKOmuy3TnkqTXpcLMiqOvc4A3AkeZ7euS6G+DL+Hb4zuJfPt9Avh0ouu5yNpXEPkm+wXg0Ev1A2XAE8Dx6J+lia51mvq/S+RX3jEiPYqPXKh24NPR63QM2Jzo+mM4l38FDgD7o/+BLZzr5wLcQuRX8/3AvujPncl4XS5wLsl4Xa4B9kZrPgh8Jrp/Vq+Lbv0XEUkRyTbkIiIi01Cgi4ikCAW6iEiKUKCLiKQIBbqISIpQoIuIpAgFuohIivj/uSI+BPzGaBQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model \n",
    "t_total = time.time() \n",
    "acc_epoch = []\n",
    "best_acc = 0\n",
    "start = time.time()\n",
    "for epoch in range(300   ):   \n",
    "    acc_train = train( epoch,  model2,optimizer2,  features_X    , labels   , Adj, ind_train, fastmode, ind_train, batch_size)\n",
    "    acc_test, acc_hop = test(  model2, features_X , labels ,A , Adj, ind_test)\n",
    "    acc_epoch.append(acc_test.numpy())\n",
    "    if acc_test.numpy() > best_acc:\n",
    "        best_acc = acc_test.numpy() \n",
    "        state = {'epoch': epoch + 1, 'state_dict': model2.state_dict() ,\n",
    "             'optimizer': optimizer2.state_dict() }\n",
    "        torch.save(state, savepath2)\n",
    "print(\"Optimization Finished!\")\n",
    "#print(\"Total time elapsed: {:.4f}s\".format(time.time() - start))\n",
    "print('The max acc is %.4f' %np.max(acc_epoch))\n",
    "#plt.plot(acc_epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Total time elapsed: {:.4f}s\".format(time.time() - start))\n",
    "print('The test max acc is %.4f' %np.max(acc_epoch))\n",
    "print('The test argmax acc is %2d' %np.argmax(acc_epoch))\n",
    "plt.plot(acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
